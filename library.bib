Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{RSSL,
author = {Krijthe, Jesse Hendrik},
title = {{RSSL: Semi-supervised Learning in R}},
url = {https://github.com/jkrijthe/RSSL},
year = {2016}
}
@unpublished{Hennig2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02555v1},
author = {Hennig, Christian},
eprint = {arXiv:1502.02555v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig - 2015 - What are the true clusters.pdf:pdf},
keywords = {active scientific realism,cat-,comparison of clustering methods,constructivism,egorization,mixture models,natural kinds,variable},
title = {{What are the true clusters?}},
year = {2015}
}
@article{Celik2016,
abstract = {Modern detection systems use sensor outputs available in the deployment environment to probabilistically identify attacks. These systems are trained on past or synthetic feature vectors to create a model of anomalous or normal behavior. Thereafter, run-time collected sensor outputs are compared to the model to identify attacks (or the lack of attack). While this approach to detection has been proven to be effective in many environments, it is limited to training on only features that can be reliably collected at test-time. Hence, they fail to leverage the often vast amount of ancillary information available from past forensic analysis and post-mortem data. In short, detection systems don't train (and thus don't learn from) features that are unavailable or too costly to collect at run-time. In this paper, we leverage recent advances in machine learning to integrate privileged information --features available at training time, but not at run-time-- into detection algorithms. We apply three different approaches to model training with privileged information; knowledge transfer, model influence, and distillation, and empirically validate their performance in a range of detection domains. Our evaluation shows that privileged information can increase detector precision and recall: we observe an average of 4.8{\%} decrease in detection error for malware traffic detection over a system with no privileged information, 3.53{\%} for fast-flux domain bot detection, 3.33{\%} for malware classification, 11.2{\%} for facial user authentication. We conclude by exploring the limitations and applications of different privileged information techniques in detection systems.},
archivePrefix = {arXiv},
arxivId = {1603.09638},
author = {Celik, Z. Berkay and McDaniel, Patrick and Izmailov, Rauf and Papernot, Nicolas and Swami, Ananthram},
eprint = {1603.09638},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Celik et al. - 2016 - Building Better Detection with Privileged Information.pdf:pdf},
title = {{Building Better Detection with Privileged Information}},
url = {http://arxiv.org/abs/1603.09638},
year = {2016}
}
@article{Rasmus2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08215v1},
author = {Rasmus, Antti and Valpola, Harri},
eprint = {arXiv:1504.08215v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmus, Valpola - 2015 - Lateral Connections in Denoising Autoencoders Support Supervised Learning.pdf:pdf},
pages = {1--5},
title = {{Lateral Connections in Denoising Autoencoders Support Supervised Learning}},
year = {2015}
}
@article{Gelman2010,
abstract = {We review some approaches and philosophies of causal inference coming from sociology, economics, computer science, cognitive science, and statistics},
archivePrefix = {arXiv},
arxivId = {1003.2619},
author = {Gelman, Andrew},
doi = {10.1086/662659},
eprint = {1003.2619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2010 - Causality and Statistical Learning.pdf:pdf},
issn = {00029602},
journal = {American Journal of Sociology},
number = {3},
pages = {955--966},
title = {{Causality and Statistical Learning}},
url = {http://arxiv.org/abs/1003.2619},
volume = {117},
year = {2010}
}
@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
eprint = {1601.00670},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf},
isbn = {1601.00670},
journal = {arXiv},
keywords = {Graphical Model,Variational Inference},
pages = {1--33},
title = {{Variational Inference: A Review for Statisticians}},
url = {http://arxiv.org/abs/1601.00670},
year = {2016}
}
@article{Stigler2013,
author = {Stigler, Stephen M.},
doi = {10.1214/13-STS438},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stigler - 2013 - The True Title of Bayes's Essay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Thomas Bayes, Richard Price, Bayes's theorem, hist,and phrases},
month = {aug},
number = {3},
pages = {283--288},
title = {{The True Title of Bayes's Essay}},
url = {http://projecteuclid.org/euclid.ss/1377696937},
volume = {28},
year = {2013}
}
@article{Ireland1968,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Minimum Discrimination Information Estimation.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {707--713},
title = {{Minimum Discrimination Information Estimation}},
url = {http://www.jstor.org/stable/10.2307/2528330},
volume = {24},
year = {1968}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu - 2005 - Semi-supervised learning literature survey.pdf:pdf},
institution = {University of Wisconsin - Madison},
pages = {1--59},
title = {{Semi-supervised learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@article{Cesa-Bianchi2007,
author = {Cesa-Bianchi, Nicol{\`{o}}},
doi = {10.1016/j.tcs.2007.03.053},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cesa-Bianchi - 2007 - Applications of regularized least squares to pattern classification.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {on-line learning,perceptron,ridge regression,selective sampling},
month = {sep},
number = {3},
pages = {221--231},
title = {{Applications of regularized least squares to pattern classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030439750700237X},
volume = {382},
year = {2007}
}
@article{Calandriello2016,
abstract = {While the harmonic function solution performs well in many semi-supervised learning (SSL) tasks, it is known to scale poorly with the number of samples. Recent successful and scalable methods, such as the eigenfunction method focus on efficiently approximating the whole spectrum of the graph Laplacian constructed from the data. This is in contrast to various subsampling and quantization methods proposed in the past, which may fail in preserving the graph spectra. However, the impact of the approximation of the spectrum on the final generalization error is either unknown, or requires strong assumptions on the data. In this paper, we introduce Sparse-HFS, an efficient edge-sparsification algorithm for SSL. By constructing an edge-sparse and spectrally similar graph, we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of Sparse-HFS. As a result, we obtain a theoretically-grounded approximation scheme for graph-based SSL that also empirically matches the performance of known large-scale methods.},
archivePrefix = {arXiv},
arxivId = {1601.05675},
author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal and Koutis, Ioannis},
eprint = {1601.05675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Calandriello et al. - 2016 - Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning.pdf:pdf},
title = {{Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning}},
url = {http://arxiv.org/abs/1601.05675},
year = {2016}
}
@article{Jaffe2013,
abstract = {BACKGROUND: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features associated with illness. We propose a new approach, called gene set bagging, for measuring the probability that a gene set replicates in future studies. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate in the bagged samples.

RESULTS: Using both simulated and publicly-available genomics data, we demonstrate that significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. We show our method estimates the replication probability (R), the probability that a gene set will replicate as a significant result in future studies, and show in simulations that this method reflects replication better than each set's p-value.

CONCLUSIONS: Our results suggest that gene lists based on p-values are not necessarily stable, and therefore additional steps like gene set bagging may improve biological inference on gene sets.},
author = {Jaffe, Andrew E and Storey, John D and Ji, Hongkai and Leek, Jeffrey T},
doi = {10.1186/1471-2105-14-360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaffe et al. - 2013 - Gene set bagging for estimating the probability a statistically significant result will replicate.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {dna methylation,gene expression,gene ontology,gene set enrichment analysis},
month = {jan},
pages = {360},
pmid = {24330332},
title = {{Gene set bagging for estimating the probability a statistically significant result will replicate.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3890500{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {14},
year = {2013}
}
@article{Bengio2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Grandvalet - 2004 - No unbiased estimator of the variance of k-fold cross-validation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of k-fold cross-validation}},
volume = {5},
year = {2004}
}
@article{Sugiyama2010,
author = {Sugiyama, Masashi},
doi = {10.1587/transinf.E93.D.2690},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama - 2010 - Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting.pdf:pdf},
isbn = {0916-8532},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Classposterior probability,Kernel logistic regression,Probabilistic classification,Squared-loss},
number = {10},
pages = {2690--2701},
title = {{Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting}},
volume = {E93-D},
year = {2010}
}
@article{Saeys2007,
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.},
author = {Saeys, Yvan and Inza, I{\~{n}}aki and Larra{\~{n}}aga, Pedro},
doi = {10.1093/bioinformatics/btm344},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Saeys, Inza, Larra{\~{n}}aga - 2007 - A review of feature selection techniques in bioinformatics.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {19},
pages = {2507--2517},
pmid = {17720704},
title = {{A review of feature selection techniques in bioinformatics}},
volume = {23},
year = {2007}
}
@article{Shannon1948,
author = {Shannon, Claude Elwood},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1948 - A mathematical theory of communication.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {J},
pages = {379--423},
title = {{A mathematical theory of communication}},
url = {http://dl.acm.org/citation.cfm?id=584093},
volume = {27},
year = {1948}
}
@article{VonHippel2007,
author = {von Hippel, Paul T.},
doi = {10.1111/j.1467-9531.2007.00180.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/von Hippel - 2007 - Regression With Missing Ys an Improved Strategy for Analyzing Multiply Imputed Data.pdf:pdf},
issn = {0081-1750},
journal = {Sociological Methodology},
month = {dec},
number = {1},
pages = {83--117},
title = {{Regression With Missing Ys: an Improved Strategy for Analyzing Multiply Imputed Data}},
url = {http://smx.sagepub.com/lookup/doi/10.1111/j.1467-9531.2007.00180.x},
volume = {37},
year = {2007}
}
@inproceedings{Grandvalet2005,
address = {Cambridge, MA},
author = {Grandvalet, Yves and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grandvalet, Bengio - 2005 - Semi-supervised learning by entropy minimization.pdf:pdf},
pages = {529--536},
publisher = {MIT Press},
title = {{Semi-supervised learning by entropy minimization}},
year = {2005}
}
@techreport{Welling,
author = {Welling, Max},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welling - Unknown - Kernel ridge Regression.pdf:pdf},
number = {3},
pages = {3--5},
title = {{Kernel ridge Regression}}
}
@article{Mooij2014,
abstract = {The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X,Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of causal sufficiency, no feedback loops, and no selection bias, such bivariate causal discovery problems are very challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: Additive Noise Methods (ANM) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 96 different cause-effect pairs selected from 34 datasets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.). We motivate our decisions regarding the "ground truth" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results indicate that certain methods are able to distinguish cause from effect using only purely observational data with an accuracy of 63-69{\%}. Because of multiple-testing corrections, however, considerably more benchmark data would be needed to obtain statistically significant conclusions. A theoretical contribution of this paper is a proof of the consistency of the additive-noise method as originally proposed by Hoyer et al. (2009).},
archivePrefix = {arXiv},
arxivId = {1412.3773},
author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"{o}}lkopf, Bernhard},
eprint = {1412.3773},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2014 - Distinguishing cause from effect using observational data methods and benchmarks.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2014 - Distinguishing cause from effect using observational data methods and benchmarks(2).pdf:pdf},
month = {dec},
pages = {96},
pmid = {1000106307},
title = {{Distinguishing cause from effect using observational data: methods and benchmarks}},
url = {http://arxiv.org/abs/1412.3773 http://arxiv.org/abs/1412.3773v1},
volume = {17},
year = {2014}
}
@article{Heitjan1994,
author = {Heitjan, Daniel F and Landis, J Richard},
doi = {10.2307/2290900},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Heitjan, Landis - 1994 - Assessing Secular Trends in Blood Pressure {\{}A{\}} Multiple-imputation Approach.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian bootstrap,hot deck,incomplete data,missing data,observational study,predictive-mean matching},
number = {August 2015},
pages = {750--759},
title = {{Assessing Secular Trends in Blood Pressure: {\{}A{\}} Multiple-imputation Approach}},
volume = {89},
year = {1994}
}
@article{Zhou2005,
author = {Zhou, Zhi-hua and Li, Ming},
doi = {10.1109/TKDE.2005.186},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Tri-training exploiting unlabeled data using three classifiers.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {nov},
number = {11},
pages = {1529--1541},
title = {{Tri-training: exploiting unlabeled data using three classifiers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1512038},
volume = {17},
year = {2005}
}
@article{Afendras2015,
abstract = {An important question in constructing Cross Validation (CV) estimators of the generalization error is whether rules can be established that allow "optimal" selection of the size of the training set, for fixed sample size {\$}n{\$}. We define the {\{}$\backslash$it resampling effectiveness{\}} of random CV estimators of the generalization error as the ratio of the limiting value of the variance of the CV estimator over the estimated from the data variance. The variance and the covariance of different average test set errors are independent of their indices, thus, the resampling effectiveness depends on the correlation and the number of repetitions used in the random CV estimator. We discuss statistical rules to define optimality and obtain the "optimal" training sample size as the solution of an appropriately formulated optimization problem. We show that in a broad class of loss functions the optimal training size equals half of the total sample size, independently of the data distribution. We optimally select the number of folds in {\$}k{\$}-fold cross validation and offer a computational procedure for obtaining the optimal splitting in the case of classification (via logistic regression). We substantiate our claims both, theoretically and empirically.},
archivePrefix = {arXiv},
arxivId = {1511.02980},
author = {Afendras, Georgios and Markatou, Marianthi},
eprint = {1511.02980},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Afendras, Markatou - 2015 - Optimality of TrainingTest Size and Resampling Effectiveness of Cross-Validation Estimators of the Generaliz.pdf:pdf},
keywords = {alo,author,both authors acknowledge support,cross validation estimator,in,of biostatistics,optimality,package to the second,provided by the department,resampling e ff ectiveness,the form of start-up,training sample size,university at bu ff},
number = {716},
title = {{Optimality of Training/Test Size and Resampling Effectiveness of Cross-Validation Estimators of the Generalization Error}},
url = {http://arxiv.org/abs/1511.02980},
volume = {1},
year = {2015}
}
@article{Leday2015,
abstract = {Reconstructing a gene network from high-throughput molecular data is often a challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighbourhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with non-sparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyper-parameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible.},
archivePrefix = {arXiv},
arxivId = {1510.03771},
author = {Leday, Gwena{\"{e}}l G. R. and de Gunst, Mathisca C. M. and Kpogbezan, Gino B. and {Van der Vaart}, Aad W. and {Van Wieringen}, Wessel N. and {Van de Wiel}, Mark A.},
eprint = {1510.03771},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leday et al. - 2015 - Gene network reconstruction using global-local shrinkage priors.pdf:pdf},
title = {{Gene network reconstruction using global-local shrinkage priors}},
url = {http://arxiv.org/abs/1510.03771},
year = {2015}
}
@inproceedings{Miguel2014,
author = {Carreira-Perpinan, Miguel A. and Wang, Weiran},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carreira-Perpinan, Wang - 2014 - Distributed Optimization of Deeply Nested Systems.pdf:pdf},
pages = {10--19},
title = {{Distributed Optimization of Deeply Nested Systems}},
year = {2014}
}
@inproceedings{Abney2002,
author = {Abney, Steven},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2002 - Bootstrapping.pdf:pdf},
number = {July},
pages = {360--367},
title = {{Bootstrapping}},
year = {2002}
}
@misc{Hanczar2015,
author = {Hanczar, Blaise and Zucker, Jean-Daniel},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Zucker - 2015 - A generic approach to optimizing abstaining area for small sample data classification.pdf:pdf},
keywords = {abstract,accuracy relies on the,an approach to improve,classifiers,given a classification task,in the feature space,not,reject option,reliable enough,small-sample setting,supervised learning,these classifiers are trained,these rejected observations belong,to an abstaining area,to reject observations for,two,use of abstaining,which predicted values are},
title = {{A generic approach to optimizing abstaining area for small sample data classification}},
year = {2015}
}
@article{Ben-David2015a,
abstract = {It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such "clusterable" instances. This paper addresses the thesis that the computational hardness of clustering tasks goes away for inputs that one really cares about. In other words, that "Clustering is difficult only when it does not matter" (the $\backslash$emph{\{}CDNM thesis{\}} for short). I wish to present a a critical bird's eye overview of the results published on this issue so far and to call attention to the gap between available and desirable results on this issue. A longer, more detailed version of this note is available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal support to the the CDNM thesis and then examine existing results in view of these requirements and list some significant unsolved research challenges in that direction.},
archivePrefix = {arXiv},
arxivId = {1510.05336},
author = {Ben-David, Shai},
eprint = {1510.05336},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David - 2015 - Clustering is Easy When ....What.pdf:pdf},
pages = {2--7},
title = {{Clustering is Easy When ....What?}},
url = {http://arxiv.org/abs/1510.05336},
year = {2015}
}
@article{Bareinboim2014,
abstract = {Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.},
author = {Bareinboim, Elias and Tian, Jin and Pearl, Judea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bareinboim, Tian, Pearl - 2014 - Recovering from Selection Bias in Causal and Statistical Inference.pdf:pdf},
journal = {Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI 2014)},
keywords = {causal inference,causality,sampling bias,selection bias,statistical inference},
number = {Pearl},
pages = {2410--2416},
title = {{Recovering from Selection Bias in Causal and Statistical Inference}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8628/8707 http://ftp.cs.ucla.edu/pub/stat{\_}ser/r425.pdf},
year = {2014}
}
@inproceedings{Mey2016,
author = {Mey, Alexander and Loog, Marco},
booktitle = {International Conference on Pattern Recognition (To Appear)},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mey, Loog - 2016 - A Soft-Labeled Self-Training Approach.pdf:pdf},
title = {{A Soft-Labeled Self-Training Approach}},
year = {2016}
}
@article{Ben-David2011b,
author = {Ben-david, Shai and Srebro, Nati and Urner, Ruth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-david, Srebro, Urner - 2011 - Is learning possible without Prior Knowledge Do Universal Learners exist High level view of ( Statis.pdf:pdf},
title = {{Is learning possible without Prior Knowledge ? Do Universal Learners exist ? High level view of ( Statistical ) Machine Learning}},
year = {2011}
}
@inproceedings{Krijthe2014,
address = {Stockholm},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {International Conference on Pattern Recognition},
pages = {3762--3767},
title = {{Implicitly Constrained Semi-Supervised Linear Discriminant Analysis}},
year = {2014}
}
@article{Platanios2014,
author = {Platanios, Emmanouil Antonios and Blum, Avrim and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Platanios, Blum, Mitchell - 2014 - Estimating Accuracy from Unlabeled Data.pdf:pdf},
isbn = {9780974903910},
journal = {30th Conference on Uncertainty in Artificial Intelligence},
title = {{Estimating Accuracy from Unlabeled Data}},
year = {2014}
}
@article{Vandewalle2013,
author = {Vandewalle, Vincent and Biernacki, Christophe},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle, Biernacki - 2013 - A predictive deviance criterion for selecting a generative model in semi-supervised classification.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
pages = {220--236},
title = {{A predictive deviance criterion for selecting a generative model in semi-supervised classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000546},
volume = {64},
year = {2013}
}
@article{Leistner2009,
author = {Leistner, Christian and Saffari, Amir and Santner, Jakob and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459198},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leistner et al. - 2009 - Semi-Supervised Random Forests.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = {sep},
pages = {506--513},
publisher = {Ieee},
title = {{Semi-Supervised Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459198},
year = {2009}
}
@article{Soudry2016,
abstract = {We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almost every dataset and dropout-like noise realization. We then extend these results to the case of more than one hidden layer. Our theoretical guarantees assume essentially nothing on the training data, and are verified numerically. These results suggest why the highly non-convex loss of such MNNs can be easily optimized using local updates (e.g., stochastic gradient descent), as observed empirically.},
archivePrefix = {arXiv},
arxivId = {1605.08361},
author = {Soudry, Daniel and Carmon, Yair},
eprint = {1605.08361},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soudry, Carmon - 2016 - No bad local minima Data independent training error guarantees for multilayer neural networks.pdf:pdf},
number = {DLM},
pages = {1--12},
title = {{No bad local minima: Data independent training error guarantees for multilayer neural networks}},
url = {http://arxiv.org/abs/1605.08361},
year = {2016}
}
@article{Gelman2011,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Induction and deduction in Bayesian data analysis.pdf:pdf},
journal = {Rationality, Markets and Morals (RMM)},
pages = {67--78},
title = {{Induction and deduction in Bayesian data analysis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/philosophy{\_}online4.pdf},
volume = {2},
year = {2011}
}
@article{Kleinberg2000,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 2000 - On the algorithmic implementation of stochastic discrimination.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {473--490},
publisher = {IEEE},
title = {{On the algorithmic implementation of stochastic discrimination}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=857004},
volume = {22},
year = {2000}
}
@article{Kasabov2003,
author = {Kasabov, Nikola and Pang, Shaoning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kasabov, Pang - 2003 - Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition.pdf:pdf},
journal = {Proceedings of the International Conference on Neural networks and signal processing},
keywords = {inductive svm,motif,promoter,promoter recognition,transductive svm},
number = {2},
pages = {31--38},
title = {{Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition}},
volume = {3},
year = {2003}
}
@article{Kosmidis2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01388v1},
author = {Kosmidis, Ioannis and Passfield, Louis},
eprint = {arXiv:1506.01388v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kosmidis, Passfield - 2015 - Linking the performance of endurance runners to training and physiological effects via multi-resolution ela.pdf:pdf},
keywords = {collinearity,grouping effect,law,power,regularization,training distribution profile,wearable gps devices},
number = {1975},
title = {{Linking the performance of endurance runners to training and physiological effects via multi-resolution elastic net}},
year = {2015}
}
@article{Vehtari2015,
abstract = {Importance weighting is a convenient general way to adjust for draws from the wrong distribution, but the resulting ratio estimate can be noisy when the importance weights have a heavy right tail, as routinely occurs when there are aspects of the target distribution not well captured by the approximating distribution. More stable estimates can be obtained by truncating the importance ratios. Here we present a new method for stabilizing importance weights using generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios.},
archivePrefix = {arXiv},
arxivId = {1507.02646},
author = {Vehtari, Aki and Gelman, Andrew},
eprint = {1507.02646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vehtari, Gelman - 2015 - Very Good Importance Sampling.pdf:pdf},
keywords = {bayesian computation,importance sampling,leave-one-,loo,monte carlo,out cross-validation},
number = {July},
title = {{Very Good Importance Sampling}},
url = {http://arxiv.org/abs/1507.02646},
year = {2015}
}
@inproceedings{Vandewalle2008,
author = {Vandewalle, Vincent and Biernacki, Christophe and Celeux, Gilles and Govaert, Gerard},
booktitle = {vincent.vandewalle.perso.sfr.fr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle et al. - 2008 - Are unlabeled data useful in semi-supervised model-based classification Combining hypothesis testing and mode.pdf:pdf},
title = {{Are unlabeled data useful in semi-supervised model-based classification? Combining hypothesis testing and model choice}},
url = {http://vincent.vandewalle.perso.sfr.fr/documents/recherche/articles/vbcg.pdf},
year = {2008}
}
@article{Tran2015,
abstract = {Representations offered by deep generative models are fundamentally tied to their inference method from data. Variational inference methods require a rich family of approximating distributions. We construct the variational Gaussian process (VGP), a Bayesian nonparametric model which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by autoencoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.},
archivePrefix = {arXiv},
arxivId = {1511.06499},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
eprint = {1511.06499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tran, Ranganath, Blei - 2015 - Variational Gaussian Process.pdf:pdf},
pages = {1--14},
title = {{Variational Gaussian Process}},
url = {http://arxiv.org/abs/1511.06499},
year = {2015}
}
@article{Gardner2015,
abstract = {Machine learning is increasingly used in high impact applications such as prediction of hospital re-admission, cancer screening or bio-medical research applications. As predictions become increasingly accurate, practitioners may be interested in identifying actionable changes to inputs in order to alter their class membership. For example, a doctor might want to know what changes to a patient's status would predict him/her to not be re-admitted to the hospital soon. Szegedy et al. (2013b) demonstrated that identifying such changes can be very hard in image classification tasks. In fact, tiny, imperceptible changes can result in completely different predictions without any change to the true class label of the input. In this paper we ask the question if we can make small but meaningful changes in order to truly alter the class membership of images from a source class to a target class. To this end we propose deep manifold traversal, a method that learns the manifold of natural images and provides an effective mechanism to move images from one area (dominated by the source class) to another (dominated by the target class).The resulting algorithm is surprisingly effective and versatile. It allows unrestricted movements along the image manifold and only requires few images from source and target to identify meaningful changes. We demonstrate that the exact same procedure can be used to change an individual's appearance of age, facial expressions or even recolor black and white images.},
archivePrefix = {arXiv},
arxivId = {1511.06421},
author = {Gardner, Jacob R. and Kusner, Matt J. and Li, Yixuan and Upchurch, Paul and Weinberger, Kilian Q. and Hopcroft, John E.},
eprint = {1511.06421},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gardner et al. - 2015 - Deep Manifold Traversal Changing Labels with Convolutional Features.pdf:pdf},
pages = {1--11},
title = {{Deep Manifold Traversal: Changing Labels with Convolutional Features}},
url = {http://arxiv.org/abs/1511.06421},
year = {2015}
}
@article{Goodman2016a,
author = {Goodman, Steven N and Fanelli, Daniele and Ioannidis, John P A},
doi = {10.1126/scitranslmed.aaf5027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goodman, Fanelli, Ioannidis - 2016 - What does research reproducibility mean.pdf:pdf},
issn = {1946-6234},
journal = {Science Translational Medicine},
number = {341},
pages = {341ps12},
pmid = {27252173},
title = {{What does research reproducibility mean ?}},
volume = {8},
year = {2016}
}
@inproceedings{Collins1999,
author = {Collins, Michael and Singer, Yoram},
booktitle = {Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collins, Singer - 1999 - Unsupervised models for named entity classification.pdf:pdf},
pages = {189--196},
title = {{Unsupervised models for named entity classification}},
url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR{\%}7B.{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}C7{\%}9D{\%}EF{\%}BF{\%}BD{\%}E2{\%}80{\%}A1{\%}5E{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}C3{\%}A87},
year = {1999}
}
@article{Balcan2010,
author = {Balcan, Maria-Florina and Blum, Avrim},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Blum - 2010 - A Discriminative Model for Semi-Supervised Learning.pdf:pdf},
journal = {Journal of the ACM (JACM)},
number = {3},
title = {{A Discriminative Model for Semi-Supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1706599},
volume = {57},
year = {2010}
}
@article{Donoho2015,
author = {Donoho, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Donoho - 2015 - 50 years of data collection UNHCR expce.pdf:pdf},
pages = {1--41},
title = {{50 years of data collection UNHCR expce}},
year = {2015}
}
@article{Pilanci2014,
abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including {\$}\backslashell{\_}1{\$}-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
archivePrefix = {arXiv},
arxivId = {1411.0347},
author = {Pilanci, Mert and Wainwright, Martin J.},
eprint = {1411.0347},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pilanci, Wainwright - 2014 - Iterative Hessian sketch Fast and accurate solution approximation for constrained least-squares.pdf:pdf},
month = {nov},
number = {1},
pages = {1--33},
title = {{Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares}},
url = {http://arxiv.org/abs/1411.0347},
year = {2014}
}
@article{Kall2007,
abstract = {Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17{\%} more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77{\%} more spectra from non-tryptic digests, relative to a fully supervised approach.},
author = {K{\"{a}}ll, Lukas and Canterbury, Jesse D and Weston, Jason and Noble, William Stafford and MacCoss, Michael J},
doi = {10.1038/nmeth1113},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{a}}ll et al. - 2007 - Semi-supervised learning for peptide identification from shotgun proteomics datasets.pdf:pdf},
journal = {Nature methods},
number = {11},
pages = {923--925},
title = {{Semi-supervised learning for peptide identification from shotgun proteomics datasets.}},
volume = {4},
year = {2007}
}
@article{Seth2013,
abstract = {Archetypal analysis represents a set of observations as convex combinations of pure patterns, or archetypes. The original geometric formulation of finding archetypes by approximating the convex hull of the observations assumes them to be real valued. This, unfortunately, is not compatible with many practical situations. In this paper we revisit archetypal analysis from the basic principles, and propose a probabilistic framework that accommodates other observation types such as integers, binary, and probability vectors. We corroborate the proposed methodology with convincing real-world applications on finding archetypal winter tourists based on binary survey data, archetypal disaster-affected countries based on disaster count data, and document archetypes based on term-frequency data. We also present an appropriate visualization tool to summarize archetypal analysis solution better.},
archivePrefix = {arXiv},
arxivId = {1312.7604},
author = {Seth, Sohan and Eugster, Manuel J. a.},
doi = {10.1007/s10994-015-5498-8},
eprint = {1312.7604},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seth, Eugster - 2013 - Probabilistic Archetypal Analysis.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Archetypal analysis,Binary observation,Convex hull,Majorizationâ€“minimization,Probabilistic modeling,Visualization,archetypal analysis,binary observation,convex hull,majorization,minimization,probabilistic modeling,visualization},
number = {April 2014},
pages = {24},
publisher = {Springer US},
title = {{Probabilistic Archetypal Analysis}},
url = {http://arxiv.org/abs/1312.7604},
year = {2013}
}
@article{Kouw2016,
author = {Kouw, Wouter M and Krijthe, Jesse Hendrik and Loog, Marco and {Van der Maaten}, Laurens},
journal = {Forthcoming},
title = {{Feature-Level Domain Adaptation}},
year = {2016}
}
@inproceedings{Matti2006,
author = {Kaariainen, Matti},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kaariainen - 2006 - Semi-Supervised Model Selection Based on Cross-Validation.pdf:pdf},
number = {510},
title = {{Semi-Supervised Model Selection Based on Cross-Validation}},
year = {2006}
}
@article{Hennig2016,
abstract = {SUMMARY Classification with small samples of high-dimensional data is important in many application areas. Quantile classifiers are distance-based classifiers that require a single parameter, regard-less of the dimension, and classify observations according to a sum of weighted componentwise distances of the components of an observation to the within-class quantiles. An optimal per-centage for the quantiles can be chosen by minimizing the misclassification error in the training sample. It is shown that this choice is consistent for the classification rule with the asymptotically optimal quantile and that under some assumptions, as the number of variables goes to infinity, the probability of correct classification converges to unity. The effect of skewness of the distributions of the predictor variables is discussed. The optimal quantile classifier gives low misclassification rates in a comprehensive simulation study and in a real-data application.},
archivePrefix = {arXiv},
arxivId = {1303.1282},
author = {Hennig, C and Viroli, C},
doi = {10.1093/biomet/asw015},
eprint = {1303.1282},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig, Viroli - 2016 - Quantile-based classifiers.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
keywords = {High-dimensional data,Median-based classifier,Misclassification rate,Skewness,Some key words},
number = {2},
pages = {435--446},
title = {{Quantile-based classifiers}},
volume = {103},
year = {2016}
}
@article{Kanamori2012,
author = {Kanamori, Takafumi and Takeda, A and Suzuki, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kanamori, Takeda, Suzuki - 2012 - A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {consistency,convex conjugate,loss function,uncertainty set},
pages = {1461--1504},
title = {{A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems}},
url = {http://arxiv.org/abs/1204.6583},
volume = {14},
year = {2012}
}
@article{Zhang2004,
author = {Zhang, P. and Peng, J.},
doi = {10.1109/ICPR.2004.1334050},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Peng - 2004 - SVM vs regularized least squares classification.pdf:pdf},
isbn = {0-7695-2128-2},
journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
number = {3},
pages = {176--179 Vol.1},
publisher = {Ieee},
title = {{SVM vs regularized least squares classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334050},
year = {2004}
}
@article{Gu2012,
author = {Gu, Quanquan and Han, Jiawei},
doi = {10.1109/ICDM.2012.72},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gu, Han - 2012 - Towards active learning on graphs An error bound minimization approach.pdf:pdf},
isbn = {9780769549057},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Active learning,Generalization error bound,Graph,Sequential optimization},
pages = {882--887},
title = {{Towards active learning on graphs: An error bound minimization approach}},
year = {2012}
}
@book{Basu2006,
author = {Basu, Mitra and Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Basu, Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
title = {{Data complexity in pattern recognition}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GflBKbzym9oC{\&}oi=fnd{\&}pg=PR11{\&}dq=Data+Complexity+in+Pattern+Recognition{\&}ots=igbI3IXn6d{\&}sig=-7L3L4iU5lzLaNaCVoEux{\_}GbVn4},
year = {2006}
}
@inproceedings{Narasimhan,
author = {Narasimhan, Mukund and Jojic, Nebojsa and Bilmes, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Narasimhan, Jojic, Bilmes - 2005 - Q-Clustering.pdf:pdf},
pages = {979--986},
title = {{Q-Clustering}},
year = {2005}
}
@article{Carlo2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07645v1},
author = {Carlo, Monte and Feb, M L},
eprint = {arXiv:1502.07645v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carlo, Feb - 2015 - Privacy for Free Posterior Sampling and Stochastic Gradient.pdf:pdf},
pages = {1--27},
title = {{Privacy for Free : Posterior Sampling and Stochastic Gradient}},
year = {2015}
}
@misc{Klein2004,
author = {Klein, Dan},
booktitle = {University of California at Berkeley, Computer Science {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klein - 2004 - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {http://www.ee.columbia.edu/{~}vittorio/LagrangeMultipliers-Klein.pdf},
year = {2004}
}
@article{Balcan2014,
abstract = {One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise $\backslash$cite{\{}qcluster2005{\}}. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm consistently achieves better performance than other hierarchical algorithms in the presence of noise.},
archivePrefix = {arXiv},
arxivId = {1401.0247},
author = {Balcan, Mf Maria-florina and Liang, Yingyu and Gupta, Pramod},
eprint = {1401.0247},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Liang, Gupta - 2014 - Robust hierarchical clustering.pdf:pdf},
isbn = {9780982252925},
journal = {arXiv preprint arXiv:1401.0247},
keywords = {agglomerative algorithms,clustering,robustness,unsupervised learning},
pages = {35},
title = {{Robust hierarchical clustering}},
url = {http://arxiv.org/abs/1401.0247},
volume = {15},
year = {2014}
}
@inproceedings{Krijthe2015,
address = {Saint {\'{E}}tienne. France},
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {14th International Symposium on Advances in Intelligent Data Analysis XIV (Lecture Notes in Computer Science Volume 9385)},
editor = {Fromont, Elisa and Bie, Tijl De and van Leeuwen, Matthijs},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Krijthe, Loog - 2015 - Implicitly Constrained Semi-Supervised Least Squares Classification.pdf:pdf},
keywords = {constrained,least squares classification,semi-supervised learning},
pages = {158--169},
title = {{Implicitly Constrained Semi-Supervised Least Squares Classification}},
year = {2015}
}
@article{Williams1998,
author = {Williams, Christopher K. I.},
doi = {10.1162/089976698300017412},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williams - 1998 - Computation with Infinite Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {5},
pages = {1203--1216},
title = {{Computation with Infinite Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017412},
volume = {10},
year = {1998}
}
@article{Lockhart2014c,
author = {Wasserman, Larry},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - 2014 - Discussion â€œa significance test for the lassoâ€.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {501--508},
title = {{Discussion: â€œa significance test for the lassoâ€}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Efron,
abstract = {In the absence of relevant prior experience, popular Bayesian estimation techniques usually begin with some form of 'uninformative' prior distribution intended to have minimal infer-ential influence. The Bayes rule will still produce nice looking estimates and credible intervals, but these lack the logical force that is attached to experience-based priors and require further justification. The paper concerns the frequentist assessment of Bayes estimates. A simple for-mula is shown to give the frequentist standard deviation of a Bayesian point estimate. The same simulations as required for the point estimate also produce the standard deviation. Exponen-tial family models make the calculations particularly simple and bring in a connection to the parametric bootstrap.},
author = {Efron, Bradley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Efron - 2015 - Frequentist Accuracy of Bayesian Estimates.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Efron - 2015 - Frequentist Accuracy of Bayesian Estimates.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {Approximate bootstrap confidence intervals,General accuracy formula,Hierarchical and empirical Bayes,Markov chain Monte Carlo methods,Parametric bootstrap},
number = {3},
pages = {617--646},
title = {{Frequentist Accuracy of Bayesian Estimates}},
volume = {77},
year = {2015}
}
@article{Eddelbuettel2014,
author = {Eddelbuettel, Dirk and Sanderson, Conrad},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Eddelbuettel, Sanderson - 2014 - Rcpparmadillo Accelerating R with high-performance C linear algebra.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {c,linear algebra,r,software},
pages = {1054--1063},
title = {{Rcpparmadillo: Accelerating R with high-performance C++ linear algebra}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000492},
volume = {71},
year = {2014}
}
@article{MaalÃ¸e2015a,
abstract = {Deep generative models based upon continuous variational distributions parameterized by deep networks give state-of-the-art performance. In this paper we propose a framework for extending the latent representation with extra auxiliary variables in order to make the variational distribution more expressive for semi-supervised learning. By utilizing the stochasticity of the auxiliary variable we demonstrate how to train discriminative classifiers resulting in state-of-the-art performance within semi-supervised learning exemplified by an 0.96{\%} error on MNIST using 100 labeled data points. Furthermore we observe empirically that using auxiliary variables increases convergence speed suggesting that less expressive variational distributions, not only lead to looser bounds but also slower model training.},
archivePrefix = {arXiv},
arxivId = {1602.05473},
author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
eprint = {1602.05473},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maal{\o}e et al. - 2015 - Improving Semi-Supervised Learning with Auxiliary Deep Generative Models.pdf:pdf},
journal = {NIPS workshop},
pages = {1--5},
title = {{Improving Semi-Supervised Learning with Auxiliary Deep Generative Models}},
year = {2015}
}
@article{Nowak2008,
abstract = {When applying hierarchical clustering algorithms to cluster patient samples from microarray data, the clustering patterns generated by most algorithms tend to be dominated by groups of highly differentially expressed genes that have closely related expression patterns. Sometimes, these genes may not be relevant to the biological process under study or their functions may already be known. The problem is that these genes can potentially drown out the effects of other genes that are relevant or have novel functions. We propose a procedure called complementary hierarchical clustering that is designed to uncover the structures arising from these novel genes that are not as highly expressed. Simulation studies show that the procedure is effective when applied to a variety of examples. We also define a concept called relative gene importance that can be used to identify the influential genes in a given clustering. Finally, we analyze a microarray data set from 295 breast cancer patients, using clustering with the correlation-based distance measure. The complementary clustering reveals a grouping of the patients which is uncorrelated with a number of known prognostic signatures and significantly differing distant metastasis-free probabilities.},
author = {Nowak, Gen and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm046},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nowak, Tibshirani - 2008 - Complementary hierarchical clustering.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Computer Simulation,Female,Fuzzy Logic,Gene Expression,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Profiling: statistics {\&} numerical,Genetic Markers,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neoplasm Metastasis,Neoplasm Metastasis: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Principal Component Analysis,Reference Values},
month = {jul},
number = {3},
pages = {467--83},
pmid = {18093965},
title = {{Complementary hierarchical clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3294318{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@unpublished{Amasyali2009,
author = {Amasyali, M Fatih and Ersoy, Okan K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amasyali, Ersoy - 2009 - A Study of Meta Learning for Regression.pdf:pdf},
institution = {Purdue University},
title = {{A Study of Meta Learning for Regression}},
year = {2009}
}
@inproceedings{Bottou2011,
author = {Bottou, Leon and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems 24},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou, Bousquet - 2011 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {In Advances in Neural Information Processing Syste},
title = {{The Tradeoffs of Large-Scale Learning}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large+Scale+Learning{\&}ots=vbhayjhcGc{\&}sig=kWCMo7N51TgoLQSVSv2f{\_}ILArjo http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large-Scale+Learning{\&}ots=vbjaAkg8Fe{\&}sig=chdz7lCKXTFdUaLPYAgH{\_}FfgLmA},
year = {2011}
}
@inproceedings{Zhu2003,
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Ghahramani, Lafferty - 2003 - Semi-supervised learning using gaussian fields and harmonic functions.pdf:pdf},
pages = {912--919},
title = {{Semi-supervised learning using gaussian fields and harmonic functions}},
year = {2003}
}
@article{Zhou2003,
abstract = {We consider the general problem of hlearning from labelled and unlabelled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labelled and unlabelled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrated effective use of unlabelled data.},
author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Sch, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou et al. - 2003 - Learning with Local and Global Consistency.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {595--602},
title = {{Learning with Local and Global Consistency}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA41.pdf},
volume = {1},
year = {2003}
}
@inproceedings{Skurichina2002,
author = {Skurichina, Marina and Kuncheva, Ludmila I and Duin, Robert P.W.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Kuncheva, Duin - 2002 - Bagging and Boosting for the Nearest Mean Classifier Effects of Sample Size on Diversity and Accura.pdf:pdf},
pages = {62--71},
publisher = {Springer},
title = {{Bagging and Boosting for the Nearest Mean Classifier : Effects of Sample Size on Diversity and Accuracy}},
year = {2002}
}
@book{Lehmann1986,
author = {Lehmann, E.L.},
edition = {Second},
publisher = {Wiley},
title = {{Testing Statistical Hyptoheses}},
year = {1986}
}
@article{Bengio2009,
author = {Bengio, Yoshua},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
journal = {Foundations and trends{\textregistered} in Machine Learning},
title = {{Learning deep architectures for AI}},
url = {http://dl.acm.org/citation.cfm?id=1658424},
year = {2009}
}
@article{Johansson2009,
abstract = {Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, " Would this pa-tient have lower blood sugar had she received a different medication? " . We propose a new algo-rithmic framework for counterfactual inference which brings together ideas from domain adapta-tion and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1605.03661},
author = {Johansson, Fredrik D and Shalit, Uri and Sontag, David},
eprint = {1605.03661},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johansson, Shalit, Sontag - 2009 - Learning Representations for Counterfactual Inference.pdf:pdf},
title = {{Learning Representations for Counterfactual Inference}},
year = {2009}
}
@article{Betancourt2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01510v1},
author = {Betancourt, Michael},
eprint = {arXiv:1502.01510v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling.pdf:pdf},
journal = {Arxiv preprint arXiv:1502.01510v1},
title = {{The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling}},
year = {2015}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brodley - 1995 - Recursive automatic bias selection for classifier construction.pdf:pdf},
journal = {Machine Learning},
keywords = {automatic algorithm selection,decision trees,hybrid classifiers,inductive bias,learning from},
pages = {63--94},
title = {{Recursive automatic bias selection for classifier construction}},
url = {http://link.springer.com/article/10.1023/A:1022686102325},
volume = {94},
year = {1995}
}
@article{Yang2015b,
abstract = {The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates, guaranteeing geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions.},
archivePrefix = {arXiv},
arxivId = {1512.08269},
author = {Yang, Fanny and Balakrishnan, Sivaraman and Wainwright, Martin J},
eprint = {1512.08269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Balakrishnan, Wainwright - 2015 - Statistical and Computational Guarantees for the Baum-Welch Algorithm.pdf:pdf},
title = {{Statistical and Computational Guarantees for the Baum-Welch Algorithm}},
url = {http://arxiv.org/abs/1512.08269},
year = {2015}
}
@book{Berger1985,
author = {Berger, James O},
publisher = {Springer},
title = {{Statistical decision theory and Bayesian analysis}},
year = {1985}
}
@inproceedings{Cai2007,
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
booktitle = {IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408856},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, He, Han - 2007 - Semi-supervised Discriminant Analysis.pdf:pdf},
pages = {1--7},
title = {{Semi-supervised Discriminant Analysis}},
year = {2007}
}
@article{Duin2015,
author = {Duin, Robert P.W.},
doi = {10.1016/j.patrec.2015.04.015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 2015 - The dissimilarity representation for finding universals from particulars by an anti-essentialist approach.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Anti-essentialism,Generalization,Nearest Neighbor Rule,Representation},
publisher = {Elsevier Ltd.},
title = {{The dissimilarity representation for finding universals from particulars by an anti-essentialist approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001324},
year = {2015}
}
@article{Skurichina1996,
abstract = {In this paper the possibilities for constructing linear classifiers are considered for very small sample sizes. We propose a stability measure and present a study on the performance and stability of the following techniques: regularization by the Ridge-estimate of the covariance matrix [12], bootstrapping followed by aggregation (â€˜bagging', [9]) and editing combined with pseudo- inversion [8]. It is shown that by these techniques a smooth transition can be made between the nearest mean classifier and the Fisher discriminant based on large samples sizes. Especially for highly correlated data very good results are obtained compared with the nearest mean method.},
author = {Skurichina, Marina and Duin, Robert P W},
doi = {10.1109/ICPR.1996.547204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 1996 - Stabilizing classifiers for very small sample sizes.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {891--896},
title = {{Stabilizing classifiers for very small sample sizes}},
volume = {2},
year = {1996}
}
@article{Benavoli2015,
archivePrefix = {arXiv},
arxivId = {1505.02288v1},
author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca},
eprint = {1505.02288v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Benavoli, Corani, Mangili - 2015 - Should we really use post-hoc tests based on mean-ranks.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {friedman test,post-hoc test,statistical comparison},
pages = {1--10},
title = {{Should we really use post-hoc tests based on mean-ranks?}},
volume = {17},
year = {2015}
}
@article{Guo2010,
author = {Guo, Yuanyuan and Niu, Xiaoda and Zhang, Harry},
doi = {10.1109/ICDM.2010.66},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guo, Niu, Zhang - 2010 - An Extensive Empirical Study on Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {IEEE International Conference on Data Mining},
keywords = {-semi-supervised learning,bayesian classifiers},
month = {dec},
pages = {186--195},
publisher = {Ieee},
title = {{An Extensive Empirical Study on Semi-supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693972},
year = {2010}
}
@article{Kulesza2012,
abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
archivePrefix = {arXiv},
arxivId = {1207.6083},
author = {Kulesza, Alex and Taskar, Ben},
doi = {10.1561/2200000044},
eprint = {1207.6083},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:pdf},
isbn = {9781601986283},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {2-3},
pages = {123--286},
title = {{Determinantal Point Processes for Machine Learning}},
url = {http://arxiv.org/abs/1207.6083$\backslash$nhttp://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000044},
volume = {5},
year = {2012}
}
@article{Tu2015,
author = {Tu, Enmei and Yang, Jie and Kasabov, Nicola and Zhang, Yaqian},
doi = {10.1016/j.neucom.2015.01.020},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tu et al. - 2015 - Posterior Distribution Learning (PDL) A novel supervised learning framework using unlabeled samples to improve classi.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Posterior distribution learning,Supervised learning,Supervised manifold classification},
pages = {173--186},
publisher = {Elsevier},
title = {{Posterior Distribution Learning (PDL): A novel supervised learning framework using unlabeled samples to improve classification performance}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215000417},
volume = {157},
year = {2015}
}
@book{Little2002,
author = {Little, Roderick J. A. and Rubin, Donald B.},
booktitle = {Wiley, New York.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little, Rubin - 2002 - Statistical Analysis with Missing Data.pdf:pdf},
isbn = {3175723993},
title = {{Statistical Analysis with Missing Data}},
year = {2002}
}
@article{Zafar2015,
abstract = {Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead to user discrimination, even in the absence of intent. In this paper, we introduce fairness constraints, a mechanism to ensure fairness in a wide variety of classifiers in a principled manner. Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data. We then instantiate fairness constraints on three well-known classifiers -- logistic regression, hinge loss and support vector machines (SVM) -- and evaluate their performance in a real-world dataset with meaningful sensitive human attributes. Experiments show that fairness constraints allow for an optimal trade-off between accuracy and fairness.},
archivePrefix = {arXiv},
arxivId = {1507.05259},
author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
eprint = {1507.05259},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zafar et al. - 2015 - Fairness Constraints A Mechanism for Fair Classification.pdf:pdf},
title = {{Fairness Constraints: A Mechanism for Fair Classification}},
url = {http://arxiv.org/abs/1507.05259},
year = {2015}
}
@article{Pearl2009a,
author = {Pearl, Judea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - The Art and Science of Cause and Effect.pdf:pdf},
journal = {Cambridge University Press},
number = {November 1996},
pages = {401--428},
title = {{The Art and Science of Cause and Effect}},
year = {2009}
}
@article{Ferkingstad2014,
abstract = {As increasingly complex hypothesis-testing scenarios are considered in many scientific fields, analytic derivation of null distributions is often out of reach. To the rescue comes Monte Carlo testing, which may appear deceptively simple: as long as you can sample test statistics under the null hypothesis, the p-value is just the proportion of sampled test statistics that exceed the observed test statistic. Sampling test statistics is often simple once you have a Monte Carlo null model for your data, and defining some form of randomization procedure is also, in many cases, relatively straightforward. However, there may be several possible choices of randomization null model for the data, and no clear-cut criteria for choosing among them. Obviously, different null models may lead to very different p-values, and a very low p-value may thus occur due to the inadequacy of the chosen null model. It is preferable to use assumptions about the underlying random data generation process to guide selection of a null model. In many cases, we may order the null models by increasing preservation of the data characteristics, and we argue in this paper that this ordering in most cases gives increasing p-values, i.e. lower significance. We denote this as the null complexity principle. The principle gives a better understanding of the different null models and may guide in the choice between the different models.},
archivePrefix = {arXiv},
arxivId = {1404.5970},
author = {Ferkingstad, Egil and Holden, Lars and Sandve, Geir Kjetil},
eprint = {1404.5970},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ferkingstad, Holden, Sandve - 2014 - Monte Carlo null models for genomic data.pdf:pdf},
keywords = {Genomics,Hypothesis testing,Monte Carlo methods,and phrases,ge-,hypothesis testing,monte carlo methods},
pages = {1--20},
title = {{Monte Carlo null models for genomic data}},
url = {http://arxiv.org/abs/1404.5970},
year = {2014}
}
@article{Chapelle2006a,
address = {New York, New York, USA},
author = {Chapelle, Olivier and Chi, Mingmin and Zien, Alexander},
doi = {10.1145/1143844.1143868},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Chi, Zien - 2006 - A continuation method for semi-supervised SVMs.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {185--192},
publisher = {ACM Press},
title = {{A continuation method for semi-supervised SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143868},
year = {2006}
}
@inproceedings{Ho2001a,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2001 - Data Complexity Analysis for Classifier Combination.pdf:pdf},
pages = {53--67},
title = {{Data Complexity Analysis for Classifier Combination}},
year = {2001}
}
@article{Tsakonas2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02348v1},
author = {Tsakonas, Efthymios and Jald{\'{e}}n, Joakim and Member, Senior and Sidiropoulos, Nicholas D and Ottersten, Bjorn},
eprint = {arXiv:1506.02348v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tsakonas et al. - 2013 - Likelihood Estimation.pdf:pdf},
number = {22},
pages = {5704--5715},
title = {{Likelihood Estimation}},
volume = {61},
year = {2013}
}
@inproceedings{Klinkenberg2001,
author = {Klinkenberg, Ralf},
booktitle = {Workshop notes of the IJCAI-01 Workshop on Learning from Temporal and Spatial Data},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klinkenberg - 2001 - Using labeled and unlabeled data to learn drifting concepts.pdf:pdf},
pages = {16--24},
title = {{Using labeled and unlabeled data to learn drifting concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.1798{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@article{Xu2015,
author = {Xu, Xinxing and Li, Wen and Xu, Dong and Tsang, Ivor},
doi = {10.1109/TPAMI.2015.2476813},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu et al. - 2015 - Co-Labeling for Multi-view Weakly Labeled Learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {c},
pages = {1--1},
title = {{Co-Labeling for Multi-view Weakly Labeled Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7243351},
volume = {8828},
year = {2015}
}
@article{Janson2015,
author = {Janson, L. and Fithian, W. and Hastie, T. J.},
doi = {10.1093/biomet/asv019},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janson, Fithian, Hastie - 2015 - Effective degrees of freedom a flawed metaphor.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {May},
pages = {479--485},
title = {{Effective degrees of freedom: a flawed metaphor}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asv019},
year = {2015}
}
@article{Weston2005,
abstract = {MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
author = {Weston, Jason and Leslie, Christina and Ie, Eugene and Zhou, Dengyong and Elisseeff, Andre and Noble, William Stafford},
doi = {10.1093/bioinformatics/bti497},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2005 - Semi-supervised protein classification using cluster kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Pattern Recognition,Protein,Protein: methods,Proteins,Proteins: analysis,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = {aug},
number = {15},
pages = {3241--7},
pmid = {15905279},
title = {{Semi-supervised protein classification using cluster kernels.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905279},
volume = {21},
year = {2005}
}
@inproceedings{Jaakkola2002,
author = {Jaakkola, MST and Szummer, Martin},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Szummer - 2002 - Partially labeled classification with Markov random walks.pdf:pdf},
pages = {945--952},
title = {{Partially labeled classification with Markov random walks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA945{\&}dq=Partially+labeled+classification+with+Markov+random+walks{\&}ots=ZvP5J{\_}YBx6{\&}sig=dk27TWzUdp9G-e9OyvfYcGR14ro},
year = {2002}
}
@article{Lockhart2014d,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - A significance test for the lasso(3).pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {413--468},
title = {{A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Hothorn2015,
abstract = {We propose and study properties of maximum likelihood estimators in the class of conditional transformation models. Based on a suitable explicit parameterisation of the unconditional or conditional transformation function, we establish a cascade of increasingly complex transformation models that can be estimated, compared and analysed in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable can be set-up and estimated in the same theoretical and computational framework simply by choosing an appropriate transformation function and parameterisation thereof. The ability to evaluate the distribution function directly allows us to estimate models based on the exact full likelihood, especially in the presence of random censoring or truncation. For discrete and continuous responses, we establish the asymptotic normality of the proposed estimators. A reference software implementation of maximum likelihood-based estimation for conditional transformation models allowing the same flexibility as the theory developed here was employed to illustrate the wide range of possible applications.},
archivePrefix = {arXiv},
arxivId = {1508.06749},
author = {Hothorn, Torsten and M{\"{o}}st, Lisa and B{\"{u}}hlmann, Peter},
eprint = {1508.06749},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hothorn, M{\"{o}}st, B{\"{u}}hlmann - 2015 - Most Likely Transformations.pdf:pdf},
keywords = {censoring,conditional distribution function,conditional quantile function,distribution regression,transformation model,truncation},
title = {{Most Likely Transformations}},
url = {http://arxiv.org/abs/1508.06749},
year = {2015}
}
@article{Loog2016,
abstract = {Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets.},
archivePrefix = {arXiv},
arxivId = {1503.00269},
author = {Loog, Marco},
eprint = {1503.00269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2016 - Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {contrast,ear discriminant analysis,lin-,maximum likelihood,pessimism,semi-supervised learning},
number = {3},
pages = {462--475},
title = {{Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification}},
volume = {38},
year = {2016}
}
@techreport{Seeger2001,
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2001 - Learning with labeled and unlabeled data.pdf:pdf},
pages = {1--62},
title = {{Learning with labeled and unlabeled data}},
year = {2001}
}
@article{Poon2011,
abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs) and will present in this abstract.},
archivePrefix = {arXiv},
arxivId = {1202.3732},
author = {Poon, Hoifung and Domingos, Pedro},
doi = {10.1109/ICCVW.2011.6130310},
eprint = {1202.3732},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poon, Domingos - 2011 - Sum-product networks A new deep architecture.pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {689--690},
title = {{Sum-product networks: A new deep architecture}},
year = {2011}
}
@book{Chapelle2006,
author = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, Alexander},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Sch{\"{o}}lkopf, Zien - 2006 - Semi-supervised learning.pdf:pdf},
isbn = {9780262033589},
publisher = {MIT press},
title = {{Semi-supervised learning}},
year = {2006}
}
@inproceedings{Giraud-Carrier2005,
author = {Giraud-carrier, Christophe and Provost, Foster},
booktitle = {In Proceedings of the ICML-2005 Workshop on Meta-learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Provost - 2005 - Toward a justification of meta-learning Is the no free lunch theorem a show-stopper.pdf:pdf},
pages = {12--19},
title = {{Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper}},
url = {http://dml.cs.byu.edu/{~}cgc/pubs/ICML2005WS.pdf},
year = {2005}
}
@inproceedings{Zhang2000,
author = {Zhang, Tong},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2000 - The value of unlabeled data for classification problems.pdf:pdf},
pages = {1191--1198},
title = {{The value of unlabeled data for classification problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Carroll2007,
author = {Carroll, James L. and Seppi, Kevin D.},
booktitle = {IJCNN Workshop on Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carroll, Seppi - 2007 - No-free-lunch and Bayesian optimality.pdf:pdf},
title = {{No-free-lunch and Bayesian optimality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.7564{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@article{Suykens1999,
author = {Suykens, Johan A. K. and Vandewalle, J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suykens, Vandewalle - 1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
journal = {Neural Processing Letters},
keywords = {abbreviations,classification,linear least squares,radial basis,radial basis function kernel,rbf,support vector machines,svm,vapnik-chervonenkis,vc},
pages = {293--300},
title = {{Least Squares Support Vector Machine Classifiers}},
volume = {9},
year = {1999}
}
@article{Opper2001,
author = {Opper, Manfred},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Opper - 2001 - Learning to generalize.pdf:pdf},
journal = {Frontiers of Life 3},
pages = {763--775},
title = {{Learning to generalize}},
year = {2001}
}
@article{Erren2007,
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Erren, Bourne - 2007 - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = {may},
number = {5},
pages = {e102},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2007}
}
@unpublished{Loog2013,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2013 - Conservative Transductive and Semi-Supervised Empirical Risk Minimization.pdf:pdf},
pages = {1--9},
title = {{Conservative Transductive and Semi-Supervised Empirical Risk Minimization}},
year = {2013}
}
@article{Kullback1968,
author = {Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kullback - 1968 - Probability Densities with Given Marginals.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {1236--1243},
title = {{Probability Densities with Given Marginals}},
url = {http://www.jstor.org/stable/10.2307/2239692},
volume = {39},
year = {1968}
}
@article{Rooyen,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00083v1},
author = {Rooyen, Brendan Van and Williamson, Robert C},
eprint = {arXiv:1504.00083v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rooyen, Williamson - Unknown - A Theory of Feature Learning.pdf:pdf},
title = {{A Theory of Feature Learning}}
}
@article{Isaksson2008,
author = {Isaksson, Anders and Wallman, M. and Goransson, H. and Gustafsson, Mats G},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
month = {oct},
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Chan1997,
author = {Chan, Philip K. and Stolfo, Salvatore J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chan, Stolfo - 1997 - On the accuracy of meta-learning for scalable data mining.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
title = {{On the accuracy of meta-learning for scalable data mining}},
url = {http://www.springerlink.com/index/M27133K052552242.pdf},
year = {1997}
}
@article{Breiman2001,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling+:+The+Two+Cultures{\#}2 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling:+The+Two+Cultures{\#}2},
volume = {16},
year = {2001}
}
@article{Ghahramani2002,
author = {Ghahramani, Z},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani - 2002 - Graphical models parameter learning.pdf:pdf},
keywords = {BRAIN, learning, MODEL, models, Theories},
pages = {1--305},
title = {{Graphical models: parameter learning}},
url = {http://discovery.ucl.ac.uk/185880/},
volume = {1},
year = {2002}
}
@phdthesis{Hamers2012,
author = {Hamers, Adrian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamers - 2012 - The Evolution of Coeval Stellar Hierarchical Triple Systems.pdf:pdf},
school = {Utrecht University},
title = {{The Evolution of Coeval Stellar Hierarchical Triple Systems}},
year = {2012}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, Leon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Large scale transductive SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large scale transductive SVMs}},
volume = {7},
year = {2006}
}
@inproceedings{Dasgupta2002,
author = {Dasgupta, Sanjoy and Littman, Michael L. and McAlles},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta, Littman, McAlles - 2002 - PAC generalization bounds for co-training.pdf:pdf},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=PGrlRWV5-v0C{\&}oi=fnd{\&}pg=PA375{\&}dq=PAC+Generalization+Bounds+for+Co-training{\&}ots=auaN1CGPip{\&}sig=0dID1oXJYgeENxwSzfsntvwz{\_}oU},
year = {2002}
}
@article{Zhang2016a,
abstract = {We propose a general semi-supervised inference framework focused on the estimation of the population mean. We consider both the ideal semi-supervised setting where infinitely many unlabeled samples are available, as well as the ordinary semi-supervised setting in which only a finite number of unlabeled samples is available. As usual in semi-supervised settings, there exists an unlabeled sample of covariate vectors and a labeled sample consisting of covariate vectors along with real-valued responses ("labels"). Otherwise the formulation is "assumption-lean" in that no major conditions are imposed on the statistical or functional form of the data. Estimators are proposed along with corresponding confidence intervals for the population mean. Theoretical analysis on both the asymptotic behavior and {\$}\backslashell{\_}2{\$}-risk for the proposed procedures are given. Surprisingly, the proposed estimators, based on a simple form of the least squares method, outperform the ordinary sample mean. The method is further extended to a nonparametric setting, in which the oracle rate can be achieved asymptotically. The proposed estimators are further illustrated by simulation studies and a real data example involving estimation of the homeless population.},
archivePrefix = {arXiv},
arxivId = {1606.07268},
author = {Zhang, Anru and Brown, Lawrence D. and Cai, T. Tony},
eprint = {1606.07268},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Brown, Cai - 2016 - Semi-supervised Inference General Theory and Estimation of Means.pdf:pdf},
keywords = {confidence interval,department of statistics,efficiency,estimation of mean,limiting distribution,madison,pa 19104,philadelphia,semi-,supervised inference,the wharton school,university of pennsylvania,university of wisconsin-madison,wi},
title = {{Semi-supervised Inference: General Theory and Estimation of Means}},
url = {http://arxiv.org/abs/1606.07268},
year = {2016}
}
@article{Shimodaira2015,
abstract = {We derive an information criterion for selecting a parametric model of complete-data distribution when only incomplete or partially observed data is available. Compared with AIC, the new criterion has an additional penalty term for missing data expressed by the Fisher information matrices of complete data and incomplete data. We prove that the new criterion is an asymptotically unbiased estimator of the expected Kullback-Leibler divergence between the true distribution and the estimated distribution for complete data, whereas AIC is that for the incomplete data. Information criteria PDIO (Shimodaira 1994) and AICcd (Cavanaugh and Shumway 1998) have been previously proposed for the same purpose. Recently, an error is found in the derivation of PDIO, and the new information criterion is obtained by correcting the error. The additional penalty for missing data turns out to be only the half of what is claimed in PDIO. Geometrical view of alternating minimizations of the EM algorithm plays an important role for the derivation.},
archivePrefix = {arXiv},
arxivId = {1509.02870},
author = {Shimodaira, Hidetoshi and Maeda, Haruyoshi},
eprint = {1509.02870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shimodaira, Maeda - 2015 - An information criterion for model selection with missing data via complete-data divergence.pdf:pdf},
keywords = {akaike information criterion,alternating projections,and phrases,data,divergence,em algorithm,fisher information matrix,incomplete data,kullback-leibler,manifold,misspecification,takeuchi information criterion},
title = {{An information criterion for model selection with missing data via complete-data divergence}},
url = {http://arxiv.org/abs/1509.02870},
year = {2015}
}
@article{Poggio2003,
author = {Poggio, Tomaso and Smale, Steve},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio, Smale - 2003 - The Mathematics of Learning Dealing with Data.pdf:pdf},
journal = {Notices of the AMS},
pages = {537--544},
title = {{The Mathematics of Learning: Dealing with Data}},
year = {2003}
}
@article{Reif2012,
author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
doi = {10.1007/s10044-012-0280-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reif et al. - 2012 - Automatic classifier selection for non-experts.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {classifier recommendation,classifier selection,landmarking,meta-features,meta-learning,regression},
month = {jul},
title = {{Automatic classifier selection for non-experts}},
url = {http://www.springerlink.com/index/10.1007/s10044-012-0280-z},
year = {2012}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {incomplete,likelihood,maximum},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
volume = {39},
year = {1977}
}
@article{Vovk2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06254v1},
author = {Vovk, Vladimir},
eprint = {arXiv:1502.06254v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vovk - 2015 - The fundamental nature of the log loss function arXiv 1502 . 06254v1 cs . LG 22 Feb 2015.pdf:pdf},
pages = {1--6},
title = {{The fundamental nature of the log loss function arXiv : 1502 . 06254v1 [ cs . LG ] 22 Feb 2015}},
year = {2015}
}
@article{Maltoni2015,
abstract = {Recent works demonstrated the usefulness of temporal coherence to regularize supervised training or to learn invariant features with deep architectures. In particular, enforcing a smooth output change while presenting temporally-closed frames from video sequences, proved to be an effective strategy. In this paper we prove the efficacy of temporal coherence for semi-supervised incremental tuning. We show that a deep architecture, just mildly trained in a supervised manner, can progressively improve its classification accuracy, if exposed to video sequences of unlabeled data. The extent to which, in some cases, a semi-supervised tuning allows to improve classification accuracy (approaching the supervised one) is somewhat surprising. A number of control experiments pointed out the fundamental role of temporal coherence.},
archivePrefix = {arXiv},
arxivId = {1511.03163},
author = {Maltoni, Davide and Lomonaco, Vincenzo},
eprint = {1511.03163},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maltoni, Lomonaco - 2015 - Semi-supervised Tuning from Temporal Coherence.pdf:pdf},
number = {2005},
pages = {1--15},
title = {{Semi-supervised Tuning from Temporal Coherence}},
url = {http://arxiv.org/abs/1511.03163},
year = {2015}
}
@article{Sivaganesan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02689v1},
author = {Berger, James O. and Bernardo, Jose M. and Sun, Dongchu},
doi = {10.1214/14-BA935},
eprint = {arXiv:1504.02689v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berger, Bernardo, Sun - 2015 - Overall Objective Priors.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Joint Reference Prior,Logarithmic Divergence,Mul,joint reference prior,logarithmic divergence,multinomial model,objective priors,reference analysis},
number = {1},
pages = {189--221},
title = {{Overall Objective Priors}},
url = {http://projecteuclid.org/euclid.ba/1422556417},
year = {2015}
}
@inproceedings{Cortes2004,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2004 - AUC optimization vs. error rate minimization.pdf:pdf},
pages = {313--320},
title = {{AUC optimization vs. error rate minimization}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA313{\&}dq=AUC+Optimization+vs+.+Error+Rate+Minimization{\&}ots=TGKup{\_}Ra93{\&}sig=VTdv-C5TW9itNMlz43YJjmxRKAc},
year = {2004}
}
@book{Barber2012,
author = {Barber, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
title = {{Bayesian reasoning and machine learning}},
year = {2012}
}
@article{Li2013,
author = {Li, YF and Tsang, IW and Kwok, JT and Zhou, ZH},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Convex and Scalable Weakly Labeled SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2151--2188},
title = {{Convex and Scalable Weakly Labeled SVMs}},
url = {http://arxiv.org/abs/1303.1271},
volume = {14},
year = {2013}
}
@article{Ma2008,
abstract = {In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.},
author = {Ma, Shuangge and Huang, Jian},
doi = {10.1093/bib/bbn027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Huang - 2008 - Penalized feature selection and classification in bioinformatics.pdf:pdf},
isbn = {1477-4054},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics application,Feature selection,Penalization},
number = {5},
pages = {392--403},
pmid = {18562478},
title = {{Penalized feature selection and classification in bioinformatics}},
volume = {9},
year = {2008}
}
@inproceedings{Graepel2013,
author = {Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
booktitle = {Information Security and Cryptology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graepel, Lauter, Naehrig - 2013 - ML confidential Machine learning on encrypted data.pdf:pdf},
title = {{ML confidential: Machine learning on encrypted data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37682-5{\_}1},
year = {2013}
}
@article{Dawid2007,
abstract = {A decision problem is defined in terms of an outcome space, an action space and a loss function. Starting from these simple ingredients, we can construct: Proper Scoring Rule; Entropy Function; Divergence Function; Riemannian Metric; and Unbiased Estimating Equation. From an abstract viewpoint, the loss function defines a duality between the outcome and action spaces, while the correspondence between a distribution and its Bayes act induces a self-duality. Together these determine a "decision geometry" for the family of distributions on outcome space. This allows generalisation of many standard statistical concepts and properties. In particular we define and study generalised exponential families. Several examples are analysed, including a general Bregman geometry.},
author = {Dawid, A. P.},
doi = {10.1007/s10463-006-0099-8},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dawid - 2007 - The geometry of proper scoring rules.pdf:pdf},
issn = {00203157},
journal = {Annals of the Institute of Statistical Mathematics},
keywords = {Bregman geometry,Decision geometry,Generalised exponential family,Information geometry,Proper scoring rule,Unbiased estimating equation},
number = {1},
pages = {77--93},
title = {{The geometry of proper scoring rules}},
volume = {59},
year = {2007}
}
@article{Jager2013a,
abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported {\$}P{\$}-values as the data. We then collect {\$}P{\$}-values from the abstracts of all 77â€‰430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported {\$}P{\$}-values. We estimate that the overall rate of false discoveries among reported results is 14{\%} (s.d. 1{\%}), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5{\%} more false positives (FP) per year, {\$}P = 0.18{\$}) or with respect to journal submissions (0.5{\%} more FP per 100 submissions, {\$}P = 0.12{\$}). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
author = {Jager, Leah R and Leek, Jeffrey T},
doi = {10.1093/biostatistics/kxt007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jager, Leek - 2013 - An estimate of the science-wise false discovery rate and application to the top medical literature.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {false discovery rate,genomics,meta-analysis,multiple testing,science-wise false discovery rate},
month = {sep},
pages = {1--12},
pmid = {24068246},
title = {{An estimate of the science-wise false discovery rate and application to the top medical literature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24068246},
year = {2013}
}
@inproceedings{Sokolovska2008,
address = {Helsinki, Finland},
author = {Sokolovska, Nataliya and Capp{\'{e}}, Olivier and Yvon, Francois},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska, Capp{\'{e}}, Yvon - 2008 - The asymptotics of semi-supervised learning in discriminative probabilistic models.pdf:pdf},
pages = {984--991},
publisher = {ACM Press},
title = {{The asymptotics of semi-supervised learning in discriminative probabilistic models}},
year = {2008}
}
@article{Lockhart2014f,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - Rejoinder A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {518--531},
title = {{Rejoinder: A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@unpublished{Grunwald2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3730v1},
author = {Gr{\"{u}}nwald, Peter and van Ommen, Thijs},
eprint = {arXiv:1412.3730v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Ommen - 2014 - Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It.pdf:pdf},
pages = {1--70},
title = {{Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It}},
year = {2014}
}
@inproceedings{Brefeld2006,
author = {Brefeld, Ulf and G{\"{a}}rtner, Thomas and Scheffer, Tobias and Wrobel, Stefan},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brefeld et al. - 2006 - Efficient co-regularised least squares regression.pdf:pdf},
pages = {137--144},
title = {{Efficient co-regularised least squares regression}},
url = {http://dl.acm.org/citation.cfm?id=1143862},
year = {2006}
}
@article{Bengio2010,
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
journal = {Computational Intelligence},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.2010.00366.x/full},
volume = {26},
year = {2010}
}
@article{Kawakita2014a,
abstract = {We are interested in developing a safe semi-supervised learning that works in any situation. Semi-supervised learning postulates that n(') unlabeled data are available in addition to n labeled data. However, almost all of the previous semi-supervised methods require additional assumptions (not only unlabeled data) to make improvements on supervised learning. If such assumptions are not met, then the methods possibly perform worse than supervised learning. Sokolovska, Capp{\'{e}}, and Yvon (2008) proposed a semi-supervised method based on a weighted likelihood approach. They proved that this method asymptotically never performs worse than supervised learning (i.e., it is safe) without any assumption. Their method is attractive because it is easy to implement and is potentially general. Moreover, it is deeply related to a certain statistical paradox. However, the method of Sokolovska et al. (2008) assumes a very limited situation, i.e., classification, discrete covariates, n(')â†’âˆž and a maximum likelihood estimator. In this paper, we extend their method by modifying the weight. We prove that our proposal is safe in a significantly wide range of situations as long as nâ‰¤n('). Further, we give a geometrical interpretation of the proof of safety through the relationship with the above-mentioned statistical paradox. Finally, we show that the above proposal is asymptotically safe even when n('){\textless}n by modifying the weight. Numerical experiments illustrate the performance of these methods.},
author = {Kawakita, Masanori and Takeuchi, Jun'ichi Jun'ichi},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood(2).pdf:pdf},
journal = {Neural Networks},
keywords = {semi-supervised learning},
month = {may},
pages = {146--64},
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
volume = {53},
year = {2014}
}
@article{Bartlett2006,
author = {Bartlett, Peter L and Jordan, Michael I. and McAuliffe, Jon D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Jordan, McAuliffe - 2006 - Convexity, Classification, and Risk Bounds.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {boosting,convex optimization},
number = {473},
pages = {138--156},
title = {{Convexity, Classification, and Risk Bounds}},
volume = {101},
year = {2006}
}
@article{Tolstikhin2014,
archivePrefix = {arXiv},
arxivId = {1411.7200},
author = {Tolstikhin, Ilya and Blanchard, Gilles and Kloft, Marius},
eprint = {1411.7200},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Blanchard, Kloft - 2014 - Localized Complexities for Transductive Learning.pdf:pdf},
keywords = {centration inequalities,con-,empirical processes,fast rates,kernel classes,localized complexities,statistical learning,transductive learning},
month = {nov},
pages = {1--28},
title = {{Localized Complexities for Transductive Learning}},
url = {http://arxiv.org/abs/1411.7200v1},
volume = {35},
year = {2014}
}
@article{Buhmann2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.0375v1},
author = {Buhmann, Joachim M},
eprint = {arXiv:1006.0375v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhmann - 2010 - Information theoretic model validation for clustering.pdf:pdf},
number = {X},
title = {{Information theoretic model validation for clustering}},
volume = {2010},
year = {2010}
}
@article{Peng2002,
author = {Peng, Yonghong and Flach, Peter A. and Soares, Carlos and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peng et al. - 2002 - Improved dataset characterisation for meta-learning.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {141--152},
title = {{Improved dataset characterisation for meta-learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-36182-0{\_}14},
volume = {2534},
year = {2002}
}
@inproceedings{Ben-David2006,
author = {Ben-David, Shai and Luxburg, Ulrike Von and P{\'{a}}l, David},
booktitle = {Proceedings of the 19th Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Luxburg, P{\'{a}}l - 2006 - A sober look at clustering stability.pdf:pdf},
number = {2002},
pages = {5--19},
title = {{A sober look at clustering stability}},
url = {http://link.springer.com/chapter/10.1007/11776420{\_}4},
year = {2006}
}
@article{Balsubramani,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01811v1},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {arXiv:1503.01811v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - Unknown - Optimally Combining Classifiers Using Unlabeled Data.pdf:pdf},
title = {{Optimally Combining Classifiers Using Unlabeled Data}}
}
@article{Smola2003,
abstract = {We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.},
author = {Smola, Aj Alexander J and Kondor, Risi},
doi = {10.1007/978-3-540-45167-9_12},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Kondor - 2003 - Kernels and Regularization on Graphs.pdf:pdf},
isbn = {3540407200},
issn = {03029743},
journal = {Machine Learning},
pages = {1--15},
title = {{Kernels and Regularization on Graphs}},
url = {http://www.springerlink.com/index/H96KDX90DCMM6FX0.pdf$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}12},
volume = {2777},
year = {2003}
}
@inproceedings{Herbrich2006a,
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
title = {{TrueSkill: A Bayesian Skill Rating System}},
year = {2006}
}
@article{VanderMaaten2008,
author = {van der Maaten, L.J.P. and Hinton, G.E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Maaten, Hinton - 2008 - Visualizing High-Dimensional Data Using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
volume = {9},
year = {2008}
}
@article{Hand2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0606441v1},
author = {Hand, David J.},
doi = {10.1214/088342306000000060},
eprint = {0606441v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2006 - Classifier Technology and the Illusion of Progress.pdf:pdf},
isbn = {0883423060000},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,empirical com-,error rate,flat maximum effect,lectivity bias,misclas-,population drift,principle of parsimony,problem uncertainty,se-,sification rate,simplicity,supervised classification},
month = {feb},
number = {1},
pages = {1--14},
primaryClass = {arXiv:math},
title = {{Classifier Technology and the Illusion of Progress}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1149600839/},
volume = {21},
year = {2006}
}
@inproceedings{Walt2007,
author = {Walt, Christiaan Van Der and Barnard, Etienne},
booktitle = {18th Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Walt, Barnard - 2007 - Measures for the characterisation of pattern-recognition data sets.pdf:pdf},
title = {{Measures for the characterisation of pattern-recognition data sets}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/1979},
year = {2007}
}
@article{Cvpr2016,
archivePrefix = {arXiv},
arxivId = {1604.07093},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1604.07093},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cvpr, Id - 2016 - Semi-supervised Vocabulary-informed Learning.pdf:pdf},
title = {{Semi-supervised Vocabulary-informed Learning}},
year = {2016}
}
@inproceedings{Grunwald2000,
author = {Gr{\"{u}}nwald, Peter},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald - 2000 - Maximum entropy and the glasses you are looking through.pdf:pdf},
pages = {238--246},
title = {{Maximum entropy and the glasses you are looking through}},
url = {http://dl.acm.org/citation.cfm?id=2073975},
year = {2000}
}
@inproceedings{Joachims1999,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 16th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 1999 - Transductive inference for text classification using support vector machines.pdf:pdf},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers},
title = {{Transductive inference for text classification using support vector machines}},
year = {1999}
}
@inproceedings{Kulesza2010,
abstract = {We present a novel probabilistic model for distributions over sets of structuresâ€” for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of},
author = {Kulesza, Alex and Taskar, Ben},
booktitle = {Advances in Neural Information Processing Systems 23},
doi = {10.1080/00036840500405656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2010 - Structured Determinantal Point Processes.pdf:pdf},
isbn = {0003684050040},
issn = {{\textless}null{\textgreater}},
pages = {1--9},
title = {{Structured Determinantal Point Processes}},
year = {2010}
}
@article{Reid2011,
abstract = {We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.},
archivePrefix = {arXiv},
arxivId = {0901.0356},
author = {Reid, Mark D. and Williamson, Robert C.},
eprint = {0901.0356},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {731--817},
title = {{Information, divergence and risk for binary experiments}},
url = {http://arxiv.org/abs/0901.0356 http://dl.acm.org/citation.cfm?id=2021029},
volume = {12},
year = {2011}
}
@article{Baldassarre2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1303.3207v3},
author = {Baldassarre, Luca and Bhan, Nirav},
eprint = {arXiv:1303.3207v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baldassarre, Bhan - 2013 - Group-Sparse Model Selection Hardness and Relaxations.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--18},
title = {{Group-Sparse Model Selection: Hardness and Relaxations}},
url = {http://arxiv.org/abs/1303.3207},
year = {2013}
}
@inproceedings{Ratsaby1995,
author = {Ratsaby, Joel and Venkatesht, Santosh S.},
booktitle = {Proceedings of the 8th Annual conference on Computational learning theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ratsaby, Venkatesht - 1995 - Learning from a mixture of labeled and unlabeled examples with parametric side information.pdf:pdf},
pages = {412--417},
title = {{Learning from a mixture of labeled and unlabeled examples with parametric side information}},
url = {http://dl.acm.org/citation.cfm?id=225348},
year = {1995}
}
@article{Nock2009,
abstract = {Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same "master" algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.},
author = {Nock, Richard and Nielsen, Frank},
doi = {10.1109/TPAMI.2008.225},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nock, Nielsen - 2009 - Bregman divergences and surrogates for learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Decision Support Techniques,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {nov},
number = {11},
pages = {2048--59},
pmid = {19762930},
title = {{Bregman divergences and surrogates for learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19762930},
volume = {31},
year = {2009}
}
@article{King1995,
author = {King, R.D. and Feng, C and Sutherland, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/King, Feng, Sutherland - 1995 - Statlog comparison of classification algorithms on large real-world problems.pdf:pdf},
journal = {Applied Artificial Intelligence an International Journal},
number = {3},
pages = {289--333},
title = {{Statlog: comparison of classification algorithms on large real-world problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839519508945477},
volume = {9},
year = {1995}
}
@article{Lin2011a,
abstract = {We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.},
author = {Lin, Hui and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Bilmes - 2011 - A Class of Submodular Functions for Document Summarization(2).pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {510--520},
title = {{A Class of Submodular Functions for Document Summarization}},
url = {http://ssli.ee.washington.edu/people/hlin/papers/lin-acl11-summ.pdf},
volume = {1},
year = {2011}
}
@article{Taddy2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02312v1},
author = {Taddy, Matt and Chen, Chun-sheng and Yun, Jun},
eprint = {arXiv:1502.02312v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taddy, Chen, Yun - 2001 - Bayesian and Empirical Bayesian Forests.pdf:pdf},
number = {1},
title = {{Bayesian and Empirical Bayesian Forests}},
year = {2001}
}
@article{Lehmann1993,
author = {Lehmann, EL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lehmann - 1993 - The Fisher, Neyman-Pearson theories of testing hypotheses One theory or two.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson theories of testing hypotheses: One theory or two?}},
url = {http://www.jstor.org/stable/10.2307/2291263},
volume = {88},
year = {1993}
}
@inproceedings{Wolpert2002,
author = {Wolpert, David H},
booktitle = {Soft Computing and Industry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf:pdf},
pages = {25--42},
title = {{The Supervised Learning No-Free-Lunch Theorems}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0123-9{\_}3},
year = {2002}
}
@inproceedings{Ogawa2013,
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ogawa et al. - 2013 - Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines.pdf:pdf},
pages = {897--905},
title = {{Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}sugi/2013/ICML2013b.pdf},
year = {2013}
}
@inproceedings{Jordan2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation - which is borne out in repeated experiments - that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
author = {Jordan, Michael I. and Ng, Andrew Y},
booktitle = {Advances in neural information processing systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan, Ng - 2002 - On Discriminative vs. Generative classifiers comparison of logistic regression and naive Bayes.pdf:pdf},
number = {14},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: comparison of logistic regression and naive Bayes}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA841{\&}dq=On+Discriminative+vs.+Generative+classifiers:+comparison+of+logistic+regression+and+naive+Bayes{\&}ots=ZvO0F2{\_}vx9{\&}sig=0nMLd-CWMsb8-jyrI6YetIH6ZZU},
volume = {2},
year = {2002}
}
@article{Wang2013,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Jebara, Chang - 2013 - Semi-Supervised Learning Using Greedy Max-Cut.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {771--800},
title = {{Semi-Supervised Learning Using Greedy Max-Cut}},
url = {http://www.ee.columbia.edu/ln/dvmm/publications/13/ggmc{\_}13.pdf},
volume = {14},
year = {2013}
}
@article{Smola2005,
abstract = {We present methods for dealing with missing variables in the context
  of Gaussian Processes and Support Vector Machines. This solves an
  important problem which has largely been ignored by kernel methods:
  How to systematically deal with incomplete data? Our method can also
  be applied to problems with partially observed labels as well as to
  the transductive setting where we view the labels as missing data.
  
  Our approach relies on casting kernel methods as an estimation
  problem in exponential families. Hence, estimation with missing
  variables becomes a problem of computing marginal distributions, and
  finding efficient optimization methods. To that extent we propose an
  optimization scheme which extends the Concave Convex Procedure (CCP)
  of Yuille and Rangarajan, and present a simplified and intuitive
  proof of its convergence. We show how our algorithm can be
  specialized to various cases in order to efficiently solve the
  optimization problems that arise. Encouraging preliminary
  experimental results on the USPS dataset are also presented.},
author = {Smola, Alex and Vishwanathan, S V N and Hoffman, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Vishwanathan, Hoffman - 2005 - Kernel Methods for Missing Variables.pdf:pdf},
isbn = {097273581X},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
title = {{Kernel Methods for Missing Variables}},
url = {http://eprints.pascal-network.org/archive/00002053/},
year = {2005}
}
@article{Krahenbuhl2011,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Krahenbuhl, Philipp and Koltun, Vladlen and KrÂ¨ahenbÂ¨uhl, Philipp and Koltun, Vladlen and Krahenbuhl, Philipp},
eprint = {1210.5644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Krahenbuhl et al. - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop//Krahenbuhl et al. - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24 (Proceedings of NIPS)},
keywords = {conditional random field,filtering,message passing,sampling,segmentation},
number = {4},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
year = {2011}
}
@article{Belot2013a,
abstract = {Schervish (1985b) showed that every forecasting system is noncalibrated for uncountably many data sequences that it might see. This result is strengthened here: from a topological point of view, failure of calibration is typical and calibration rare. Meanwhile, Bayesian forecasters are certain that they are calibrated-this invites worries about the connection between Bayesianism and rationality. ?? 2013 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.4943v1},
author = {Belot, Gordon},
doi = {10.1016/j.spl.2013.06.024},
eprint = {arXiv:1306.4943v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belot - 2013 - Failure of calibration is typical.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Banach-Mazur game,Bayesianism,Calibration},
number = {10},
pages = {2316--1318},
title = {{Failure of calibration is typical}},
volume = {83},
year = {2013}
}
@book{Lehmann1998,
author = {Lehmann, E. L. and Casella, G.},
publisher = {Springer-Verlag},
title = {{Theory of Point Estimation}},
year = {1998}
}
@article{Ma2015,
author = {Ma, Jianping and Jiang, Jin},
doi = {10.1016/j.net.2014.12.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Jiang - 2015 - Semisupervised classification for fault diagnosis in nuclear power plants.pdf:pdf},
issn = {17385733},
journal = {Nuclear Engineering and Technology},
number = {2},
pages = {176--186},
publisher = {Elsevier B.V},
title = {{Semisupervised classification for fault diagnosis in nuclear power plants}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1738573315000054},
volume = {47},
year = {2015}
}
@article{Kalousis1999,
author = {Kalousis, Alexis and Theoharis, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Theoharis - 1999 - NOEMON An intelligent Assistant for Classifier Selection.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {classifier comparison,classifier selection,dataset morphology,multidimensional metrics},
number = {5},
pages = {319--337},
title = {{NOEMON: An intelligent Assistant for Classifier Selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.7762},
volume = {3},
year = {1999}
}
@article{Gomez-Chova2008,
abstract = {This letter presents a semisupervised method based on kernel machines and graph theory for remote sensing image classification. The support vector machine (SVM) is regularized with the unnormalized graph Laplacian, thus leading to the Laplacian SVM (LapSVM). The method is tested in the challenging problems of urban monitoring and cloud screening, in which an adequate exploitation of the wealth of unlabeled samples is critical. Results obtained using different sensors, and with low number of training samples, demonstrate the potential of the proposed LapSVM for remote sensing image classification.},
author = {G{\'{o}}mez-Chova, Luis and Camps-Valls, Gustavo and Mu{\~{n}}oz-Mari, Jordi and Calpe, Javier},
doi = {10.1109/LGRS.2008.916070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/G{\'{o}}mez-Chova et al. - 2008 - Semisupervised image classification with Laplacian support vector machines.pdf:pdf},
isbn = {1545-598X},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Kernel methods,Manifold learning,Regularization,Semisupervised learning (SSL),Support vector machines (SVMs)},
number = {3},
pages = {336--340},
title = {{Semisupervised image classification with Laplacian support vector machines}},
volume = {5},
year = {2008}
}
@article{Balcan2005,
abstract = {Semi Supervised Learning;},
author = {Balcan, M.F. and Blum, a. and Choi, P.P. and Lafferty, J. and Pantano, B. and Rwebangira, M.R. and Zhu, X.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2005 - Person identification in webcam images An application of semi-supervised learning.pdf:pdf},
journal = {ICML 2005 Workshop on Learning with Partially Classified Training Data},
pages = {6},
title = {{Person identification in webcam images: An application of semi-supervised learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.1706{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2005}
}
@unpublished{DeDeo2014,
abstract = {A recurring problem with statistical prediction for policy-making is that many useful variables are associated with others on which it would be ethically problematic to base decisions. This problem becomes particularly acute in the Big Data era, when predictions are often made in the absence of strong theories for the underlying causal mechanisms. Given this, we show how to use information theory to construct the distribution closest in predictive power to the full distribution, but in which predictions---and thus policy outcomes, provision of services, and so forth---are not correlated with protected variables.},
archivePrefix = {arXiv},
arxivId = {1412.4643},
author = {DeDeo, Simon},
eprint = {1412.4643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/DeDeo - 2014 - Wrong side of the tracks Big Data and Protected Categories.pdf:pdf},
month = {dec},
pages = {3},
title = {{"Wrong side of the tracks": Big Data and Protected Categories}},
url = {http://arxiv.org/abs/1412.4643},
year = {2014}
}
@article{Zhang2004a,
author = {Zhang, Tong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2004 - Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {56--134},
title = {{Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization}},
url = {http://www.jstor.org/stable/10.2307/3448494},
volume = {32},
year = {2004}
}
@article{Yang2015a,
abstract = {Fractional imputation (FI) is a relatively new method of imputation for handling item nonresponse in survey sampling. In FI, several imputed values with their fractional weights are created for each missing item. Each fractional weight represents the conditional probability of the imputed value given the observed data, and the parameters in the conditional probabilities are often computed by an iterative method such as EM algorithm. The underlying model for FI can be fully parametric, semiparametric, or nonparametric, depending on plausibility of assumptions and the data structure. In this paper, we give an overview of FI, introduce key ideas and methods to readers who are new to the FI literature, and highlight some new development. We also provide guidance on practical implementation of FI and valid inferential tools after imputation. We demonstrate the empirical performance of FI with respect to multiple imputation using a pseudo finite population generated from a sample in Monthly Retail Trade Survey in US Census Bureau.},
archivePrefix = {arXiv},
arxivId = {1508.06945},
author = {Yang, Shu and Kim, Jae Kwang},
eprint = {1508.06945},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Kim - 2015 - Fractional Imputation in Survey Sampling A Comparative Review.pdf:pdf},
keywords = {and phrases,carlo em,item nonresponse,missing at random,monte,multiple imputation,synthetic imputation},
title = {{Fractional Imputation in Survey Sampling: A Comparative Review}},
url = {http://arxiv.org/abs/1508.06945},
volume = {02115},
year = {2015}
}
@article{Janzing2013,
author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1214/13-AOS1145},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing et al. - 2013 - Quantifying causal influences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {oct},
number = {5},
pages = {2324--2358},
title = {{Quantifying causal influences}},
url = {http://projecteuclid.org/euclid.aos/1383661266},
volume = {41},
year = {2013}
}
@inproceedings{Ho2008,
author = {Ho, Tin Kam},
booktitle = {Proceedings of the 2008 Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89689-0{\_}102},
year = {2008}
}
@article{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
month = {jun},
pages = {1--9},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@article{Gelman2013,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
doi = {10.1111/j.2044-8317.2011.02037.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:pdf},
issn = {2044-8317},
journal = {The British journal of mathematical and statistical psychology},
month = {feb},
number = {1},
pages = {8--38},
pmid = {22364575},
title = {{Philosophy and the practice of Bayesian statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22364575},
volume = {66},
year = {2013}
}
@article{Tuia,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02338v1},
author = {Tuia, Devis and Member, Senior and Camps-valls, Gustau and Member, Senior},
eprint = {arXiv:1504.02338v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tuia et al. - Unknown - Kernel Manifold Alignment.pdf:pdf},
pages = {1--6},
title = {{Kernel Manifold Alignment}}
}
@article{Azizyan2013,
author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
doi = {10.1214/13-AOS1092},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Azizyan, Singh, Wasserman - 2013 - Density-sensitive semisupervised inference.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {751--771},
title = {{Density-sensitive semisupervised inference}},
url = {http://projecteuclid.org/euclid.aos/1368018172},
volume = {41},
year = {2013}
}
@inproceedings{Kuncheva2001,
author = {Kuncheva, Ludmila I and Roli, Fabio and Marcialis, Gian Luca and Shipp, Catherine A.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2001 - Complexity of Data Subsets Generated by the Random Subspace Method An Experimental Investigation.pdf:pdf},
pages = {349--358},
title = {{Complexity of Data Subsets Generated by the Random Subspace Method: An Experimental Investigation}},
year = {2001}
}
@techreport{Bensusan2000,
author = {Bensusan, H. and Giraud-carrier, Christophe and Kennedy, C.J.},
booktitle = {ILP Work-in-progress {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bensusan, Giraud-carrier, Kennedy - 2000 - A Higher-order Approach to Meta-learning.pdf:pdf},
institution = {University of Bristol},
title = {{A Higher-order Approach to Meta-learning}},
url = {http://137.222.102.8/Publications/Papers/1000471.pdf},
year = {2000}
}
@article{Ranganath2013,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ranganath, Gerrish, Blei - 2013 - Black Box Variational Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1401.0118},
title = {{Black Box Variational Inference}},
url = {http://arxiv.org/abs/1401.0118},
year = {2013}
}
@article{Ho2004,
abstract = {Studies on ensemble methods for classification suffer from the difficulty of modeling the complementary strengths of the components. Kleinberg's theory of stochastic discrimination (SD) addresses this rigorously via mathematical notions of enrichment, uniformity, and projectability of an ensemble. We explain these concepts via a very simple numerical example that captures the basic principles of the SD theory and method. We focus on a fundamental symmetry in point set covering that is the key observation leading to the foundation of the theory. We believe a better understanding of the SD method will lead to developments of better tools for analyzing other ensemble methods.},
archivePrefix = {arXiv},
arxivId = {cs/0402021},
author = {Ho, Tin Kam},
eprint = {0402021},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2004 - A Numerical Example on the Principles of Stochastic Discrimination.pdf:pdf},
primaryClass = {cs},
title = {{A Numerical Example on the Principles of Stochastic Discrimination}},
url = {http://arxiv.org/abs/cs/0402021},
year = {2004}
}
@article{Sun2010,
author = {Sun, Shiliang and Shawe-taylor, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Shawe-taylor - 2010 - Sparse Semi-supervised Learning Using Conjugate Functions.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fenchel-legendre conjugate,multi-,representer theorem,semi-supervised learning,statistical learning theory,support vector machine,view regularization},
pages = {2423--2455},
title = {{Sparse Semi-supervised Learning Using Conjugate Functions}},
volume = {11},
year = {2010}
}
@unpublished{Savov,
author = {Savov, Ivan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Savov - Unknown - Linear algebra explained in four pages.pdf:pdf},
pages = {1--4},
title = {{Linear algebra explained in four pages}}
}
@article{Kuncheva2003,
author = {Kuncheva, Ludmila I and Whitaker, Christopher J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva, Whitaker - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
journal = {Machine Learning},
keywords = {committee of learners,dependency and diversity,multiple classifiers ensemble,pattern recognition},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
volume = {51},
year = {2003}
}
@inproceedings{Foulds2011,
author = {Foulds, James and Smyth, Padhraic},
booktitle = {SIAM International Conference on Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foulds, Smyth - 2011 - Multi-instance mixture models and semi-supervised learning.pdf:pdf},
number = {Mi},
title = {{Multi-instance mixture models and semi-supervised learning}},
url = {http://siam.omnibooksonline.com/2011datamining/data/papers/256.pdf},
year = {2011}
}
@article{Anand2013,
abstract = {Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function.We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.},
author = {Anand, Saket and Mittal, Sushil and Tuzel, Oncel and Meer, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Anand et al. - 2013 - Semi-Supervised Kernel Mean Shift Clustering.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {sep},
number = {6},
pages = {1201--1215},
pmid = {24101327},
title = {{Semi-Supervised Kernel Mean Shift Clustering.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24101327},
volume = {36},
year = {2013}
}
@article{Gelman1999,
abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be difficult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artefacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artefacts. We present examples from Bayesian modelling, but, as we explain, the artefacts occur generally.},
author = {Gelman, Andrew and Price, Phlllip N.},
doi = {10.1002/(SICI)1097-0258(19991215)18:23<3221::AID-SIM312>3.0.CO;2-M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Price - 1999 - All maps of parameter estimates are misleading.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
pages = {3221--3234},
pmid = {10602147},
title = {{All maps of parameter estimates are misleading}},
volume = {18},
year = {1999}
}
@article{Schafer2002,
author = {Schafer, Joseph L. and Graham, John W.},
doi = {10.1037//1082-989X.7.2.147},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schafer, Graham - 2002 - Missing data Our view of the state of the art.pdf:pdf},
issn = {1082-989X},
journal = {Psychological Methods},
number = {2},
pages = {147--177},
title = {{Missing data: Our view of the state of the art.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.2.147},
volume = {7},
year = {2002}
}
@article{Dietterich1998,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1998 - Approximate statistical tests for comparing supervised classification learning algorithms.pdf:pdf},
journal = {Neural computation},
title = {{Approximate statistical tests for comparing supervised classification learning algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
year = {1998}
}
@article{Qiu2013,
abstract = {A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a high-rank structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separation between the subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, called Robust Sparse Subspace Clustering, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.},
archivePrefix = {arXiv},
arxivId = {1309.2074},
author = {Qiu, Qiang and Sapiro, Guillermo},
eprint = {1309.2074},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Qiu, Sapiro - 2013 - Learning Transformations for Clustering and Classification.pdf:pdf},
pages = {187--225},
title = {{Learning Transformations for Clustering and Classification}},
url = {http://arxiv.org/abs/1309.2074},
volume = {16},
year = {2013}
}
@article{Hussami2013,
abstract = {We propose a new sparse regression method called the component lasso, based on a simple idea. The method uses the connected-components structure of the sample covariance matrix to split the problem into smaller ones. It then applies the lasso to each subproblem separately, obtaining a coefficient vector for each one. Then, it uses non-negative least squares to recombine the different vectors into a single so- lution. This step is useful in selecting and reweighting components that are correlated with the response. Simulated and real data examples show that the component lasso can outperform standard regression methods such as the lasso and elastic net, achieving a lower mean squared error as well as better support recovery. Keywords.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.4472v2},
author = {Hussami, Nadine and Tibshirani, Robert},
eprint = {arXiv:1311.4472v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hussami, Tibshirani - 2013 - A Component Lasso.pdf:pdf},
journal = {arXiv preprint arXiv:1311.4472},
keywords = {connected components,elastic net,graphical lasso,grouping effect,lasso,negative least squares,sparsity},
number = {2},
pages = {1--19},
title = {{A Component Lasso}},
url = {http://arxiv.org/abs/1311.4472},
year = {2013}
}
@inproceedings{Nigam2000a,
author = {Nigam, Kamal and Ghani, R},
booktitle = {Proceedings of the 9th International Conference on Information and Knowledge Management},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training.pdf:pdf},
isbn = {1581133200},
keywords = {a related set of,blum and mitchell 1,for example,in problem domains where,into,present,research uses labeled and,the features naturally divide,two disjoint sets,unlabeled data},
pages = {86--93},
title = {{Analyzing the effectiveness and applicability of co-training}},
url = {http://dl.acm.org/citation.cfm?id=354805},
year = {2000}
}
@article{Jayasumana2015,
abstract = {In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.},
archivePrefix = {arXiv},
arxivId = {1412.0265},
author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
doi = {10.1109/TPAMI.2015.2414422},
eprint = {1412.0265},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jayasumana et al. - 2015 - Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {10,1109,2015,2414422,a future issue of,accepted for publication in,analysis and machine intelligence,but has not been,citation information,content may change prior,doi,fully edited,ieee transactions on pattern,s article has been,this journal,to final publication,tpami},
number = {c},
pages = {1--1},
title = {{Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7063231},
volume = {8828},
year = {2015}
}
@article{Wainwright2008,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@book{Spirtes2000,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
booktitle = {Journal of Chemical Information and Modeling},
edition = {Second},
eprint = {arXiv:1011.1669v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spirtes, Glymour, Scheines - 2000 - Causation, Prediction, and Search.pdf:pdf},
isbn = {0-262-19440-6},
issn = {1098-6596},
number = {9},
pmid = {25246403},
title = {{Causation, Prediction, and Search}},
volume = {53},
year = {2000}
}
@article{ONeill1978,
author = {O'Neill, Terence J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/O'Neill - 1978 - Normal discrimination with unclassified observations.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {unclassified},
number = {364},
pages = {821--826},
title = {{Normal discrimination with unclassified observations}},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480106},
volume = {73},
year = {1978}
}
@inproceedings{Krijthe2016a,
author = {Krijthe, Jesse Hendrik and Loog, Marco},
booktitle = {International Conference on Pattern Recognition (To Appear)},
title = {{Optimistic Semi-supervised Least Squares Classification}},
year = {2016}
}
@article{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
eprint = {1506.03431},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:pdf},
pages = {1--22},
title = {{Automatic Variational Inference in Stan}},
url = {http://arxiv.org/abs/1506.03431},
year = {2015}
}
@unpublished{Liu2014,
author = {Liu, Mingxia and Zhang, Daoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Zhang - 2014 - CGS A Novel Pairwise Constraint-Guided Sparse Feature Selection Method.pdf:pdf},
title = {{CGS: A Novel Pairwise Constraint-Guided Sparse Feature Selection Method}},
year = {2014}
}
@article{Zou2014,
abstract = {In epigenome-wide association studies, cell-type composition often differs between cases and controls, yielding associations that simply tag cell type rather than reveal fundamental biology. Current solutions require actual or estimated cell-type composition--information not easily obtainable for many samples of interest. We propose a method, FaST-LMM-EWASher, that automatically corrects for cell-type composition without the need for explicit knowledge of it, and then validate our method by comparison with the state-of-the-art approach. Corresponding software is available from http://www.microsoft.com/science/.},
author = {Zou, James and Lippert, Christoph and Heckerman, David and Aryee, Martin and Listgarten, Jennifer},
doi = {10.1038/nmeth.2815},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zou et al. - 2014 - Epigenome-wide association studies without the need for cell-type composition.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7105},
journal = {Nature methods},
keywords = {Cells,Epigenomics,Genome-Wide Association Study,Humans,Linear Models},
number = {3},
pages = {309--11},
pmid = {24464286},
title = {{Epigenome-wide association studies without the need for cell-type composition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24464286},
volume = {11},
year = {2014}
}
@inproceedings{Auger2007,
author = {Auger, Anne and Teytaud, Olivier},
booktitle = {Proceedings of the 9th annual conference on Genetic and evolutionary computation},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Auger, Teytaud - 2007 - Continuous lunches are free!.pdf:pdf},
isbn = {9781595936974},
keywords = {free-lunch,kolmogorov,no-free-lunch,s extension theo-},
pages = {916--922},
title = {{Continuous lunches are free!}},
url = {http://dl.acm.org/citation.cfm?id=1277145},
year = {2007}
}
@article{Behl2014,
abstract = {The performance of binary classification tasks, such as action classification and object detection, is often measured in terms of the average precision (AP). Yet it is common practice in computer vision to employ the support vector machine (SVM) classifier, which optimizes a surrogate 0-1 loss. The popularity of SVM can be attributed to its empirical performance. Specifically, in fully supervised settings, SVM tends to provide similar accuracy to the AP-SVM classifier, which directly optimizes an AP-based loss. However, we hypothesize that in the significantly more challenging and practically useful setting of weakly supervised learning, it becomes crucial to optimize the right accuracy measure. In order to test this hypothesis, we propose a novel latent AP-SVM that minimizes a carefully designed upper bound on the AP-based loss function over weakly supervised samples. Using publicly available datasets, we demonstrate the advantage of our approach over standard loss-based binary classifiers on two challenging problems: action classification and character recognition.},
author = {Behl, Aseem and Jawahar, C.V. and Kumar, M. Pawan},
doi = {10.1109/CVPR.2014.133},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Behl, Jawahar, Kumar - 2014 - Optimizing Average Precision Using Weakly Supervised Data.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
number = {12},
pages = {1011--1018},
title = {{Optimizing Average Precision Using Weakly Supervised Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909529},
volume = {37},
year = {2014}
}
@article{Wanga,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01782v1},
author = {Wang, Xiangyu and Leng, Chenlei},
eprint = {arXiv:1506.01782v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Leng - Unknown - High-dimensional Ordinary Least-squares Projection for Screening Variables.pdf:pdf},
keywords = {consistency,forward regression,generalized inverse,high dimensionality,lasso,marginal correlation,moore-penrose inverse,ordinary least squares,screening,sure independent,variable selection},
pages = {1--49},
title = {{High-dimensional Ordinary Least-squares Projection for Screening Variables}}
}
@article{Tolstikhin2016a,
abstract = {Transductive learning considers a training set of {\$}m{\$} labeled samples and a test set of {\$}u{\$} unlabeled samples, with the goal of best labeling that particular test set. Conversely, inductive learning considers a training set of {\$}m{\$} labeled samples drawn iid from {\$}P(X,Y){\$}, with the goal of best labeling any future samples drawn iid from {\$}P(X){\$}. This comparison suggests that transduction is a much easier type of inference than induction, but is this really the case? This paper provides a negative answer to this question, by proving the first known minimax lower bounds for transductive, realizable, binary classification. Our lower bounds show that {\$}m{\$} should be at least {\$}\backslashOmega(d/\backslashepsilon + \backslashlog(1/\backslashdelta)/\backslashepsilon){\$} when {\$}\backslashepsilon{\$}-learning a concept class {\$}\backslashmathcal{\{}H{\}}{\$} of finite VC-dimension {\$}d{\textless}\backslashinfty{\$} with confidence {\$}1-\backslashdelta{\$}, for all {\$}m \backslashleq u{\$}. This result draws three important conclusions. First, general transduction is as hard as general induction, since both problems have {\$}\backslashOmega(d/m){\$} minimax values. Second, the use of unlabeled data does not help general transduction, since supervised learning algorithms such as ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring the unlabeled test set. Third, our transductive lower bounds imply lower bounds for semi-supervised learning, which add to the important discussion about the role of unlabeled data in machine learning.},
archivePrefix = {arXiv},
arxivId = {1602.03027},
author = {Tolstikhin, Ilya and Lopez-Paz, David},
eprint = {1602.03027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Lopez-Paz - 2016 - Minimax Lower Bounds for Realizable Transductive Classification.pdf:pdf},
keywords = {binary classification,minimax lower bounds,realizable learning,transductive learning},
title = {{Minimax Lower Bounds for Realizable Transductive Classification}},
url = {http://arxiv.org/abs/1602.03027},
year = {2016}
}
@inproceedings{Chapelle2002,
author = {Chapelle, Olivier and Weston, Jason and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Weston, Sch{\"{o}}lkopf - 2002 - Cluster kernels for semi-supervised learning.pdf:pdf},
pages = {585--592},
title = {{Cluster kernels for semi-supervised learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA13.pdf},
year = {2002}
}
@article{Wang2009a,
author = {Wang, Fei and Wang, Xin and Li, Tao},
doi = {10.1109/CVPR.2009.5206675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Wang, Li - 2009 - Beyond the graphs Semi-parametric semi-supervised discriminant analysis.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {2113--2120},
publisher = {Ieee},
title = {{Beyond the graphs: Semi-parametric semi-supervised discriminant analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206675},
year = {2009}
}
@inproceedings{Cortes1993,
author = {Cortes, Corinna and Jackel, L.D.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Jackel - 1993 - Learning Cuves Asymptotic Values and Rate of Convergence.pdf:pdf},
pages = {327--334},
title = {{Learning Cuves: Asymptotic Values and Rate of Convergence}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Cuves:+Asymptotic+Values+and+Rate+of+Convergence{\#}0},
year = {1993}
}
@article{Huang2013,
abstract = {Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.},
author = {Huang, Xiaolin and Shi, Lei and Suykens, Johan a K},
doi = {D7CF84C8-DF7E-492B-A669-0B4CFDEE5D3D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Shi, Suykens - 2013 - Support Vector Machine Classifier with Pinball Loss.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {sep},
number = {5},
pages = {984--997},
pmid = {24062537},
title = {{Support Vector Machine Classifier with Pinball Loss.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24062537},
volume = {36},
year = {2013}
}
@article{Poggio2004,
abstract = {Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.},
author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
doi = {10.1038/nature02341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2004 - General conditions for predictivity in learning theory.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Algorithms,Intelligence,Language,Learning,Learning: physiology,Models, Theoretical,Probability,Research Design},
month = {mar},
number = {6981},
pages = {419--22},
pmid = {15042089},
title = {{General conditions for predictivity in learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15042089},
volume = {428},
year = {2004}
}
@unpublished{Graves2014,
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
month = {oct},
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401v1},
year = {2014}
}
@inproceedings{Macia2009,
author = {Macia, Nuria and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Hybrid Artificial Intelligence Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia, Orriols-puig, Bernad{\'{o}}-Mansilla - 2009 - Beyond Homemade Artificial Data Sets.pdf:pdf},
keywords = {artificial data sets,data complexity,machine learning},
pages = {605--612},
title = {{Beyond Homemade Artificial Data Sets}},
url = {http://www.springerlink.com/index/N23720WL67U355MV.pdf http://link.springer.com/chapter/10.1007/978-3-642-02319-4{\_}73},
year = {2009}
}
@manual{RCoreTeam2016,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2016}
}
@inproceedings{Aha1992,
author = {Aha, David W.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Aha - 1992 - Generalizing from case studies A case study.pdf:pdf},
title = {{Generalizing from case studies: A case study}},
year = {1992}
}
@book{MacKay2008,
author = {MacKay, David},
doi = {10.1109/PES.2004.1373296.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/MacKay - 2008 - Sustainable Energy-without the hot air.pdf:pdf},
isbn = {9780954452933},
issn = {00029505},
title = {{Sustainable Energy-without the hot air}},
url = {http://www.dspace.cam.ac.uk/handle/1810/217849 https://www.repository.cam.ac.uk/handle/1810/217849},
year = {2008}
}
@article{Robert2016,
abstract = {This note is made of comments on Watson and Holmes (2016) and about their proposals towards more robust decisions. While we acknowledge and commend the authors for setting new and all-encompassing principles of Bayesian robustness, we remain uncertain as to which extent such principles can be applied outside binary decision. We also wonder at the ultimate relevance of Kullback-Leibler neighbourhoods to characterise robustness.},
archivePrefix = {arXiv},
arxivId = {1603.09088},
author = {Robert, Christian P. and Rousseau, Judith},
eprint = {1603.09088},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert, Rousseau - 2016 - Some comments about James Watson's and Chris Holmes' Approximate Models and Robust Decisions.pdf:pdf},
keywords = {1,and phrases,decision-theory,decision-theory, prior selection, robust methodolo,first-hand,introduction,misspecification,ology,prior selection,robust method-,there is nothing like},
pages = {1--7},
title = {{Some comments about James Watson's and Chris Holmes' "Approximate Models and Robust Decisions"}},
url = {http://arxiv.org/abs/1603.09088},
year = {2016}
}
@inproceedings{Kim2014,
author = {Kim, Do-kyum and Der, Matthew and Saul, Lawrence K.},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kim, Der, Saul - 2014 - A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data.pdf:pdf},
title = {{A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data}},
url = {http://jmlr.org/proceedings/papers/v33/kim14a.pdf},
volume = {33},
year = {2014}
}
@article{Vanschoren2012,
author = {Vanschoren, Joaquin and Blockeel, Hendrik and Pfahringer, Bernhard and Holmes, Geoffrey},
doi = {10.1007/s10994-011-5277-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vanschoren et al. - 2012 - Experiment databases.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jan},
number = {2},
pages = {127--158},
title = {{Experiment databases}},
url = {http://www.springerlink.com/index/10.1007/s10994-011-5277-0},
volume = {87},
year = {2012}
}
@article{Nigam2000,
author = {Nigam, Kamal and McCallum, Andrew and Kachites and Thrun, Sebastian and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam et al. - 2000 - Text classification from labeled and unlabeled documents using EM.pdf:pdf},
journal = {Machine learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
pages = {1--34},
title = {{Text classification from labeled and unlabeled documents using EM}},
volume = {34},
year = {2000}
}
@article{Subramanya2011a,
author = {Subramanya, A and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2011 - Semi-supervised learning with measure propagation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph-based semi-supervised learning,large-scale semi-supervised,learning,non-parametric models,transductive inference},
pages = {3311--3370},
title = {{Semi-supervised learning with measure propagation}},
url = {http://dl.acm.org/citation.cfm?id=2078212},
volume = {12},
year = {2011}
}
@article{Seeger2009,
abstract = {Here, I review facts that are most probably known, namely that the information gain criterion used to drive experimental design in a linear-Gaussian model is submodular, so that a well-known approximation guarantee holds for the sequential greedy algorithm. The criterion is equal to a certain mutual information, which is not submodular in general. I point out the high potential relevance of obtaining approximation guarantees for nonlinear experimental design as well.},
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2009 - On the submodularity of linear experimental design.pdf:pdf},
journal = {Experimental Design},
pages = {1--3},
title = {{On the submodularity of linear experimental design}},
url = {http://infoscience.epfl.ch/record/175483},
year = {2009}
}
@article{Hong2015a,
author = {Hong, Yi and Zhu, Weiping},
doi = {10.1016/j.patrec.2015.06.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification(2).pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Co-training,Image classif,Image classification,Semi-supervised learning},
pages = {59--65},
publisher = {Elsevier Ltd.},
title = {{Spatial Co-Training for Semi-Supervised Image Classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001816},
volume = {63},
year = {2015}
}
@article{Chapelle2007,
abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
author = {Chapelle, Olivier},
doi = {10.1162/neco.2007.19.5.1155},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle - 2007 - Training a support vector machine in the primal.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated},
month = {may},
number = {5},
pages = {1155--78},
pmid = {17381263},
title = {{Training a support vector machine in the primal.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381263},
volume = {19},
year = {2007}
}
@book{Casella2002,
author = {Casella, George and Berger, Roger L.},
edition = {Second},
title = {{Statistical Inference}},
year = {2002}
}
@article{Janzing2015,
abstract = {We postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law. We argue that this links thermodynamics and causal inference. On the one hand, it entails behaviour that is similar to the usual arrow of time. On the other hand, it motivates a statistical asymmetry between cause and effect that has recently postulated in the field of causal inference, namely, that the probability distribution P(cause) contains no information about the conditional distribution P(effect|cause) and vice versa, while P(effect) may contain information about P(cause|effect).},
archivePrefix = {arXiv},
arxivId = {1512.02057},
author = {Janzing, Dominik and Chaves, Rafael and Schoelkopf, Bernhard},
eprint = {1512.02057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing, Chaves, Schoelkopf - 2015 - Algorithmic independence of initial condition and dynamical law in thermodynamics and causal infere.pdf:pdf},
pages = {1--11},
title = {{Algorithmic independence of initial condition and dynamical law in thermodynamics and causal inference}},
url = {http://arxiv.org/abs/1512.02057},
year = {2015}
}
@article{Magliacane2016,
abstract = {Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.},
archivePrefix = {arXiv},
arxivId = {1606.07035},
author = {Magliacane, Sara and Claassen, Tom and Mooij, Joris M.},
eprint = {1606.07035},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Magliacane, Claassen, Mooij - 2016 - Ancestral Causal Inference.pdf:pdf},
number = {Nips},
title = {{Ancestral Causal Inference}},
url = {http://arxiv.org/abs/1606.07035},
year = {2016}
}
@article{Doksum2007,
author = {Doksum, Kjell and Ozeki, Akichika and Kim, Jihoon and {Chaibub Neto}, Elias},
doi = {10.1016/j.spl.2007.03.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doksum et al. - 2007 - Thinking outside the box Statistical inference based on Kullbackâ€“Leibler empirical projections.pdf:pdf},
issn = {01677152},
journal = {Statistics {\&} Probability Letters},
keywords = {bootstrap,box-cox transformation,classification,covariate,k-l divergence,klep,outside the box,sandwich formula},
month = {jul},
number = {12},
pages = {1201--1213},
title = {{Thinking outside the box: Statistical inference based on Kullbackâ€“Leibler empirical projections}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715207000843},
volume = {77},
year = {2007}
}
@article{Pearl2009,
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Structural equation models, confounding, graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@article{Pearson1926,
author = {Pearson, Karl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearson - 1926 - Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population.pdf:pdf},
journal = {Proceedings of the Royal Society of London. Series A},
number = {760},
pages = {1--14},
title = {{Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population}},
volume = {112},
year = {1926}
}
@article{Kucukelbir2014,
abstract = {Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference. We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.},
archivePrefix = {arXiv},
arxivId = {1411.0292},
author = {Kucukelbir, Alp and Blei, David M.},
eprint = {1411.0292},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kucukelbir, Blei - 2014 - Profile Predictive Inference.pdf:pdf},
month = {nov},
pages = {8},
title = {{Profile Predictive Inference}},
url = {http://arxiv.org/abs/1411.0292},
year = {2014}
}
@article{Liu,
author = {Liu, Anqi and Ziebart, Brian D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Ziebart - Unknown - Robust Classification Under Sample Selection Bias.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Ziebart - Unknown - Robust Classification Under Sample Selection Bias(2).pdf:pdf},
pages = {1--9},
title = {{Robust Classification Under Sample Selection Bias}}
}
@article{Berthold2008,
archivePrefix = {arXiv},
arxivId = {1601.02213},
author = {Berthold, Michael R and H{\"{o}}ppner, Frank},
eprint = {1601.02213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berthold, H{\"{o}}ppner - 2008 - On Clustering Time Series Using Euclidean Distance and Pearson Correlation.pdf:pdf},
title = {{On Clustering Time Series Using Euclidean Distance and Pearson Correlation}},
year = {2008}
}
@misc{Tibshirani,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - Unknown - Machine Learning vs. Statistics.pdf:pdf},
title = {{Machine Learning vs. Statistics}}
}
@article{Scott2009,
author = {Scott, Clayton and Blanchard, Gilles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Scott, Blanchard - 2009 - Novelty detection Unlabeled data definitely help.pdf:pdf},
pages = {464--471},
title = {{Novelty detection: Unlabeled data definitely help}},
url = {http://eprints.pascal-network.org/archive/00004475/},
volume = {5},
year = {2009}
}
@article{Senn2011,
author = {Senn, Stephen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn - 2011 - You may believe you are a Bayesian but you are probably wrong.pdf:pdf},
journal = {Rationality, Markets and Morals},
pages = {48--66},
title = {{You may believe you are a Bayesian but you are probably wrong}},
url = {http://www.rmm-journal.com/downloads/Article{\_}Senn.pdf},
volume = {2},
year = {2011}
}
@inproceedings{Pfahringer2000,
author = {Pfahringer, Bernhard and Giraud-carrier, Christophe},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pfahringer, Giraud-carrier - 2000 - Meta-Learning by Landmarking Various Learning Algorithms.pdf:pdf},
pages = {743--750},
title = {{Meta-Learning by Landmarking Various Learning Algorithms}},
year = {2000}
}
@article{Hanczar2010a,
abstract = {The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = {mar},
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@inproceedings{Zhou2007a,
author = {Zhou, Zhi-hua and Xu, Jun-Ming},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Xu - 2007 - On the relation between multi-instance learning and semi-supervised learning.pdf:pdf},
number = {1997},
title = {{On the relation between multi-instance learning and semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1273643},
year = {2007}
}
@article{Jain1999a,
author = {Jain, A.K. and Murty, M.N. and Flynn, P.J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
journal = {ACM computing surveys (CSUR)},
number = {3},
title = {{Data clustering: a review}},
url = {http://dl.acm.org/citation.cfm?id=331504},
volume = {31},
year = {1999}
}
@article{Huang1998,
author = {Huang, Jianhua Z.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,anova,curse of dimensionality,finite elements,interaction,least},
number = {1},
pages = {242--272},
title = {{Projection estimation in multiple regression with application to functional ANOVA models}},
url = {http://projecteuclid.org/euclid.aos/1030563984},
volume = {26},
year = {1998}
}
@article{Culp2008,
author = {Culp, Mark and Michailidis, George},
doi = {10.1198/106186008X344748},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting(2).pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
month = {sep},
number = {3},
pages = {545--571},
title = {{An iterative algorithm for extending learners to a semi-supervised setting}},
volume = {17},
year = {2008}
}
@article{Haffari2012,
author = {Haffari, Gholamreza and Sarkar, Anoop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haffari, Sarkar - 2012 - Analysis of semi-supervised learning with the yarowsky algorithm.pdf:pdf},
journal = {arXiv preprint},
title = {{Analysis of semi-supervised learning with the yarowsky algorithm}},
url = {http://arxiv.org/abs/1206.5240},
year = {2012}
}
@article{Rendell1990,
author = {Rendell, Larry and Cho, Howard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rendell, Cho - 1990 - Empirical learning as a function of concept character.pdf:pdf},
journal = {Machine Learning},
keywords = {concepts as functions,empirical concept learning,experimental studies},
pages = {267--298},
title = {{Empirical learning as a function of concept character}},
url = {http://www.springerlink.com/index/K5311727465WLH07.pdf},
volume = {5},
year = {1990}
}
@article{Marcum2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02914v1},
author = {Marcum, Christopher Steven},
eprint = {arXiv:1512.02914v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marcum - 2015 - Yet Another Statistical Analysis of Bob Ross Paintings.pdf:pdf},
keywords = {art,bob ross,linear subspace,paintings},
pages = {1--16},
title = {{Yet Another Statistical Analysis of Bob Ross Paintings}},
year = {2015}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andre},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halevy, Norvig, Pereira - 2009 - The Unreasonable Effectiveness of Data.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
month = {mar},
number = {2},
pages = {8--12},
title = {{The Unreasonable Effectiveness of Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804817},
volume = {24},
year = {2009}
}
@article{Powers2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.07994v1},
author = {Powers, Scott and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1214/15-AOAS866},
eprint = {arXiv:1601.07994v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Powers, Hastie, Tibshirani - 2015 - Customized training with an application to mass spectrometric imaging of cancer tissue.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {Transductive learning, local regression, classific},
number = {4},
pages = {1709--1725},
title = {{Customized training with an application to mass spectrometric imaging of cancer tissue}},
url = {http://projecteuclid.org/euclid.aoas/1453993091},
volume = {9},
year = {2015}
}
@inproceedings{Rasmus2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02672v1},
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1507.02672v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmus et al. - 2015 - Semi-Supervised Learning with Ladder Network.pdf:pdf},
pages = {3532--3540},
title = {{Semi-Supervised Learning with Ladder Network}},
year = {2015}
}
@article{Yuille2003,
abstract = {The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.},
author = {Yuille, a L and Rangarajan, Anand},
doi = {10.1162/08997660360581958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yuille, Rangarajan - 2003 - The concave-convex procedure.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Energy Metabolism,Neural Networks (Computer)},
month = {apr},
number = {4},
pages = {915--36},
pmid = {12689392},
title = {{The concave-convex procedure.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12689392},
volume = {15},
year = {2003}
}
@article{Lv2014,
author = {Lv, Jinchi and Zheng, Zemin},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv, Zheng - 2014 - Discussion â€œa significance test for the lassoâ€.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {493--500},
title = {{Discussion: â€œa significance test for the lassoâ€}},
volume = {42},
year = {2014}
}
@article{Jung2008,
abstract = {In recent years, there has been a growing interest among researchers in the use of latent class and growth mixture modeling techniques for applications in the social and psychological sciences, in part due to advances in and availability of computer software designed for this purpose (e.g., Mplus and SAS Proc Traj). Latent growth modeling approaches, such as latent class growth analysis (LCGA) and growth mixture modeling (GMM), have been increasingly recognized for their usefulness for identifying homogeneous subpopulations within the larger heterogeneous population and for the identification of meaningful groups or classes of individuals. The purpose of this paper is to provide an overview of LCGA and GMM, compare the different techniques of latent growth modeling, discuss current debates and issues, and provide readers with a practical guide for conducting LCGA and GMM using the Mplus software.},
author = {Jung, Tony and Wickrama, K. A.},
doi = {10.1111/j.1751-9004.2007.00054.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jung, Wickrama - 2008 - An introduction to latent class growth analysis and growth mixture modeling.pdf:pdf},
isbn = {1751-9004},
issn = {1751-9004},
journal = {Social and Personality Psychology Compass},
number = {1},
pages = {302--317},
title = {{An introduction to latent class growth analysis and growth mixture modeling}},
url = {http://doi.wiley.com/10.1111/j.1751-9004.2007.00054.x$\backslash$nhttp://onlinelibrary.wiley.com/doi/10.1111/j.1751-9004.2007.00054.x/full},
volume = {2},
year = {2008}
}
@book{Jaynes,
author = {Jaynes, E.T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaynes - 2003 - Probability Theory The Logic of Science.pdf:pdf},
title = {{Probability Theory: The Logic of Science}},
year = {2003}
}
@article{Zhang2014,
author = {Zhang, Kai and Lan, Liang and Kwok, James T. and Vucetic, Slobodan and Parvin, Bahram},
doi = {10.1109/TNNLS.2014.2315526},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2014 - Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--1},
title = {{Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6803073},
year = {2014}
}
@article{Balsubramania,
author = {Balsubramani, Akshay},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani - Unknown - Scalable Semi-Supervised Aggregation of Classifiers.pdf:pdf},
pages = {1--9},
title = {{Scalable Semi-Supervised Aggregation of Classifiers}}
}
@article{McLachlan1982,
author = {McLachlan, Geoffrey J. and Ganesalingam, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan, Ganesalingam - 1982 - Updating a discriminant function on the basis of unclassified data.pdf:pdf},
journal = {Communication in Statistics- Simulation and Computation},
title = {{Updating a discriminant function on the basis of unclassified data}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918208812293},
year = {1982}
}
@article{Geer2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0910.0722v1},
author = {Geer, Sara Van De and B{\"{u}}hlmann, Peter},
eprint = {arXiv:0910.0722v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geer, B{\"{u}}hlmann - 2009 - On the conditions used to prove oracle results for the Lasso.pdf:pdf},
journal = {Electronic Journal of Statistics},
keywords = {and phrases,coherence,compatibility,irrepresentable condition,lasso,re-,restricted isometry,sparsity,stricted eigenvalue},
pages = {1--33},
title = {{On the conditions used to prove oracle results for the Lasso}},
url = {http://projecteuclid.org/euclid.ejs/1260801227},
year = {2009}
}
@article{Dagenais1971,
abstract = {The purpose of this article is to suggest a method of estimating parameters of linear regressions containing two independent variables, when data is missing among these variables. The problem envisaged concerns the case where: (1) the independent variables are considered as fixed numbers; (2) each observation contains the values of the dependent variable and at least one of the independent variables; (3) some observations are complete. In contrast with other approaches dealing with similar problems, the technique developed in this article has the following advantages: (1) it is based on rather unrestrictive hypotheses; (2) the resulting estimators are consistent; (3) the asymptotic variances of these estimators are smaller than those of comparable estimators described in the literature. Although the question is not examined in the present article, it seems also that the proposed method offers good possibilities of generalization.},
author = {Dagenais, Marcel G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dagenais - 1971 - Further suggestions concerning the utilization of incomplete observations in regression analysis.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Missing data,Model specification},
number = {333},
pages = {93--98},
title = {{Further suggestions concerning the utilization of incomplete observations in regression analysis }},
volume = {66},
year = {1971}
}
@article{Gelman2015,
abstract = {We argue that the words " objectivity " and " subjectivity " in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality, and correspon-dence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. The advantage of these reformulations is that the replacement terms do not oppose each other. Instead of debating over whether a given statistical method is subjec-tive or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledg-ment of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling, and socioeconomic stratification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.05453v1},
author = {Gelman, Andrew and Hennig, Christian},
eprint = {arXiv:1508.05453v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hennig - 2015 - Beyond subjective and objective in statistics.pdf:pdf},
keywords = {()},
number = {August},
title = {{Beyond subjective and objective in statistics}},
year = {2015}
}
@inproceedings{Ho1996,
author = {Ho, Tin Kam and Kleinberg, Eeugene M.},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Kleinberg - 1996 - Building projectable classifiers of arbitrary complexity.pdf:pdf},
pages = {880--885},
title = {{Building projectable classifiers of arbitrary complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547202},
volume = {2},
year = {1996}
}
@article{Grunwald2016,
abstract = {We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of `safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as "validity" (the strongest notion), "calibration", "confidence safety" and "unbiasedness" (almost the weakest notion).},
archivePrefix = {arXiv},
arxivId = {1604.01785},
author = {Gr{\"{u}}nwald, Peter},
eprint = {1604.01785},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald - 2016 - Safe Probability.pdf:pdf},
number = {1979},
pages = {1--39},
title = {{Safe Probability}},
url = {http://arxiv.org/abs/1604.01785},
year = {2016}
}
@article{Sun2013,
author = {Sun, Quan and Pfahringer, Bernhard},
doi = {10.1007/s10994-013-5387-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Pfahringer - 2013 - Pairwise meta-rules for better meta-learning-based algorithm ranking.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {algorithm ranking,ensemble learning,meta-learning,ranking trees},
month = {jul},
number = {1},
pages = {141--161},
title = {{Pairwise meta-rules for better meta-learning-based algorithm ranking}},
url = {http://link.springer.com/10.1007/s10994-013-5387-y},
volume = {93},
year = {2013}
}
@article{Hayashi2015,
author = {Hayashi, K. and Takai, K.},
doi = {10.1080/03610918.2014.957847},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hayashi, Takai - 2015 - Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analy.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {June},
pages = {00--00},
title = {{Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analysis}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918.2014.957847},
year = {2015}
}
@article{Efron2004,
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron et al. - 2004 - Least Angle Regression.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
number = {2},
pages = {407--499},
title = {{Least Angle Regression}},
url = {http://projecteuclid.org/euclid.aos/1083178935},
volume = {32},
year = {2004}
}
@inproceedings{Macia2010,
author = {Macia, Nuria and Ho, Tin Kam and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia et al. - 2010 - The Landscape Contest at ICPR 2010.pdf:pdf},
pages = {29--45},
title = {{The Landscape Contest at ICPR 2010}},
year = {2010}
}
@article{Shiao2014,
author = {Shiao, Han-Tai and Cherkassky, Vladimir},
doi = {10.1109/IJCNN.2014.6889517},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shiao, Cherkassky - 2014 - Learning using privileged information (LUPI) for modeling survival data.pdf:pdf},
isbn = {978-1-4799-1484-5},
journal = {2014 International Joint Conference on Neural Networks (IJCNN)},
month = {jul},
pages = {1042--1049},
publisher = {Ieee},
title = {{Learning using privileged information (LUPI) for modeling survival data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6889517},
year = {2014}
}
@article{Leisch2008,
author = {Leisch, Friedrich},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leisch - 2008 - FlexMix Version 2 Finite Mixtures with Concomitant Variables and Varying and Constant Parameters.pdf:pdf},
keywords = {concomitant variables,finite mixture models,generalized linear models,r},
number = {4},
title = {{FlexMix Version 2 : Finite Mixtures with Concomitant Variables and Varying and Constant Parameters}},
volume = {28},
year = {2008}
}
@article{Chatterjee2015,
abstract = {The goal of importance sampling is to estimate the expected value of a given function with respect to a probability measure {\$}\backslashnu{\$} using a random sample of size {\$}n{\$} drawn from a different probability measure {\$}\backslashmu{\$}. If the two measures {\$}\backslashmu{\$} and {\$}\backslashnu{\$} are nearly singular with respect to each other, which is often the case in practice, the sample size required for accurate estimation is large. In this article it is shown that in a fairly general setting, a sample of size approximately {\$}\backslashexp(D(\backslashnu||\backslashmu)){\$} is necessary and sufficient for accurate estimation by importance sampling, where {\$}D(\backslashnu||\backslashmu){\$} is the Kullback--Leibler divergence of {\$}\backslashmu{\$} from {\$}\backslashnu{\$}. In particular, the required sample size exhibits a kind of cut-off in the logarithmic scale. The theory is applied to obtain a fairly general formula for the sample size required in importance sampling for exponential families (Gibbs measures). We also show that the standard variance-based diagnostic for convergence of importance sampling is fundamentally problematic. An alternative diagnostic that provably works in certain situations is suggested.},
archivePrefix = {arXiv},
arxivId = {1511.01437},
author = {Chatterjee, Sourav and Diaconis, Persi},
eprint = {1511.01437},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Diaconis - 2015 - The sample size required in importance sampling.pdf:pdf},
keywords = {and phrases,gibbs measure,importance sampling,monte carlo methods,phase transition},
pages = {1--31},
title = {{The sample size required in importance sampling}},
url = {http://arxiv.org/abs/1511.01437},
year = {2015}
}
@unpublished{Looga,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - Unknown - A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification.pdf:pdf},
keywords = {constrained estimation,log-likelihood,nearest mean classifier,semi-supervised learning},
number = {1},
title = {{A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification}},
volume = {1}
}
@article{Michie1994,
author = {Michie, D and Spiegelhalter, D J and Taylor, C C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Michie, Spiegelhalter, Taylor - 1994 - Statlog.pdf:pdf},
title = {{Statlog}},
year = {1994}
}
@article{Buja2014,
author = {Buja, A and Brown, L},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buja, Brown - 2014 - Discussion of A significance test for the lasso''.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {509--517},
title = {{Discussion of "A significance test for the lasso''}},
volume = {42},
year = {2014}
}
@article{Le2015,
author = {Le, Thanh-Binh and Kim, Sang-Woon},
doi = {10.1016/j.patrec.2015.04.011},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Le, Kim - 2015 - Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Semi-supervised learning,Semi-supervised boosting,,semi-supervised boosting,semi-supervised learning,support vector machines},
pages = {48--56},
publisher = {Elsevier Ltd.},
title = {{Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001282},
volume = {60-61},
year = {2015}
}
@article{Wilson2015a,
abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost {\$}O(n){\$} for {\$}n{\$} training points, and predictions cost {\$}O(1){\$} per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
archivePrefix = {arXiv},
arxivId = {1511.02222},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1511.02222},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson et al. - 2015 - Deep Kernel Learning.pdf:pdf},
number = {1998},
pages = {1--19},
title = {{Deep Kernel Learning}},
url = {http://arxiv.org/abs/1511.02222},
year = {2015}
}
@article{Lindner1999,
author = {Lindner, Guido and Studer, Rudi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lindner, Studer - 1999 - AST Support for algorithm selection with a CBR approach.pdf:pdf},
journal = {Principles of Data Mining and Knowledge Discovery},
title = {{AST: Support for algorithm selection with a CBR approach}},
url = {http://www.springerlink.com/index/QBDF3R2GKVW57LUF.pdf http://link.springer.com/chapter/10.1007/978-3-540-48247-5{\_}52},
year = {1999}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@article{OpenScienceCollaboration2015,
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6251},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rahimi, Recht - 2007 - Random features for large-scale kernel machines.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {1--8},
title = {{Random features for large-scale kernel machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}
@article{DeBrebisson2016,
abstract = {Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax.},
archivePrefix = {arXiv},
arxivId = {1604.08859},
author = {de Br{\'{e}}bisson, Alexandre and Vincent, Pascal},
eprint = {1604.08859},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/de Br{\'{e}}bisson, Vincent - 2016 - The Z-loss a shift and scale invariant classification loss belonging to the Spherical Family.pdf:pdf},
title = {{The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family}},
url = {http://arxiv.org/abs/1604.08859},
year = {2016}
}
@article{Lopez-paz2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02398v1},
author = {Lopez-paz, David and Sch, Bernhard},
eprint = {arXiv:1502.02398v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-paz, Sch - 2012 - Towards a Learning Theory of Causation.pdf:pdf},
title = {{Towards a Learning Theory of Causation}},
year = {2012}
}
@inproceedings{Ji2012,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ji et al. - 2012 - A simple algorithm for semi-supervised learning with improved generalization error bound.pdf:pdf},
number = {2},
title = {{A simple algorithm for semi-supervised learning with improved generalization error bound}},
url = {http://arxiv.org/abs/1206.6412},
year = {2012}
}
@article{Rifkin2003,
author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin, Yeo, Poggio - 2003 - Regularized least-squares classification.pdf:pdf},
journal = {Nato Science Series Sub Series III Computer and Systems Sciences 190},
title = {{Regularized least-squares classification}},
year = {2003}
}
@article{Ridgway2014,
archivePrefix = {arXiv},
arxivId = {1410.1771},
author = {Ridgway, James and Alquier, Pierre and Chopin, Nicolas and Liang, Feng},
eprint = {1410.1771},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ridgway et al. - 2014 - PAC-Bayesian AUC classification and scoring.pdf:pdf},
month = {oct},
pages = {1--18},
title = {{PAC-Bayesian AUC classification and scoring}},
url = {http://arxiv.org/abs/1410.1771v1},
year = {2014}
}
@article{Weston2006,
address = {New York, New York, USA},
author = {Weston, Jason and Collobert, Ronan and Sinz, Fabian and Bottou, L{\'{e}}on and Vapnik, Vladimir},
doi = {10.1145/1143844.1143971},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2006 - Inference with the Universum.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {1009--1016},
publisher = {ACM Press},
title = {{Inference with the Universum}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143971},
year = {2006}
}
@article{Witten2011,
abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p â‰« n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00783.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2011 - Penalized classification using Fisher's linear discriminant.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {classification,discriminant analysis,feature selection,high dimensional problems,lasso,linear,supervised learning},
month = {nov},
number = {5},
pages = {753--772},
pmid = {22323898},
title = {{Penalized classification using Fisher's linear discriminant.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3272679{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {73},
year = {2011}
}
@article{Zhou2007b,
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {1159--1166},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273642},
year = {2007}
}
@book{Pearl,
abstract = {Written by one of the pre-eminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, philosophy, cognitive science, and the health and social sciences. Pearl presents a unified account of the probabilistic, manipulative, counterfactual and structural approaches to causation, and devises simple mathematical tools for analyzing the relationships between causal connections, statistical associations, actions and observations. The book will open the way for including causal analysis in the standard curriculum of statistics, artifical intelligence, business, epidemiology, social science and economics. Students in these areas will find natural models, simple identification procedures, and precise mathematical definitions of causal concepts that traditional texts have tended to evade or make unduly complicated. This book will be of interest to professionals and students in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable. Professor of Computer Science at the UCLA, Judea Pearl is the winner of the 2008 Benjamin Franklin Award in Computers and Cognitive Science.},
author = {Pearl, Judea},
doi = {10.1215/00318108-110-4-639},
edition = {Second},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causality Models, Reasoning, and Inference.pdf:pdf},
isbn = {0521773628},
issn = {00323470},
pmid = {11768929},
publisher = {Cambridge University Press},
title = {{Causality: Models, Reasoning, and Inference}},
year = {2009}
}
@article{Leemis2008,
author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
doi = {10.1198/000313008X270448},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leemis, McQueston - 2008 - Univariate Distribution Relationships.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {asymptotic relationships,distribution proper-,limiting distributions,stochastic parameters,ties,transforma-},
month = {feb},
number = {1},
pages = {45--53},
title = {{Univariate Distribution Relationships}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
volume = {62},
year = {2008}
}
@article{Nguyen2010,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization(2).pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {nov},
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@article{Fraser2011a,
abstract = {Bayes [Philos. Trans. R. Soc. Lond. 53 (1763) 370â€“418; 54 296â€“325] introduced the observed likelihood function to statistical in- ference and provided a weight function to calibrate the parameter; he also introduced a confidence distribution on the parameter space but did not provide present justifications. Of course the names likelihood and confidence did not appear until much later: Fisher [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 222 (1922) 309â€“368] for likelihood and Neyman [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 237 (1937) 333â€“380] for confidence. Lindley [J. Roy. Statist. Soc. Ser. B 20 (1958) 102â€“107] showed that the Bayes and the confidence results were different when the model was not location. This paper examines the occurrence of true statements from the Bayes approach and from the confidence approach, and shows that the pro- portion of true statements in the Bayes case depends critically on the presence of linearity in the model; and with departure from this linear- ity the Bayes approach can be a poor approximation and be seriously misleading. Bayesian integration of weighted likelihood thus provides a first-order linear approximation to confidence, but without linearity can give substantially incorrect results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.5582v1},
author = {Fraser, D. a. S.},
doi = {10.1214/11-STS352},
eprint = {arXiv:1112.5582v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fraser - 2011 - Is Bayes Posterior just Quick and Dirty Confidence.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {a prior,and phrases,bayes,bayes error rate,confidence,nonlinear parameter,posterior,prior},
number = {3},
pages = {299--316},
title = {{Is Bayes Posterior just Quick and Dirty Confidence?}},
volume = {26},
year = {2011}
}
@article{Culp2008b,
abstract = {Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers, and therefore a semi-supervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced, and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.},
author = {Culp, Mark and Michailidis, George},
doi = {10.1109/TPAMI.2007.70765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - Graph-based semisupervised learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer Simulation,Data Interpretation,Models,Pattern Recognition,Statistical},
month = {jan},
number = {1},
pages = {174--9},
pmid = {18000333},
title = {{Graph-based semisupervised learning.}},
volume = {30},
year = {2008}
}
@article{Braga-Neto2004,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Braga-Neto, Dougherty - 2004 - Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Reproducibility of Results,Sample Size,Sensitivity and Specificity},
month = {feb},
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{Browner1987,
abstract = {Just as diagnostic tests are most helpful in light of the clinical presentation, statistical tests are most useful in the context of scientific knowledge. Knowing the specificity and sensitivity of a diagnostic test is necessary, but insufficient: the clinician must also estimate the prior probability of the disease. In the same way, knowing the P value and power, or the confidence interval, for the results of a research study is necessary but insufficient: the reader must estimate the prior probability that the research hypothesis is true. Just as a positive diagnostic test does not mean that a patient has the disease, especially if the clinical picture suggests otherwise, a significant P value does not mean that a research hypothesis is correct, especially if it is inconsistent with current knowledge. Powerful studies are like sensitive tests in that they can be especially useful when the results are negative. Very low P values are like very specific tests; both result in few false-positive results due to chance. This Bayesian approach can clarify much of the confusion surrounding the use and interpretation of statistical tests.},
author = {Browner, W S and Newman, T B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Browner, Newman - 1987 - Are all significant P values created equal The analogy between diagnostic tests and clinical research.pdf:pdf},
issn = {0098-7484},
journal = {JAMA : the journal of the American Medical Association},
keywords = {Bayes Theorem,Predictive Value of Tests,Research,Statistics as Topic},
month = {may},
number = {18},
pages = {2459--63},
pmid = {3573245},
title = {{Are all significant P values created equal? The analogy between diagnostic tests and clinical research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3573245},
volume = {257},
year = {1987}
}
@article{Nguyen2009,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1214/08-AOS595},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2009 - On surrogate loss functions and f -divergences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {876--904},
title = {{On surrogate loss functions and f -divergences}},
url = {http://projecteuclid.org/euclid.aos/1236693153},
volume = {37},
year = {2009}
}
@article{2016b,
author = {æ¾å…ƒ, å¡ä¸€},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/æ¾å…ƒ - 2016 - èª­æ›¸ä¼šè³‡æ–™ Semi-supervised learning with Ladder Networks.pdf:pdf},
pages = {1--9},
title = {{èª­æ›¸ä¼šè³‡æ–™ Semi-supervised learning with Ladder Networks}},
year = {2016}
}
@article{Gelman2011a,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Ethics and statistics Open data and open methods.pdf:pdf},
journal = {Chance},
pages = {51--53},
title = {{Ethics and statistics: Open data and open methods}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Ethics+and+Statistics+Open+Data+and+Open+Methods{\#}3},
year = {2011}
}
@inproceedings{Loog2010,
author = {Loog, Marco},
booktitle = {Proceedings of the 2010 European Conference on Machine learning and Knowledge Discovery in Databases},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2010 - Constrained Parameter Estimation for Semi-Supervised Learning The Case of the Nearest Mean Classifier.pdf:pdf},
pages = {291--304},
title = {{Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier}},
year = {2010}
}
@article{Li2011a,
abstract = {Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification.},
author = {Li, Limin and Rakitsch, Barbara and Borgwardt, Karsten},
doi = {10.1093/bioinformatics/btr204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Rakitsch, Borgwardt - 2011 - ccSVM Correcting Support Vector Machines for confounding factors in biological data classification.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {13},
pmid = {21685091},
title = {{ccSVM: Correcting Support Vector Machines for confounding factors in biological data classification}},
volume = {27},
year = {2011}
}
@article{Gelman2013e,
abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data. 1.},
author = {Gelman, Andrew and Loken, Eric},
doi = {10.1037/a0037714},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Loken - 2013 - The garden of forking paths Why multiple comparisons can be a problem, even when there is no â€œfishing exp.pdf:pdf},
issn = {1939-1455},
journal = {Downloaded January},
pages = {1--17},
pmid = {25180805},
title = {{The garden of forking paths: Why multiple comparisons can be a problem, even when there is no â€œfishing expeditionâ€ or â€œp-hackingâ€ and the research hypothesis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/p{\_}hacking.pdf},
year = {2013}
}
@phdthesis{Lu2009,
author = {Lu, Tyler},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu - 2009 - Fundamental Limitations of Semi-Supervised Learning.pdf:pdf},
title = {{Fundamental Limitations of Semi-Supervised Learning}},
year = {2009}
}
@inproceedings{Widrow1960,
author = {Widrow, Bernard and Hoff, Marcian E.},
booktitle = {IRE WESCON Convention Record 4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Widrow, Hoff - 1960 - Adaptive switching circuits.pdf:pdf},
pages = {96--104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@inproceedings{Fujino2005,
author = {Fujino, Akinori and Ueda, Naonori and Saito, Kazumi},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fujino, Ueda, Saito - 2005 - A hybrid generativediscriminative approach to semi-supervised classifier design.pdf:pdf},
number = {2},
pages = {764--769},
title = {{A hybrid generative/discriminative approach to semi-supervised classifier design}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-120.pdf},
volume = {20},
year = {2005}
}
@article{Breiman1996,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 1996 - Bagging predictors.pdf:pdf},
journal = {Machine learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging predictors}},
url = {http://www.springerlink.com/index/L4780124W2874025.pdf},
volume = {140},
year = {1996}
}
@article{Waldram1971,
archivePrefix = {arXiv},
arxivId = {1511.01844},
author = {Waldram, J.M.},
doi = {10.1177/096032717100300408},
eprint = {1511.01844},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Waldram - 1971 - A note on the evaluation of modelling.pdf:pdf},
issn = {1477-1535},
journal = {Lighting Research and Technology},
number = {4},
pages = {284--285},
title = {{A note on the evaluation of modelling}},
url = {http://lrt.sagepub.com/cgi/doi/10.1177/096032717100300408},
volume = {3},
year = {1971}
}
@article{Sch,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.2738v1},
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Zhang, Kun},
eprint = {arXiv:1112.2738v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf et al. - Unknown - Robust Learning via Cause-Effect Models.pdf:pdf},
pages = {1--15},
title = {{Robust Learning via Cause-Effect Models}}
}
@article{Smith-Miles2008,
author = {Smith-Miles, Kate},
doi = {10.1145/1456650.1456656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smith-Miles - 2008 - Cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {dec},
number = {1},
pages = {1--25},
title = {{Cross-disciplinary perspectives on meta-learning for algorithm selection}},
url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
volume = {41},
year = {2008}
}
@article{Reid2015,
abstract = {Applied statistical problems often come with pre-specified groupings to predictors. It is natural to test for the presence of simultaneous group-wide signal for groups in isolation, or for multiple groups together. Classical tests for the presence of such signals rely either on tests for the omission of the entire block of variables (the classical F-test) or on the creation of an unsupervised prototype for the group (either a group centroid or first principal component) and subsequent t-tests on these prototypes. In this paper, we propose test statistics that aim for power improvements over these classical approaches. In particular, we first create group prototypes, with reference to the response, hopefully improving on the unsupervised prototypes, and then testing with likelihood ratio statistics incorporating only these prototypes. We propose a (potentially) novel model, called the "prototype model", which naturally models the two-step prototype-then-test procedure. Furthermore, we introduce an inferential schema detailing the unique considerations for different combinations of prototype formation and univariate/multivariate testing models. The prototype model also suggests new applications to estimation and prediction. Prototype formation often relies on variable selection, which invalidates classical Gaussian test theory. We use recent advances in selective inference to account for selection in the prototyping step and retain test validity. Simulation experiments suggest that our testing procedure enjoys more power than do classical approaches.},
archivePrefix = {arXiv},
arxivId = {1511.07839},
author = {Reid, Stephen and Taylor, Jonathan and Tibshirani, Robert},
eprint = {1511.07839},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Taylor, Tibshirani - 2015 - A general framework for estimation and inference from clusters of features.pdf:pdf},
number = {2010},
pages = {1--27},
title = {{A general framework for estimation and inference from clusters of features}},
url = {http://arxiv.org/abs/1511.07839},
year = {2015}
}
@inproceedings{Cozman2003,
author = {Cozman, Fabio Gagliardi and Cohen, Ira and Cirelo, Marcelo Cesar},
booktitle = {Proceedings of the Twentieth International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen, Cirelo - 2003 - Semi-Supervised Learning of Mixture Models.pdf:pdf},
title = {{Semi-Supervised Learning of Mixture Models}},
year = {2003}
}
@article{Macia2013,
author = {Maci{\`{a}}, N{\'{u}}ria and Bernad{\'{o}}-Mansilla, Ester},
doi = {10.1016/j.ins.2013.08.059},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maci{\`{a}}, Bernad{\'{o}}-Mansilla - 2013 - Towards UCI A mindful repository design.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
month = {sep},
title = {{Towards UCI+: A mindful repository design}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
year = {2013}
}
@inproceedings{Maddison2014,
author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maddison, Tarlow, Minka - 2014 - A Sampling.pdf:pdf},
title = {{A* Sampling}},
year = {2014}
}
@article{Gelman2012a,
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill, Yajima - 2012 - Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf:pdf},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
month = {apr},
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213},
volume = {5},
year = {2012}
}
@article{Taylor2006,
author = {Taylor, Publisher and Hunt, D N and Bell, M J},
doi = {10.1080/02664769000000011},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taylor, Hunt, Bell - 2006 - Semi-iterative missing value estimation.pdf:pdf},
issn = {0266-4763},
number = {April 2013},
pages = {37--41},
title = {{Semi-iterative missing value estimation}},
volume = {4763},
year = {2006}
}
@article{Garcia-Laencina2010,
abstract = {Pattern classification has been successfully applied in many problem domains, such as biometric recognition, document classification or medical diagnosis. Missing or unknown data are a common drawback that pattern recognition techniques need to deal with when solving real-life classification tasks. Machine learning approaches and methods imported from statistical learning theory have been most intensively studied and used in this subject. The aim of this work is to analyze the missing data problem in pattern classification tasks, and to summarize and compare some of the well-known methods used for handling missing values.},
author = {Garc{\'{i}}a-Laencina, Pedro J. and Sancho-G{\'{o}}mez, Jos{\'{e}}-Luis and Figueiras-Vidal, An{\'{i}}bal R.},
doi = {10.1007/s00521-009-0295-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Garc{\'{i}}a-Laencina, Sancho-G{\'{o}}mez, Figueiras-Vidal - 2010 - Pattern classification with missing data a review.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {data {\'{a}},learning,neural networks {\'{a}} machine,pattern classification {\'{a}} missing},
number = {2},
pages = {263--282},
title = {{Pattern classification with missing data: a review}},
volume = {19},
year = {2010}
}
@article{Hallac2015,
abstract = {Convex optimization is an essential tool for modern data analysis, as it provides a framework to formulate and solve many problems in machine learning and data mining. However, general convex optimization solvers do not scale well, and scalable solvers are often specialized to only work on a narrow class of problems. Therefore, there is a need for simple, scalable algorithms that can solve many common optimization problems. In this paper, we introduce the $\backslash$emph{\{}network lasso{\}}, a generalization of the group lasso to a network setting that allows for simultaneous clustering and optimization on graphs. We develop an algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve this problem in a distributed and scalable manner, which allows for guaranteed global convergence even on large graphs. We also examine a non-convex extension of this approach. We then demonstrate that many types of problems can be expressed in our framework. We focus on three in particular - binary classification, predicting housing prices, and event detection in time series data - comparing the network lasso to baseline approaches and showing that it is both a fast and accurate method of solving large optimization problems.},
archivePrefix = {arXiv},
arxivId = {1507.00280},
author = {Hallac, David and Leskovec, Jure and Boyd, Stephen},
doi = {10.1145/2783258.2783313},
eprint = {1507.00280},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hallac, Leskovec, Boyd - 2015 - Network Lasso Clustering and Optimization in Large Graphs.pdf:pdf},
isbn = {9781450336642},
keywords = {27,admm,and to,apply to a variety,convex optimization,formulate general classes of,it is necessary to,model,network lasso,of relevant problems,often an infeasible assumption,optimiza-,ploit structure in the,therefore,tion solvers that can},
title = {{Network Lasso: Clustering and Optimization in Large Graphs}},
url = {http://arxiv.org/abs/1507.00280 http://dx.doi.org/10.1145/2783258.2783313},
year = {2015}
}
@article{Gelman2013a,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - P values and statistical practice.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology},
month = {jan},
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Keiding2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02853v1},
author = {Keiding, Niels and Clayton, David},
doi = {10.1214/13-STS453},
eprint = {arXiv:1503.02853v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keiding, Clayton - 2014 - Standardization and Control for Confounding in Observational Studies A Historical Perspective.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {2,22K table, causality, decomposition of rates, epid,and phrases,causality,decomposition,epidemiology,expected number of deaths,g,h,k table,log-linear model,marginal structural model,national halothane study,odds ratio,of rates,rate,ratio,transportability,u,westergaard,yule},
number = {4},
pages = {529--558},
title = {{Standardization and Control for Confounding in Observational Studies: A Historical Perspective}},
url = {http://projecteuclid.org/euclid.ss/1421330546},
volume = {29},
year = {2014}
}
@article{Witten2010,
abstract = {We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1198/jasa.2010.tm09415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2010 - A framework for feature selection in clustering.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hierarchical clustering,high-dimensional,k-means clustering,lasso,model selection,sparsity,unsupervised learning},
month = {jun},
number = {490},
pages = {713--726},
pmid = {20811510},
title = {{A framework for feature selection in clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930825{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {105},
year = {2010}
}
@article{Shi2011,
author = {Shi, Mingguang and Zhang, Bing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shi, Zhang - 2011 - Semi-supervised learning improves gene expression-based prediction of cancer recurrence.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3017--3023},
title = {{Semi-supervised learning improves gene expression-based prediction of cancer recurrence}},
volume = {27},
year = {2011}
}
@book{Gelman2003,
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2003 - Bayesian Data Analysis.pdf:pdf},
title = {{Bayesian Data Analysis}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Bayesian+Data+Analysis{\#}0},
year = {2003}
}
@article{Parker2012,
abstract = {Measurements from microarrays and other high-throughput technologies are susceptible to non-biological artifacts like batch effects. It is known that batch effects can alter or obscure the set of significant results and biological conclusions in high-throughput studies. Here we examine the impact of batch effects on predictors built from genomic technologies. To investigate batch effects, we collected publicly available gene expression measurements with known outcomes, and estimated batches using date. Using these data we show (1) the impact of batch effects on prediction depends on the correlation between outcome and batch in the training data, and (2) removing expression measurements most affected by batch before building predictors may improve the accuracy of those predictors. These results suggest that (1) training sets should be designed to minimize correlation between batches and outcome, and (2) methods for identifying batch-affected probes should be developed to improve prediction results for studies with high correlation between batches and outcome.},
author = {Parker, Hilary S. and Leek, Jeffrey T.},
doi = {10.1515/1544-6115.1766},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Parker, Leek - 2012 - The practical effect of batch on genomic prediction.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
number = {3},
pmid = {22611599},
title = {{The practical effect of batch on genomic prediction}},
volume = {11},
year = {2012}
}
@article{Sohn1999,
author = {Sohn, So Young},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sohn - 1999 - Meta Analysis of Classification Algorithms for Pattern Recognition.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1137--1144},
title = {{Meta Analysis of Classification Algorithms for Pattern Recognition}},
volume = {21},
year = {1999}
}
@inproceedings{Krijthe2012b,
author = {Krijthe, Jesse Hendrik and Ho, Tin Kam and Loog, Marco},
booktitle = {Proceedings of the 21st International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Ho, Loog - 2012 - Improving cross-validation based classifier selection using meta-learning.pdf:pdf},
pages = {2873--2876},
title = {{Improving cross-validation based classifier selection using meta-learning}},
year = {2012}
}
@article{Astorino2007,
author = {Astorino, A and Fuduli, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Astorino, Fuduli - 2007 - Nonsmooth optimization techniques for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {nonsmooth optimization,semi,supervised learning},
number = {12},
pages = {2135--2142},
title = {{Nonsmooth optimization techniques for Semi-Supervised Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4359288},
volume = {29},
year = {2007}
}
@article{Shaffer1991,
author = {Shaffer, Juliet Popper},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shaffer - 1991 - The Gauss-Markov Theorem and Random Regressors.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear unbiased estimators,finite-,linear regression,population sampling,unbiased esti-},
number = {4},
pages = {269--273},
title = {{The Gauss-Markov Theorem and Random Regressors}},
volume = {45},
year = {1991}
}
@article{Esposito2015,
author = {Esposito, G. and Martin, M.},
doi = {10.1080/08839514.2015.1035951},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Esposito, Martin - 2015 - A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines.pdf:pdf},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
number = {5},
pages = {459--479},
title = {{A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines}},
url = {http://www.tandfonline.com/doi/full/10.1080/08839514.2015.1035951},
volume = {29},
year = {2015}
}
@article{Akande2015,
abstract = {Multiple imputation is a common approach for dealing with missing values in statistical databases. The imputer fills in missing values with draws from predictive models estimated from the observed data, resulting in multiple, completed versions of the database. Researchers have developed a variety of default routines to implement multiple imputation; however, there has been limited research comparing the performance of these methods, particularly for categorical data. We use simulation studies to compare repeated sampling properties of three default multiple imputation methods for categorical data, including chained equations using generalized linear models, chained equations using classification and regression trees, and a fully Bayesian joint distribution based on Dirichlet Process mixture models. We base the simulations on categorical data from the American Community Survey. The results suggest that default chained equations approaches based on generalized linear models are dominated by the default regression tree and mixture model approaches. They also suggest competing advantages for the regression tree and mixture model approaches, making both reasonable default engines for multiple imputation of categorical data.},
archivePrefix = {arXiv},
arxivId = {1508.05918},
author = {Akande, Olanrewaju and Li, Fan and Reiter, Jerome},
eprint = {1508.05918},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Akande, Li, Reiter - 2015 - An Empirical Comparison of Multiple Imputation Methods for Categorical Data.pdf:pdf},
pages = {1--26},
title = {{An Empirical Comparison of Multiple Imputation Methods for Categorical Data}},
url = {http://arxiv.org/abs/1508.05918},
volume = {27708},
year = {2015}
}
@inproceedings{McDowell2012,
author = {McDowell, Luke and Aha, David W.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McDowell, Aha - 2012 - Semi-supervised collective classification via hybrid label regularization.pdf:pdf},
pages = {975--982},
title = {{Semi-supervised collective classification via hybrid label regularization}},
url = {http://arxiv.org/abs/1206.6467},
year = {2012}
}
@article{Wang2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06309v1},
author = {Wang, Yu-xiang and Lei, Jing and Fienberg, Stephen E and Feb, M L},
eprint = {arXiv:1502.06309v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Learning with Differential Privacy Stability , Learnability and the Sufficiency and Necessity of ERM Principle.pdf:pdf},
pages = {1--33},
title = {{Learning with Differential Privacy : Stability , Learnability and the Sufficiency and Necessity of ERM Principle}},
year = {2015}
}
@article{Lattimore2011,
archivePrefix = {arXiv},
arxivId = {1111.3846},
author = {Lattimore, Tor and Hutter, Marcus},
eprint = {1111.3846},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lattimore, Hutter - 2011 - No Free Lunch versus Occam's Razor in Supervised Learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {kolmogorov complexity,no free lunch,occam,s razor,supervised learning},
title = {{No Free Lunch versus Occam's Razor in Supervised Learning}},
url = {http://arxiv.org/abs/1111.3846},
year = {2011}
}
@phdthesis{Krijthe2012,
abstract = {In order to choose from the large number of classification methods available for use, cross-validation error estimates are often employed. We present this cross-validation selection strategy in the framework of meta-learning and show that conceptually, meta- learning techniques could provide better classifier selections than traditional cross-validation selection. Using various simulation studies we illustrate and discuss this possibility. Through a collection of datasets resembling real-world data, we investigate whether these improvements could possibly exist in the real-world as well. Although the approach presented here currently requires signifi- cant investment when applied to practical applications, the concept of being able to outperform cross-validation selection opens the door to new classifier selection strategies.},
author = {Krijthe, Jesse Hendrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe - 2012 - Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning.pdf:pdf},
keywords = {Classifier Selection,Error estimation,Meta-Learning},
mendeley-tags = {Classifier Selection,Error estimation,Meta-Learning},
school = {Delft University of Technology},
title = {{Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning}},
year = {2012}
}
@article{Yates1933,
author = {Yates, Frank},
journal = {Empire Journal of Experimental Agriculture},
number = {2},
pages = {129--142},
title = {{The analysis of replicated experiments when the field results are incomplete}},
volume = {1},
year = {1933}
}
@article{Bartlett2003a,
author = {Bartlett, Peter L and Mendelson, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Mendelson - 2003 - Rademacher and Gaussian complexities Risk bounds and structural results.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
url = {http://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2003}
}
@article{Kalousis2004,
author = {Kalousis, Alexandros and Gama, Joao and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Gama, Hilario - 2004 - On data and algorithms Understanding inductive performance.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {275--312},
title = {{On data and algorithms: Understanding inductive performance}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015882.38031.85},
volume = {54},
year = {2004}
}
@article{Brummer2016a,
abstract = {This is a short note to mathematically compare two recently published methods for constructing flexible, but tractable families of variational posteriors. The first method, called "hierarchical variational models" enriches the inference model with an extra variable, while the other, called "auxiliary deep generative models", enriches the generative model instead. We conclude that the two methods are mathematically equivalent.},
archivePrefix = {arXiv},
arxivId = {1603.02443},
author = {Br{\"{u}}mmer, Niko},
eprint = {1603.02443},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Br{\"{u}}mmer - 2016 - Note on the equivalence of hierarchical variational models and auxiliary deep generative models.pdf:pdf},
number = {2},
pages = {5},
title = {{Note on the equivalence of hierarchical variational models and auxiliary deep generative models}},
url = {http://arxiv.org/abs/1603.02443},
year = {2016}
}
@book{Aubin2000,
author = {Aubin, Jean-Pierre},
edition = {Second},
isbn = {9780471179764},
publisher = {John Wiley {\&} Sons},
title = {{Applied functional analysis}},
year = {2000}
}
@inproceedings{Szummer2000,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems 13},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2000 - Kernel expansions with unlabeled examples.pdf:pdf},
pages = {626--632},
title = {{Kernel expansions with unlabeled examples}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.222{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Ben-David2008,
author = {Ben-David, Shai and Lu, Tyler and P{\'{a}}l, David},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Lu, P{\'{a}}l - 2008 - Does Unlabeled Data Provably Help Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.pdf:pdf},
pages = {33--44},
title = {{Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.}},
url = {http://www.cs.toronto.edu/{~}tl/papers/ssl.pdf},
year = {2008}
}
@inproceedings{Giraud-Carrier2008,
author = {Giraud-carrier, Christophe},
booktitle = {Tutorial at the 2008 International Conference on Machine Learning and Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier - 2008 - Metalearning - A Tutorial.pdf:pdf},
number = {December},
title = {{Metalearning - A Tutorial}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/ICMLA2008Tut/ICMLA 2008.pdf},
year = {2008}
}
@inproceedings{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, K},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muandet, Fukumizu - 2012 - Learning from distributions via support measure machines.pdf:pdf},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://arxiv.org/abs/1202.6504},
year = {2012}
}
@article{Resheff2015a,
abstract = {Often in real-world datasets, especially in high dimensional data, some feature values are missing. Since most data analysis and statistical methods do not handle gracefully missing values, the ?rst step in the analysis requires the imputation of missing values. Indeed, there has been a long standing interest in methods for the imputation of missing values as a pre-processing step. One recent and e?ective approach, the IRMI stepwise regression imputation method, uses a linear regression model for each real-valued feature on the basis of all other features in the dataset. However, the proposed iterative formulation lacks convergence guarantee. Here we propose a closely related method, stated as a single optimization problem and a block coordinate-descent solution which is guaranteed to converge to a local minimum. Experiments show results on both synthetic and benchmark datasets, which are comparable to the results of the IRMI method whenever it converges. However, while in the set of experiments described here IRMI often does not converge, the performance of our methods is shown to be markedly superior in comparison with other methods.},
archivePrefix = {arXiv},
arxivId = {1511.05309},
author = {Resheff, Yehezkel S. and Weinshall, Daphna},
eprint = {1511.05309},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Resheff, Weinshall - 2015 - Optimized Linear Imputation.pdf:pdf},
title = {{Optimized Linear Imputation}},
url = {http://arxiv.org/abs/1511.05309},
year = {2015}
}
@article{Pribram1978a,
author = {Afifi, A.A. and Elashoff, R.M.},
doi = {10.1017/S0140525X00060003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Afifi, Elashoff - 1967 - Missing Observations in Multivariate Statistics II Point Estimation in Simple Linear Regression.pdf:pdf},
isbn = {0300104251},
issn = {0140-525X},
journal = {Journal of the American Statistical Association},
number = {317},
pages = {10--29},
title = {{Missing Observations in Multivariate Statistics II: Point Estimation in Simple Linear Regression}},
volume = {62},
year = {1967}
}
@article{Nakamoto2008,
abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
archivePrefix = {arXiv},
arxivId = {43543534534v343453},
author = {Nakamoto, Satoshi},
doi = {10.1007/s10838-008-9062-0},
eprint = {43543534534v343453},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nakamoto - 2008 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:pdf},
isbn = {978-972-757-716-3},
issn = {09254560},
journal = {Consulted},
pages = {1--9},
pmid = {14533183},
title = {{Bitcoin: A Peer-to-Peer Electronic Cash System}},
url = {http://s.kwma.kr/pdf/Bitcoin/bitcoin.pdf},
year = {2008}
}
@article{Arnold2000,
author = {Arnold, Bernard F. and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arnold, Stahlecker - 2000 - The minimax adjustment principle.pdf:pdf},
journal = {Mathematical methods of operations research},
keywords = {ellipsoidal information,minimax,minimax adjustment principle,principle,projection estimator,supply policy},
pages = {103--113},
title = {{The minimax adjustment principle}},
url = {http://link.springer.com/article/10.1007/s001860050005},
volume = {51},
year = {2000}
}
@article{Rubin1976,
abstract = {SUMMARY When making sampling distribution inferences about the parameter of the data, $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are â€˜missing at random' and the observed data are â€˜observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is â€˜distinct' from $\theta$. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
author = {Rubin, Donald B.},
doi = {10.2307/2335739},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 1976 - Inference and Missing Data.pdf:pdf},
isbn = {0006344414643510},
issn = {00063444},
journal = {Biometrika},
number = {3},
pages = {581},
pmid = {86},
title = {{Inference and Missing Data}},
url = {http://biomet.oxfordjournals.org.libproxy1.nus.edu.sg/content/63/3/581$\backslash$nhttp://www.jstor.org/stable/2335739?origin=crossref},
volume = {63},
year = {1976}
}
@article{Wang2009b,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2009 - On efficient large margin semisupervised learning Method and theory.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {classification,difference convex programming,nonconvex minimization,regulariza-,support vectors,tion},
pages = {719--742},
title = {{On efficient large margin semisupervised learning: Method and theory}},
url = {http://dl.acm.org/citation.cfm?id=1577094},
volume = {10},
year = {2009}
}
@inproceedings{Furnkranz2001,
author = {F{\"{u}}rnkranz, Johannes and Petrak, Johann},
booktitle = {Proceedings of the ECML/PKDD Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/F{\"{u}}rnkranz, Petrak - 2001 - An Evaluation of Landmarking Variants.pdf:pdf},
pages = {57--68},
title = {{An Evaluation of Landmarking Variants}},
url = {http://ai.ijs.si/branax/iddm-2001-proceedings/paper9.pdf},
year = {2001}
}
@article{Yu2006,
abstract = {This paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y = f(x). We propose a novel and simple concept for active learning, transductive experimental design, that explores available unmeasured experiments (i.e., unlabeled data) and has a better scalability in comparison with classic experimental design methods. Our in-depth analysis shows that the new method tends to favor experiments that are on the one side hard-to-predict and on the other side representative for the rest of the experiments. Efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search. Extensive experimental results on synthetic problems and three real-world tasks, including questionnaire design for preference learning, active learning for text categorization, and spatial sensor placement, highlight the advantages of the proposed approaches.},
author = {Yu, Kai and Bi, Jinbo and Tresp, Volker},
doi = {10.1145/1143844.1143980},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu, Bi, Tresp - 2006 - Active learning via transductive experimental design.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning ICML 06},
number = {6},
pages = {1081--1088},
title = {{Active learning via transductive experimental design}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143980},
volume = {148},
year = {2006}
}
@article{Yang2015,
author = {Yang, Yun and Liu, Xingchen},
doi = {10.1016/j.patrec.2015.08.009},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Liu - 2015 - A robust semi-supervised learning approach via mixture of label information.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Clustering,semi-supervised learning},
publisher = {Elsevier Ltd.},
title = {{A robust semi-supervised learning approach via mixture of label information}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515002688},
year = {2015}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = {may},
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@article{Juszczak2004,
author = {Juszczak, P and Duin, R P W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Juszczak, Duin - 2004 - Combining one-class classifiers to classify missing data.pdf:pdf},
issn = {03029743},
journal = {Multiple Classifier Systems},
pages = {92--101},
title = {{Combining one-class classifiers to classify missing data}},
year = {2004}
}
@article{Buhlmann2002,
author = {Buhlmann, Peter and Yu, Bin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Yu - 2002 - Analyzing bagging.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {927--961},
title = {{Analyzing bagging}},
volume = {30},
year = {2002}
}
@article{Martella2011,
abstract = {In healthy aging research, typically multiple health outcomes are measured, representing health status. The aim of this paper was to develop a model-based clustering approach to identify homogeneous sibling pairs according to their health status. Model-based clustering approaches will be considered on the basis of linear mixed effect model for the mixture components. Class memberships of siblings within pairs are allowed to be correlated, and within a class the correlation between siblings is modeled using random sibling pair effects. We propose an expectation-maximization algorithm for maximum likelihood estimation. Model performance is evaluated via simulations in terms of estimating the correct parameters, degree of agreement, and the ability to detect the correct number of clusters. The performance of our model is compared with the performance of standard model-based clustering approaches. The methods are used to classify sibling pairs from the Leiden Longevity Study according to their health status. Our results suggest that homogeneous healthy sibling pairs are associated with a longer life span. Software is available for fitting the new models.},
author = {Martella, F. and Vermunt, J.K. and Beekman, M. and Westendorp, R.G.J. and Slagboom, P.E. and Houwing-Duistermaat, J.J.},
doi = {10.1002/sim.4365},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Martella et al. - 2011 - A mixture model with random-effects components for classifying sibling pairs.pdf:pdf},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {80 and over,Aged,Aging,Aging: physiology,Cluster Analysis,Computer Simulation,Female,Health,Humans,Longevity,Longevity: physiology,Male,Models,Siblings,Statistical},
month = {nov},
number = {27},
pages = {3252--64},
pmid = {21905068},
title = {{A mixture model with random-effects components for classifying sibling pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21905068},
volume = {30},
year = {2011}
}
@article{Briggs2015,
abstract = {Probability models are only useful at explaining the uncertainty of what we do not know, and should never be used to say what we already know. Probability and statistical models are useless at discerning cause. Classical statistical procedures, in both their frequentist and Bayesian implementations are, falsely imply they can speak about cause. No hypothesis test, or Bayes factor, should ever be used again. Even assuming we know the cause or partial cause for some set of observations, reporting via relative risk exagerates the certainty we have in the future, often by a lot. This over-certainty is made much worse when parametetric and not predictive methods are used. Unfortunately, predictive methods are rarely used; and even when they are, cause must still be an assumption, meaning (again) certainty in our scientific pronouncements is too high.},
archivePrefix = {arXiv},
arxivId = {1507.07244},
author = {Briggs, William M.},
eprint = {1507.07244},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Briggs - 2015 - The Crisis Of Evidence Why Probability And Statistics Cannot Discover Cause.pdf:pdf},
pages = {1--22},
title = {{The Crisis Of Evidence: Why Probability And Statistics Cannot Discover Cause}},
url = {http://arxiv.org/abs/1507.07244},
year = {2015}
}
@inproceedings{Drummond2009,
abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
author = {Drummond, Chris},
booktitle = {Evaluation Methods for Machine Learning Workshop at the 26th ICML},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Drummond - 2009 - Replicability is not Reproducibility Nor is it Good Science.pdf:pdf},
keywords = {Artificial Intelligence},
title = {{Replicability is not Reproducibility: Nor is it Good Science}},
year = {2009}
}
@article{Hand2013,
author = {Hand, D.J. and Anagnostopoulos, C.},
doi = {10.1016/j.patrec.2012.12.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand, Anagnostopoulos - 2013 - When is the area under the receiver operating characteristic curve an appropriate measure of classifier p.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {area under the curve},
month = {apr},
number = {5},
pages = {492--495},
title = {{When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512003923},
volume = {34},
year = {2013}
}
@article{Germain2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06944v1},
author = {Germain, Pascal},
eprint = {arXiv:1503.06944v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Germain - 2013 - PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers.pdf:pdf},
title = {{PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers}},
year = {2013}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zeiler, Fergus - 2013 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2013}
}
@inproceedings{VanderMaaten2013,
author = {{Van der Maaten}, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten et al. - 2013 - Learning with Marginalized Corrupted Features.pdf:pdf},
pages = {410--418},
title = {{Learning with Marginalized Corrupted Features}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}vandermaaten13},
year = {2013}
}
@article{Belkin2006,
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belkin, Niyogi, Sindhwani - 2006 - Manifold regularization A geometric framework for learning from labeled and unlabeled examples.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fold learning,graph transduction,kernel methods,mani-,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
pages = {2399--2434},
title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
url = {http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@article{Claassen2005,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2005 - A Logical Characterization of Constraint-Based Causal Discovery.pdf:pdf},
title = {{A Logical Characterization of Constraint-Based Causal Discovery}},
year = {2005}
}
@article{Pitt1988,
author = {Pitt, Leonard and Valiant, Leslie G.},
doi = {10.1145/48014.63140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitt, Valiant - 1988 - Computational limitations on learning from examples.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = {oct},
number = {4},
pages = {965--984},
title = {{Computational limitations on learning from examples}},
url = {http://portal.acm.org/citation.cfm?doid=48014.63140},
volume = {35},
year = {1988}
}
@article{Harville2013,
author = {Approach, Nondenominational Model-based and Harville, David A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Approach, Harville - 2013 - The Need for More Emphasis on Prediction a â€œNondenominationalâ€ Model-Based Approach.pdf:pdf},
journal = {The American Statistician},
month = {sep},
number = {September},
title = {{The Need for More Emphasis on Prediction: a â€œNondenominationalâ€ Model-Based Approach}},
year = {2013}
}
@article{Vehtari2015a,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We compute LOO using Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
eprint = {1507.04544},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vehtari, Gelman, Gabry - 2015 - Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian model.pdf:pdf},
keywords = {bayesian computation,k -fold cross-validation,leave-one-out cross-validation,loo,pareto smoothed importance sampling,stan,waic,widely applicable information criterion},
number = {July},
title = {{Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian models}},
url = {http://arxiv.org/abs/1507.04544},
year = {2015}
}
@article{Robinson1991a,
abstract = {In animal breeding, Best Linear Unbiased Predition, or BLUP, is a technique for estimating genetic merits. In general, it is a method of estimating random effects. It can be used to derive the Kalman filter, the method of Kriging used for ore reserve estimation ... Understanding of procedures for estimating random effects should help people to understand some complicated and controversial issues about fixed and random effects models and also help to bridge the apparent gulf between the Bayesian and Classical schools of thought.},
author = {Robinson, G. K.},
doi = {10.1214/ss/1177011926},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robinson - 1991 - That BLUP is a Good Thing The Estimation of Random Effects.pdf:pdf},
isbn = {08834237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,best linear unbiased prediction},
number = {1},
pages = {15--32},
title = {{That BLUP is a Good Thing: The Estimation of Random Effects}},
volume = {6},
year = {1991}
}
@article{Schmidt1995,
author = {Schmidt, Karsten and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt, Stahlecker - 1995 - Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {1--15},
title = {{Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949659508811648},
volume = {52},
year = {1995}
}
@article{Norton2003,
author = {Norton, John D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Norton - 2003 - A Material Theory of Induction.pdf:pdf},
journal = {Philosophy of Science},
number = {October},
pages = {647--670},
title = {{A Material Theory of Induction}},
volume = {70},
year = {2003}
}
@article{Telgarsky2015a,
abstract = {This paper proves, in very general settings, that convex risk minimization is a procedure to select a unique conditional probability model determined by the classification problem. Unlike most previous work, we give results that are general enough to include cases in which no minimum exists, as occurs typically, for instance, with standard boosting algorithms. Concretely, we first show that any sequence of predictors minimizing convex risk over the source distribution will converge to this unique model when the class of predictors is linear (but potentially of infinite dimension). Secondly, we show the same result holds for $\backslash$emph{\{}empirical{\}} risk minimization whenever this class of predictors is finite dimensional, where the essential technical contribution is a norm-free generalization bound.},
archivePrefix = {arXiv},
arxivId = {1506.04513},
author = {Telgarsky, Matus and Dud{\'{i}}k, Miroslav and Schapire, Robert},
eprint = {1506.04513},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Telgarsky, Dud{\'{i}}k, Schapire - 2015 - Convex Risk Minimization and Conditional Probability Estimation.pdf:pdf},
keywords = {classification,conditional probability estimation,consistency,convex duality,maximum entropy,orlicz spaces},
pages = {1--54},
title = {{Convex Risk Minimization and Conditional Probability Estimation}},
url = {http://arxiv.org/abs/1506.04513},
volume = {40},
year = {2015}
}
@article{Williamson,
author = {Williamson, Robert C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williamson - Unknown - Loss Functions.pdf:pdf},
number = {3},
pages = {1--10},
title = {{Loss Functions}}
}
@inproceedings{Rao1995,
author = {Rao, R. Bharat and Gordon, Diana and Spears, William},
booktitle = {Proceedings of the 12th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rao, Gordon, Spears - 1995 - For Every Generalization Action, Is there really an Equal and Opposite Reaction Analysis of the conservatio.pdf:pdf},
pages = {471--479},
title = {{For Every Generalization Action, Is there really an Equal and Opposite Reaction? Analysis of the conservation Law for Generalization Performance}},
url = {http://www.researchgate.net/publication/2516136{\_}For{\_}Every{\_}Generalization{\_}Action{\_}Is{\_}There{\_}Really{\_}An{\_}Equal{\_}And{\_}Opposite{\_}Reaction{\_}Analysis{\_}of{\_}the{\_}Conservation{\_}Law{\_}for{\_}Generalization{\_}Performance/file/79e4150b866697f897.pdf},
year = {1995}
}
@article{Hsu2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2363v1},
author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
eprint = {arXiv:1106.2363v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hsu, Kakade, Zhang - 2011 - An analysis of random design linear regression.pdf:pdf},
journal = {arXiv preprint},
title = {{An analysis of random design linear regression}},
url = {http://arxiv.org/abs/1106.2363},
year = {2011}
}
@article{Springenberg2015,
abstract = {In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
archivePrefix = {arXiv},
arxivId = {1511.06390},
author = {Springenberg, Jost Tobias},
eprint = {1511.06390},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Springenberg - 2015 - Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.pdf:pdf},
number = {2009},
pages = {1--20},
title = {{Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06390},
year = {2015}
}
@article{Claassen2010,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2010 - Learning causal network structure from multiple (in) dependence models.pdf:pdf},
journal = {Proceedings of the Fifth European Workshop on Probabilistic Graphical Models},
title = {{Learning causal network structure from multiple (in) dependence models}},
year = {2010}
}
@article{Meding2012,
abstract = {In clinical diagnostics, it is of outmost importance to correctly identify the source of a metastatic tumor, especially if no apparent primary tumor is present. Tissue-based proteomics might allow correct tumor classification. As a result, we performed MALDI imaging to generate proteomic signatures for different tumors. These signatures were used to classify common cancer types. At first, a cohort comprised of tissue samples from six adenocarcinoma entities located at different organ sites (esophagus, breast, colon, liver, stomach, thyroid gland, n = 171) was classified using two algorithms for a training and test set. For the test set, Support Vector Machine and Random Forest yielded overall accuracies of 82.74 and 81.18{\%}, respectively. Then, colon cancer liver metastasis samples (n = 19) were introduced into the classification. The liver metastasis samples could be discriminated with high accuracy from primary tumors of colon cancer and hepatocellular carcinoma. Additionally, colon cancer liver metastasis samples could be successfully classified by using colon cancer primary tumor samples for the training of the classifier. These findings demonstrate that MALDI imaging-derived proteomic classifiers can discriminate between different tumor types at different organ sites and in the same site.},
author = {Meding, Stephan and Nitsche, Ulrich and Balluff, Benjamin and Elsner, Mareike and Rauser, Sandra and Sch{\"{o}}ne, C{\'{e}}drik and Nipp, Martin and Maak, Matthias and Feith, Marcus and Ebert, Matthias P and Friess, Helmut and Langer, Rupert and H{\"{o}}fler, Heinz and Zitzelsberger, Horst and Rosenberg, Robert and Walch, Axel},
doi = {10.1021/pr200784p},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meding et al. - 2012 - Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Adenocarcinoma,Adenocarcinoma: metabolism,Adenocarcinoma: secondary,Algorithms,Humans,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Neoplasms: pathology,Proteome,Proteome: metabolism,Proteomics,Sensitivity and Specificity,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Support Vector Machines},
month = {mar},
number = {3},
pages = {1996--2003},
pmid = {22224404},
title = {{Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22224404},
volume = {11},
year = {2012}
}
@article{Hand2014,
author = {Hand, David J.},
doi = {10.1214/13-STS446},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2014 - Wonderful Examples, but Let's not Close Our Eyes.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Frequentist, likelihood inference, Neyman-Pearson,and phrases,frequentist,hypothesis testing,informative and thought-provoking,likelihood inference,making specific comments,neyman,on each of these,pearson,schools of inference,space prohibits me from},
month = {feb},
number = {1},
pages = {98--100},
title = {{Wonderful Examples, but Let's not Close Our Eyes}},
url = {http://projecteuclid.org/euclid.ss/1399645735},
volume = {29},
year = {2014}
}
@article{Ambikasaran2016,
abstract = {A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the {\$}n{\$} -dimensional setting, however, it requires the inversion of an {\$}n times n{\$} covariance matrix, {\$}C{\$} , as well as the evaluation of its determinant, {\$}det (C){\$} . In many cases, such as regression using Gaussian processes, the covariance matrix is of the form {\$}C = sigma {\^{}}2 I + K{\$} , where {\$}K{\$} is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix {\$}C{\$} is typically dense, causing standard direct methods for inversion and determinant evaluation to require {\$}mathcal {\{}O{\}}(n{\^{}}3){\$} work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix {\$}C{\$} can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an {\$}mathcal {\{}O{\}} (n,log{\^{}}2, n){\$} algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant {\$}det (C){\$}, permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining {\$}K{\$} . Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels.},
archivePrefix = {arXiv},
arxivId = {1403.6015},
author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
doi = {10.1109/TPAMI.2015.2448083},
eprint = {1403.6015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ambikasaran et al. - 2016 - Fast Direct Methods for Gaussian Processes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Acceleration,Approximation methods,Bayesian analysis,Covariance matrices,Gaussian process,Gaussian processes,Kernel,Matrix decomposition,Symmetric matrices,covariance function,covariance matrix,determinant,direct solver,evidence,fast multipole method,hierarchical off-diagonal low-rank,likelihood},
number = {2},
pages = {252--265},
title = {{Fast Direct Methods for Gaussian Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7130620},
volume = {38},
year = {2016}
}
@article{Kelejian1969,
author = {Kelejian, H.H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kelejian - 1969 - Missing Observations in Multivaraite Regression Efficiency of a First-Order Method.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1609--1616},
title = {{Missing Observations in Multivaraite Regression: Efficiency of a First-Order Method}},
volume = {64},
year = {1969}
}
@inproceedings{Duin2002,
author = {Pekalska, Ella and Duin, Robert P.W. and Skurichina, Marina},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pekalska, Duin, Skurichina - 2002 - A discussion on the classifier projection space for classifier combining.pdf:pdf},
pages = {137--148},
title = {{A discussion on the classifier projection space for classifier combining}},
url = {http://www.springerlink.com/index/A98FBKT93AK0YNNE.pdf},
year = {2002}
}
@article{Ramdas2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04214v1},
author = {Ramdas, Aaditya and Singh, Aarti},
doi = {10.1109/GlobalSIP.2013.6737091},
eprint = {arXiv:1505.04214v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ramdas, Singh - 2013 - Exploring the intersection of active learning and stochastic convex optimization.pdf:pdf},
isbn = {9781479902484},
issn = {03029743},
journal = {2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2013 - Proceedings},
keywords = {Active learning,Adaptive algorithms,Stochastic convex optimization,Tsybakov noise condition,Uniform convexity},
pages = {1122},
title = {{Exploring the intersection of active learning and stochastic convex optimization}},
year = {2013}
}
@article{Kyrillidis2015,
abstract = {Compressive sensing (CS) exploits sparsity to recover sparse or compressible signals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity is also used to enhance interpretability in machine learning and statistics applications: While the ambient dimension is vast in modern data analysis problems, the relevant information therein typically resides in a much lower dimensional space. However, many solutions proposed nowadays do not leverage the true underlying structure. Recent results in CS extend the simple sparsity idea to more sophisticated {\{}$\backslash$em structured{\}} sparsity models, which describe the interdependency between the nonzero components of a signal, allowing to increase the interpretability of the results and lead to better recovery performance. In order to better understand the impact of structured sparsity, in this chapter we analyze the connections between the discrete models and their convex relaxations, highlighting their relative advantages. We start with the general group sparse model and then elaborate on two important special cases: the dispersive and the hierarchical models. For each, we present the models in their discrete nature, discuss how to solve the ensuing discrete problems and then describe convex relaxations. We also consider more general structures as defined by set functions and present their convex proxies. Further, we discuss efficient optimization solutions for structured sparsity problems and illustrate structured sparsity in action via three applications.},
archivePrefix = {arXiv},
arxivId = {1507.05367},
author = {Kyrillidis, Anastasios and Baldassarre, Luca and El-Halabi, Marwa and Tran-Dinh, Quoc and Cevher, Volkan},
eprint = {1507.05367},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kyrillidis et al. - 2015 - Structured Sparsity Discrete and Convex approaches.pdf:pdf},
pages = {1--30},
title = {{Structured Sparsity: Discrete and Convex approaches}},
url = {http://arxiv.org/abs/1507.05367},
year = {2015}
}
@article{Skurichina1999,
author = {Skurichina, Marina and Duin, Robert P. W.},
doi = {10.1007/s100440050013},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 1999 - Regularisation of Linear Classifiers by Adding Redundant Features.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis {\&} Applications},
keywords = {critical sample size,generalisation error,noise injection,peaking behaviour,pseudo fisher linear discriminant,regularisation},
number = {1},
pages = {44--52},
title = {{Regularisation of Linear Classifiers by Adding Redundant Features}},
volume = {2},
year = {1999}
}
@inproceedings{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Algorithmic Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2011 - Domain adaptation in regression.pdf:pdf},
title = {{Domain adaptation in regression}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24412-4{\_}25},
year = {2011}
}
@article{Rubin2005,
abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanismâ€”a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on s...},
author = {Rubin, Donald B},
doi = {10.1198/016214504000001880},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:pdf},
isbn = {0162-1459$\backslash$r1537-274X},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {analysis of covariance,assignment mechanism,assignment-based causal inference,bayesian inference,creasy,direct causal,effects,fieller,fisher,neyman,observational studies,principal stratification,randomized experiments,rubin},
number = {469},
pages = {322--331},
title = {{Causal Inference Using Potential Outcomes}},
volume = {100},
year = {2005}
}
@article{Dhillon2013,
author = {Dhillon, Paramveer S. and Foster, Dean P. and Kakade, Sham M. and Ungar, Lyle H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dhillon et al. - 2013 - A Risk Comparison of Ordinary Least Squares vs Ridge Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {pca,ridge regression,risk inflation},
pages = {1505--1511},
title = {{A Risk Comparison of Ordinary Least Squares vs Ridge Regression}},
url = {http://adsabs.harvard.edu/abs/2011arXiv1105.0875D},
volume = {14},
year = {2013}
}
@article{Raghu2016,
archivePrefix = {arXiv},
arxivId = {1606.05336},
author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
eprint = {1606.05336},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raghu et al. - 2016 - On the expressive power of deep neural networks.pdf:pdf},
title = {{On the expressive power of deep neural networks}},
year = {2016}
}
@article{Schmidt1996,
author = {Schmidt, Karsten},
doi = {10.1007/BF00046993},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt - 1996 - A comparison of minimax and least squares estimators in linear regression with polyhedral prior information.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {1,3,average performance,estimator,inequality restricted least squares,minimax estima-,n vector of,parameter restrictions,polyhedral prior information in,projection estimators,regression model y,the linear regression model,tion,u,we consider the linear,where y is an,x},
month = {apr},
number = {1},
pages = {127--138},
title = {{A comparison of minimax and least squares estimators in linear regression with polyhedral prior information}},
url = {http://link.springer.com/10.1007/BF00046993},
volume = {43},
year = {1996}
}
@article{Bontempi2014,
abstract = {The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with {\$}n{\textgreater}2{\$} variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for {\$}n{\textgreater}2{\$} variate distributions.},
archivePrefix = {arXiv},
arxivId = {1412.6285},
author = {Bontempi, Gianluca and Flauder, Maxime},
eprint = {1412.6285},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bontempi, Flauder - 2014 - From dependency to causality a machine learning approach.pdf:pdf},
journal = {arXiv preprint arXiv:1412.6285},
keywords = {causal inference,information theory,machine learning},
pages = {1--24},
title = {{From dependency to causality: a machine learning approach}},
url = {http://arxiv.org/abs/1412.6285},
volume = {16},
year = {2014}
}
@article{Tanha2015,
author = {Tanha, Jafar and van Someren, Maarten and Afsarmanesh, Hamideh},
doi = {10.1007/s13042-015-0328-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tanha, van Someren, Afsarmanesh - 2015 - Semi-supervised self-training for decision tree classifiers.pdf:pdf},
isbn = {1304201503},
issn = {1868-8071},
journal = {International Journal of Machine Learning and Cybernetics},
number = {JANUARY},
title = {{Semi-supervised self-training for decision tree classifiers}},
url = {http://link.springer.com/10.1007/s13042-015-0328-7},
year = {2015}
}
@article{Skurichina2000,
author = {Skurichina, Marina and Duin, Robert P.W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 2000 - The role of combining rules in bagging and boosting.pdf:pdf},
journal = {Advances in Pattern Recognition},
pages = {631--640},
title = {{The role of combining rules in bagging and boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-44522-6{\_}65},
year = {2000}
}
@article{Xu2009,
address = {New York, New York, USA},
author = {Xu, Linli and White, Martha and Schuurmans, Dale},
doi = {10.1145/1553374.1553519},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, White, Schuurmans - 2009 - Optimal reverse prediction.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Optimal reverse prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553519},
year = {2009}
}
@article{Hand2009,
author = {Hand, David J.},
doi = {10.1007/s10994-009-5119-5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
month = {jun},
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: aÂ coherent alternative to the area under the ROC curve}},
url = {http://link.springer.com/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@article{McLachlan1975,
author = {McLachlan, Geoffrey J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1975 - Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant An.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {365--369},
title = {{Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis}},
volume = {70},
year = {1975}
}
@article{Grunwald2007a,
abstract = {We show that forms of Bayesian and MDL inference that are often applied to classification problems can be {\{}$\backslash$em inconsistent{\}}.  This means that there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error. We extensively discuss the result from both a Bayesian and an MDL perspective.},
archivePrefix = {arXiv},
arxivId = {math/0406221},
author = {Gr{\"{u}}nwald, Peter and Langford, John},
doi = {10.1007/s10994-007-0716-7},
eprint = {0406221},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Langford - 2007 - Suboptimal behavior of Bayes and MDL in classification under misspecification.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Bayesian statistics,Classification,Consistency,Inconsistency,Minimum description length,Misspecification},
number = {2-3},
pages = {119--149},
primaryClass = {math},
title = {{Suboptimal behavior of Bayes and MDL in classification under misspecification}},
volume = {66},
year = {2007}
}
@article{Healy1956,
author = {Healy, Michael and Westmacott, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Healy, Westmacott - 1956 - Missing Values in Experiments Analysed on Automatic Computers.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {3},
pages = {203--206},
title = {{Missing Values in Experiments Analysed on Automatic Computers}},
volume = {5},
year = {1956}
}
@misc{Lichman2013,
author = {Lichman, M.},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Hullermeier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0698v1},
author = {H{\"{u}}llermeier, Eyke},
doi = {10.1016/j.ijar.2013.09.003},
eprint = {arXiv:1305.0698v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/H{\"{u}}llermeier - 2013 - Learning from imprecise and fuzzy observations Data disambiguation through generalized loss minimization.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {data disambiguation,extension principle,fuzzy sets,imprecise data,inductive bias,logistic,loss function,machine learning,risk minimization},
pages = {1--16},
title = {{Learning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0888613X13001722},
volume = {1},
year = {2013}
}
@inproceedings{Kopf2000,
author = {K{\"{o}}pf, Christian and Taylor, Charles and Keller, Jorg},
booktitle = {Proceedings of the PKDD-00 workshop on data mining, decision support, meta-learning and ILP},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{o}}pf, Taylor, Keller - 2000 - Meta-analysis from data characterisation for meta-learning to meta-regression.pdf:pdf},
number = {Ml},
title = {{Meta-analysis: from data characterisation for meta-learning to meta-regression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.8159},
year = {2000}
}
@article{Wang2015a,
author = {Wang, Fang and Li, Renfu and Lei, Zhikun and Ni, Xuelei Sherry and Huo, Xiaoming and Chen, Ming},
doi = {10.1016/j.patrec.2015.06.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dimension reduction,Semi-supervised learning,kernel fusion-refinement},
publisher = {Elsevier Ltd.},
title = {{Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001671},
year = {2015}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Courville, Vincent - 2013 - Representation learning a review and new perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
month = {aug},
number = {8},
pages = {1798--828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@book{Newport,
author = {Newport, Cal},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Newport - Unknown - Deep Work.pdf:pdf},
title = {{Deep Work}}
}
@inproceedings{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz, Singer - 2007 - Pegasos Primal estimated sub-gradient solver for svm.pdf:pdf},
pages = {807--814},
title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
url = {http://link.springer.com/article/10.1007/s10107-010-0420-4},
year = {2007}
}
@article{Betancourt2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02273v1},
author = {Betancourt, Michael},
eprint = {arXiv:1506.02273v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - A Unified Treatment of Predictive Model Comparison arXiv 1506 . 02273v1 stat . ME 7 Jun 2015.pdf:pdf},
pages = {1--20},
title = {{A Unified Treatment of Predictive Model Comparison arXiv : 1506 . 02273v1 [ stat . ME ] 7 Jun 2015}},
year = {2015}
}
@article{Kawakita2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.3965v1},
author = {Kawakita, Masanori and Kanamori, Takafumi},
eprint = {arXiv:1204.3965v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Kanamori - 2013 - Semi-Supervised learning with Density-Ratio Estimation.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {189--209},
title = {{Semi-Supervised learning with Density-Ratio Estimation}},
volume = {91},
year = {2013}
}
@inproceedings{Ho2000,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2000 - Complexity of Classification Problems and Comparative Advantages of Combined Classifiers.pdf:pdf},
pages = {97--106},
title = {{Complexity of Classification Problems and Comparative Advantages of Combined Classifiers}},
year = {2000}
}
@article{Sugiyama2009,
author = {Sugiyama, Masashi and Id{\'{e}}, Tsuyoshi and Nakajima, Shinichi and Sese, Jun},
doi = {10.1007/s10994-009-5125-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama et al. - 2009 - Semi-supervised local Fisher discriminant analysis for dimensionality reduction.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1-2},
pages = {35--61},
title = {{Semi-supervised local Fisher discriminant analysis forÂ dimensionality reduction}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5125-7},
volume = {78},
year = {2009}
}
@article{Wasserman2009,
author = {Wasserman, Larry and Roeder, Kathryn},
doi = {10.1214/08-AOS646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman, Roeder - 2009 - High dimensional variable selection.pdf:pdf},
journal = {Annals of statistics},
number = {5},
pages = {2178--2201},
title = {{High dimensional variable selection}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2752029/},
volume = {37},
year = {2009}
}
@article{Kleijn2006a,
abstract = {We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution {\$}P{\_}0{\$}, which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback--Leibler divergence with respect to {\$}P{\_}0{\$}. An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models.},
archivePrefix = {arXiv},
arxivId = {math/0607023},
author = {Kleijn, B. J K and {Van Der Vaart}, a. W.},
doi = {10.1214/009053606000000029},
eprint = {0607023},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleijn, Van Der Vaart - 2006 - Misspecification in infinite-dimensional Bayesian statistics.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Infinite-dimensional model,Misspecification,Posterior distribution,Rate of convergence},
number = {2},
pages = {837--877},
primaryClass = {math},
title = {{Misspecification in infinite-dimensional Bayesian statistics}},
volume = {34},
year = {2006}
}
@inproceedings{Collobert2006a,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://dl.acm.org/citation.cfm?id=1143870},
year = {2006}
}
@article{Leek2007,
abstract = {It has unambiguously been shown that genetic, environmental, demographic, and technical factors may have substantial effects on gene expression levels. In addition to the measured variable(s) of interest, there will tend to be sources of signal due to factors that are unknown, unmeasured, or too complicated to capture through simple models. We show that failing to incorporate these sources of heterogeneity into an analysis can have widespread and detrimental effects on the study. Not only can this reduce power or induce unwanted dependence across genes, but it can also introduce sources of spurious signal to many genes. This phenomenon is true even for well-designed, randomized studies. We introduce "surrogate variable analysis" (SVA) to overcome the problems caused by heterogeneity in expression studies. SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest. We apply SVA to disease class, time course, and genetics of gene expression studies. We show that SVA increases the biological accuracy and reproducibility of analyses in genome-wide expression studies.},
author = {Leek, Jeffrey T. and Storey, John D.},
doi = {10.1371/journal.pgen.0030161},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leek, Storey - 2007 - Capturing heterogeneity in gene expression studies by surrogate variable analysis.pdf:pdf},
isbn = {1553-7404 (Electronic)$\backslash$n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {9},
pages = {1724--1735},
pmid = {17907809},
title = {{Capturing heterogeneity in gene expression studies by surrogate variable analysis}},
volume = {3},
year = {2007}
}
@article{Lei2015,
author = {Lei, Zhikun and Li, Renfu and {Sherry Ni}, Xuelei and Huo, Xiaoming},
doi = {10.1016/j.sigpro.2015.03.003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lei et al. - 2015 - High-dimensional semi-supervised learning via a fusion-refinement procedure.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Fusion-refinement (FR) procedure,Semi-supervised learning (SSL),Sufficient dimension reduction (SDR),fr,fusion-refinement,procedure,sdr,semi-supervised learning,ssl,sufficient dimension reduction},
pages = {171--182},
publisher = {Elsevier},
title = {{High-dimensional semi-supervised learning via a fusion-refinement procedure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168415000961},
volume = {114},
year = {2015}
}
@article{Welinder2013,
author = {Welinder, Peter and Welling, Max and Perona, Pietro},
doi = {10.1109/CVPR.2013.419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welinder, Welling, Perona - 2013 - A Lazy Man's Approach to Benchmarking Semisupervised Classifier Evaluation and Recalibration.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3262--3269},
publisher = {Ieee},
title = {{A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619263},
year = {2013}
}
@article{Dougherty2001,
abstract = {In order to study the molecular biological differences between normal and diseased tissues, it is desirable to perform classification among diseases and stages of disease using microarray-based gene-expression values. Owing to the limited number of microarrays typically used in these studies, serious issues arise with respect to the design, performance and analysis of classifiers based on microarray data. This paper reviews some fundamental issues facing small-sample classification: classification rules, constrained classifiers, error estimation and feature selection. It discusses both unconstrained and constrained classifier design from sample data, and the contributions to classifier error from constrained optimization and lack of optimality owing to design from sample data. The difficulty with estimating classifier error when confined to small samples is addressed, particularly estimating the error from training data. The impact of small samples on the ability to include more than a few variables as classifier features is explained.},
author = {Dougherty, Edward R.},
doi = {10.1002/cfg.62},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dougherty - 2001 - Small sample issues for microarray-based classification.pdf:pdf},
issn = {1531-6912},
journal = {Comparative and functional genomics},
month = {jan},
number = {1},
pages = {28--34},
pmid = {18628896},
title = {{Small sample issues for microarray-based classification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447190{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2001}
}
@article{Gelman2013b,
abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by an attitude among some theoreticians that Bayesian methods were absurdâ€”notmerely misguided but obviously wrong in prin- ciple. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We an- alyze in this note the intellectual background behind various misconcep- tions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.},
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5366v5},
author = {Gelman, Andrew and Robert, Christian P.},
doi = {10.1080/00031305.2013.760987},
eprint = {arXiv:1006.5366v5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Robert - 2013 - â€œNot Only Defended But Also Appliedâ€ The Perceived Absurdity of Bayesian Inference(2).pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian,bogosity,doomsdsay argument,foundations,frequentist,laplace law of succession},
number = {1},
pages = {1--5},
title = {{â€œNot Only Defended But Also Appliedâ€: The Perceived Absurdity of Bayesian Inference}},
url = {http://basepub.dauphine.fr/handle/123456789/11069$\backslash$nhttp://www.tandfonline.com/doi/abs/10.1080/00031305.2013.760987},
volume = {67},
year = {2013}
}
@inproceedings{Sindhwani2005,
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Niyogi, Belkin - 2005 - Beyond the point cloud from transductive to semi-supervised learning.pdf:pdf},
number = {0},
pages = {824--831},
title = {{Beyond the point cloud: from transductive to semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102455},
year = {2005}
}
@phdthesis{Marlin2008,
author = {Marlin, BM},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marlin - 2008 - Missing data problems in machine learning.pdf:pdf},
school = {University of Toronto},
title = {{Missing data problems in machine learning}},
url = {http://www-devel.cs.ubc.ca/{~}bmarlin/research/phd{\_}thesis/marlin-phd-thesis.pdf},
year = {2008}
}
@article{Louppe2015a,
abstract = {Author name disambiguation in bibliographic databases is the problem of grouping together scientific publications written by the same person, accounting for potential homonyms and/or synonyms. Among solutions to this problem, digital libraries are increasingly offering tools for authors to manually curate their publications and claim those that are theirs. Indirectly, these tools allow for the inexpensive collection of large annotated training data, which can be further leveraged to build a complementary automated disambiguation system capable of inferring patterns for identifying publications written by the same person. Building on more than 1 million publicly released crowdsourced annotations, we propose an automated author disambiguation solution exploiting this data (i) to learn an accurate classifier for identifying coreferring authors and (ii) to guide the clustering of scientific publications by distinct authors in a semi-supervised way. To the best of our knowledge, our analysis is the first to be carried out on data of this size and coverage. With respect to the state of the art, we validate the general pipeline used in most existing solutions, and improve by: (i) proposing phonetic-based blocking strategies, thereby increasing recall; and (ii) adding strong ethnicity-sensitive features for learning a linkage function, thereby tailoring disambiguation to non-Western author names whenever necessary.},
archivePrefix = {arXiv},
arxivId = {1508.07744},
author = {Louppe, Gilles and Al-Natsheh, Hussein and Susik, Mateusz and Maguire, Eamonn},
eprint = {1508.07744},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Louppe et al. - 2015 - Ethnicity sensitive author disambiguation using semi-supervised learning.pdf:pdf},
pages = {1--14},
title = {{Ethnicity sensitive author disambiguation using semi-supervised learning}},
url = {http://arxiv.org/abs/1508.07744},
year = {2015}
}
@article{Li2009a,
abstract = {Semi-Supervised Support Vector Machines (S3VMs) typically directly$\backslash$nestimate the label assignments for the unlabeled instances. This$\backslash$nis often inefficient even with recent advances in the efficient training$\backslash$nof the (supervised) SVM. In this paper, we show that S3VMs, with$\backslash$nknowledge of the means of the class labels of the unlabeled data,$\backslash$nis closely related to the supervised SVM with known labels on all$\backslash$nthe unlabeled data. This motivates us to first estimate the label$\backslash$nmeans of the unlabeled data. Two versions of the meanS3VM, which$\backslash$nwork by maximizing the margin between the label means, are proposed.$\backslash$nThe first one is based on multiple kernel learning, while the second$\backslash$none is based on alternating optimization. Experiments show that both$\backslash$nof the proposed algorithms achieve highly competitive and sometimes$\backslash$neven the best performance as compared to the state-of-the-art semi-supervised$\backslash$nlearners. Moreover, they are more efficient than existing S3VMs.},
author = {Li, Yu-Feng and Kwok, James T. and Zhou, Zhi-hua},
doi = {10.1145/1553374.1553456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Kwok, Zhou - 2009 - Semi-supervised learning using label mean(2).pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {633--640},
title = {{Semi-supervised learning using label mean}},
url = {http://dl.acm.org/citation.cfm?id=1553456},
year = {2009}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10/100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1603.05027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{Identity Mappings in Deep Residual Networks}},
url = {http://arxiv.org/abs/1603.05027},
year = {2016}
}
@inproceedings{Szummer2001,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2001 - Clustering and efficient use of unlabeled examples.pdf:pdf},
title = {{Clustering and efficient use of unlabeled examples}},
url = {http://www.ai.mit.edu/projects/ntt/projects/MIT2000-08/documents/SzummerJaakkola.pdf},
year = {2001}
}
@article{Matthews2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.07027v1},
author = {Matthews, Alexander G De G and Hensman, James and Turner, Richard E and Ghahramani, Zoubin},
eprint = {arXiv:1504.07027v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Matthews et al. - 2015 - On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes.pdf:pdf},
number = {1},
pages = {1--8},
title = {{On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes}},
year = {2015}
}
@article{Shcherbatyi2016,
abstract = {Regularized empirical risk minimization with constrained labels (in contrast to fixed labels) is a remarkably general abstraction of learning. For common loss and regularization functions, this optimization problem assumes the form of a mixed integer program (MIP) whose objective function is non-convex. In this form, the problem is resistant to standard optimization techniques. We construct MIPs with the same solutions whose objective functions are convex. Specifically, we characterize the tightest convex extension of the objective function, given by the Legendre-Fenchel biconjugate. Computing values of this tightest convex extension is NP-hard. However, by applying our characterization to every function in an additive decomposition of the objective function, we obtain a class of looser convex extensions that can be computed efficiently. For some decompositions, common loss and regularization functions, we derive a closed form.},
archivePrefix = {arXiv},
arxivId = {1602.06746},
author = {Shcherbatyi, Iaroslav and Andres, Bjoern},
eprint = {1602.06746},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shcherbatyi, Andres - 2016 - Convexification of Learning from Constraints.pdf:pdf},
number = {1},
pages = {1--13},
title = {{Convexification of Learning from Constraints}},
url = {http://arxiv.org/abs/1602.06746},
year = {2016}
}
@article{Chatterjee2014a,
author = {Chatterjee, Sourav},
doi = {10.1214/14-AOS1254},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee - 2014 - A new perspective on least squares under convex constraint.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,convex constraint,empirical pro-,least squares,maximum likelihood},
month = {dec},
number = {6},
pages = {2340--2381},
title = {{A new perspective on least squares under convex constraint}},
url = {http://projecteuclid.org/euclid.aos/1413810730},
volume = {42},
year = {2014}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
edition = {Second},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {0387848576},
publisher = {Spinger},
title = {{The Elements of Statistical Learning}},
year = {2009}
}
@misc{Shalev-Shwartz2014,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
year = {2014}
}
@article{Bach2010,
abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its $\backslash$lova extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
archivePrefix = {arXiv},
arxivId = {1008.4220},
author = {Bach, Francis},
eprint = {1008.4220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bach - 2010 - Structured sparsity-inducing norms through submodular functions.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems NIPS'2010},
pages = {1--9},
title = {{Structured sparsity-inducing norms through submodular functions}},
url = {http://arxiv.org/abs/1008.4220},
year = {2010}
}
@article{Yoder2016,
abstract = {Traditionally, practitioners initialize the {\{}$\backslash$tt k-means{\}} algorithm with centers chosen uniformly at random. Randomized initialization with uneven weights ({\{}$\backslash$tt k-means++{\}}) has recently been used to improve the performance over this strategy in cost and run-time. We consider the k-means problem with semi-supervised information, where some of the data are pre-labeled, and we seek to label the rest according to the minimum cost solution. By extending the {\{}$\backslash$tt k-means++{\}} algorithm and analysis to account for the labels, we derive an improved theoretical bound on expected cost and observe improved performance in simulated and real data examples. This analysis provides theoretical justification for a roughly linear semi-supervised clustering algorithm.},
archivePrefix = {arXiv},
arxivId = {1602.00360},
author = {Yoder, Jordan and Priebe, Carey E.},
eprint = {1602.00360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yoder, Priebe - 2016 - Semi-supervised K-means.pdf:pdf},
keywords = {approximation,clustering,kmeans,partially labeled,semi-supervised},
number = {1},
pages = {1--16},
title = {{Semi-supervised K-means++}},
url = {http://arxiv.org/abs/1602.00360},
year = {2016}
}
@unpublished{Blum2001,
author = {Blum, Avrim and Chawla, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Chawla - 2001 - Learning from labeled and unlabeled data using graph mincuts.pdf:pdf},
institution = {Carnegie Mellon University, Computer Science Department},
title = {{Learning from labeled and unlabeled data using graph mincuts}},
url = {http://repository.cmu.edu/compsci/163/?utm{\_}source=repository.cmu.edu{\%}2Fcompsci{\%}2F163{\&}utm{\_}medium=PDF{\&}utm{\_}campaign=PDFCoverPages},
year = {2001}
}
@article{Jordan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1309.7804v1},
author = {Jordan, Michael I.},
doi = {10.3150/12-BEJSP17},
eprint = {arXiv:1309.7804v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan - 2013 - On statistics, computation and scalability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {()},
month = {sep},
number = {4},
pages = {1378--1390},
title = {{On statistics, computation and scalability}},
url = {http://projecteuclid.org/euclid.bj/1377612856},
volume = {19},
year = {2013}
}
@article{Tian2016,
abstract = {In this paper, we propose a strategy dealing with the semi-supervised classification problem, in which the support vector machine with self-constructed Universum is iteratively solved. Universum data, which do not belong to either class of interest, have been illustrated to encode some prior knowledge by representing meaningful concepts in the same domain as the problem at hand. Our new method is applied to seek more reliable positive and negative examples from the unlabeled dataset step by step, and the Universum support vector machine(U-SVM) is used iteratively. Different Universum data will result in different performance, so several effective approaches are explored to construct Universum datasets. Experimental results demonstrate that appropriately constructed Universum will improve the accuracy and reduce the number of iterations.},
author = {Tian, Yingjie and Zhang, Ying and Liu, Dalian},
doi = {10.1016/j.neucom.2015.11.041},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tian, Zhang, Liu - 2016 - Semi-supervised support vector classification with self-constructed Universum.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Classification,Semi-supervised,Support vector machine,Universum},
pages = {33--42},
publisher = {Elsevier},
title = {{Semi-supervised support vector classification with self-constructed Universum}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.11.041},
volume = {189},
year = {2016}
}
@article{Lopez-Paz2014,
abstract = {We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions regarding the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, and without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's "Fast Causation Coefficient Challenge" competition, with which we won the fastest code prize and ranked third in the overall leaderboard.},
archivePrefix = {arXiv},
arxivId = {1409.4366},
author = {Lopez-Paz, David and Muandet, Krikamol and Recht, Benjamin},
eprint = {1409.4366},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-Paz, Muandet, Recht - 2014 - The Randomized Causation Coefficient.pdf:pdf},
journal = {arXiv preprint arXiv:1409.4366},
keywords = {causality,cause-effect inference,kernel mean embeddings,random features},
pages = {1--4},
title = {{The Randomized Causation Coefficient}},
url = {http://arxiv.org/abs/1409.4366},
volume = {16},
year = {2014}
}
@article{Targ2016,
archivePrefix = {arXiv},
arxivId = {1603.08029},
author = {Targ, Sasha and Almeida, Diogo and Lyman, Kevin},
eprint = {1603.08029},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Targ, Almeida, Lyman - 2016 - ResNet In ResNet Generalizing Residual Architectures.pdf:pdf},
number = {1},
pages = {1--4},
title = {{ResNet In ResNet: Generalizing Residual Architectures}},
year = {2016}
}
@article{Jones2012,
author = {Jones, Emrys A and Deininger, S{\"{o}}ren-oliver and Hogendoorn, Pancras C W and Deelder, Andr{\'{e}} M and Mcdonnell, Liam A},
doi = {10.1016/j.jprot.2012.06.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2012 - Imaging mass spectrometry statistical analysis.pdf:pdf},
issn = {1874-3919},
journal = {Journal of Proteomics},
keywords = {Biomarker discovery,Data analysis,Molecular histology,imaging mass spectrometry},
number = {16},
pages = {4962--4989},
publisher = {Elsevier B.V.},
title = {{Imaging mass spectrometry statistical analysis}},
url = {http://dx.doi.org/10.1016/j.jprot.2012.06.014},
volume = {75},
year = {2012}
}
@inproceedings{Mooij,
author = {Mooij, Joris M and Heskes, Tom and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2011 - On Causal Discovery with Cyclic Additive Noise Models.pdf:pdf},
pages = {639--647},
title = {{On Causal Discovery with Cyclic Additive Noise Models}},
year = {2011}
}
@inproceedings{Cozman2002,
author = {Cozman, FG and Cohen, Ira},
booktitle = {FLAIRS Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen - 2002 - Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.pdf:pdf},
pages = {327--331},
title = {{Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.}},
url = {http://www.aaai.org/Papers/FLAIRS/2002/FLAIRS02-065.pdf},
year = {2002}
}
@article{Bousquet2004,
author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bousquet, Boucheron, Lugosi - 2004 - Introduction to statistical learning theory.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {169--207},
title = {{Introduction to statistical learning theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@article{Sejdinovic2013,
author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
doi = {10.1214/13-AOS1140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based statistics in hypothesis testing.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2263--2291},
title = {{Equivalence of distance-based and RKHS-based statistics in hypothesis testing}},
url = {http://projecteuclid.org/euclid.aos/1383661264},
volume = {41},
year = {2013}
}
@article{Bruand2011,
abstract = {Mass Spectrometric Imaging (MSI) is a molecular imaging technique that allows the generation of 2D ion density maps for a large complement of the active molecules present in cells and sectioned tissues. Automatic segmentation of such maps according to patterns of co-expression of individual molecules can be used for discovery of novel molecular signatures (molecules that are specifically expressed in particular spatial regions). However, current segmentation techniques are biased toward the discovery of higher abundance molecules and large segments; they allow limited opportunity for user interaction, and validation is usually performed by similarity to known anatomical features. We describe here a novel method, AMASS (Algorithm for MSI Analysis by Semi-supervised Segmentation). AMASS relies on the discriminating power of a molecular signal instead of its intensity as a key feature, uses an internal consistency measure for validation, and allows significant user interaction and supervision as options. An automated segmentation of entire leech embryo data images resulted in segmentation domains congruent with many known organs, including heart, CNS ganglia, nephridia, nephridiopores, and lateral and ventral regions, each with a distinct molecular signature. Likewise, segmentation of a rat brain MSI slice data set yielded known brain features and provided interesting examples of co-expression between distinct brain regions. AMASS represents a new approach for the discovery of peptide masses with distinct spatial features of expression. Software source code and installation and usage guide are available at http://bix.ucsd.edu/AMASS/ .},
author = {Bruand, Jocelyne and Alexandrov, Theodore and Sistla, Srinivas and Wisztorski, Maxence and Meriaux, C{\'{e}}line and Becker, Michael and Salzet, Michel and Fournier, Isabelle and Macagno, Eduardo and Bafna, Vineet},
doi = {10.1021/pr2005378},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruand et al. - 2011 - AMASS algorithm for MSI analysis by semi-supervised segmentation.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: metabolism,Cluster Analysis,Computational Biology,Computational Biology: methods,Gene Expression Regulation,Gene Expression Regulation, Developmental,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Leeches,Mass Spectrometry,Mass Spectrometry: methods,Peptides,Peptides: chemistry,Rats,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = {oct},
number = {10},
pages = {4734--43},
pmid = {21800894},
title = {{AMASS: algorithm for MSI analysis by semi-supervised segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3190602{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Piironen2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
eprint = {arXiv:1503.08650v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Piironen, Vehtari - 2012 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
keywords = {bayesian model selection,cross-validation,map,median,overfitting,probability model,projection,reference model,selection bias,waic},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2012}
}
@inproceedings{Bottou2010,
author = {Bottou, Leon},
booktitle = {Proceedings of COMPSTAT'2010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2010 - Large-scale machine learning with stochastic gradient descent.pdf:pdf},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Springer},
title = {{Large-scale machine learning with stochastic gradient descent}},
year = {2010}
}
@article{Vilalta2002,
author = {Vilalta, Ricardo and Drissi, Youssef},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vilalta, Drissi - 2002 - A perspective view and survey of meta-learning.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {classification,inductive learning,meta-knowledge},
number = {1997},
pages = {77--95},
title = {{A perspective view and survey of meta-learning}},
url = {http://link.springer.com/article/10.1023/A:1019956318069},
year = {2002}
}
@article{Chakraborty2015,
archivePrefix = {arXiv},
arxivId = {1502.03491v1},
author = {Chakraborty, Mithun and Das, Sanmay and Lavoie, Allen},
eprint = {1502.03491v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chakraborty, Das, Lavoie - 2015 - How to show a probablistic model is better.pdf:pdf},
pages = {1--5},
title = {{How to show a probablistic model is better}},
year = {2015}
}
@inproceedings{Loog2012b,
author = {Loog, Marco and Jensen, Are C},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2012 - Constrained log-likelihood-based semi-supervised linear discriminant analysis.pdf:pdf},
pages = {327--335},
title = {{Constrained log-likelihood-based semi-supervised linear discriminant analysis}},
url = {http://www.springerlink.com/index/U16X1L3015777162.pdf},
year = {2012}
}
@article{Balsubramani2015,
abstract = {We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying "link functions" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning - all this structure follows from a minimax interpretation of loss minimization.},
archivePrefix = {arXiv},
arxivId = {1510.00452},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {1510.00452},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - 2015 - Minimax Binary Classifier Aggregation with General Losses.pdf:pdf},
pages = {1--13},
title = {{Minimax Binary Classifier Aggregation with General Losses}},
url = {http://arxiv.org/abs/1510.00452},
year = {2015}
}
@article{Potapov2016,
author = {Potapov, Alexey and Potapova, Vita and Peterson, Maxim},
doi = {10.1016/j.patrec.2016.05.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Potapov, Potapova, Peterson - 2016 - A feasibility study of an autoencoder meta-model for improving generalization capabilities on train.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Autoencoders,Logistic regression,Overlearning,Gene},
pages = {24--29},
publisher = {Elsevier B.V.},
title = {{A feasibility study of an autoencoder meta-model for improving generalization capabilities on training sets of small sizes}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865516300964},
volume = {80},
year = {2016}
}
@article{Sotoca2006,
author = {Sotoca, J M and Mollineda, R A and Sanchez, J.S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sotoca, Mollineda, Sanchez - 2006 - A meta-learning framework for pattern classification by means of data complexity measures.pdf:pdf},
journal = {Inteligencia artificial: Revista Iberoamericana de Inteligencia Artificial},
keywords = {classification,data complexity,feature selection,meta-learning,prototype selection},
number = {29},
pages = {31--38},
title = {{A meta-learning framework for pattern classification by means of data complexity measures}},
volume = {10},
year = {2006}
}
@incollection{Opper1996,
address = {New York},
author = {Opper, Manfred and Kinzel, Wolfgang},
booktitle = {Models of Neural Networks III},
editor = {Domany, Eytan and Hemmen, J. Leo and Schulten, Klaus},
pages = {151--209},
publisher = {Springer},
title = {{Statistical Mechanics of Generalization}},
year = {1996}
}
@article{Patil2016,
author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
doi = {10.1101/066803},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patil, Peng, Leek - 2016 - A statistical definition for reproducibility and replicability.pdf:pdf},
journal = {Biorxiv},
title = {{A statistical definition for reproducibility and replicability}},
url = {http://dx.doi.org/10.1101/066803},
year = {2016}
}
@article{Hartley1968b,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and Estimation in Analysis of Variance Problems.pdf:pdf},
journal = {Review of the International Statistical Institute},
number = {2},
pages = {141--147},
title = {{Classification and Estimation in Analysis of Variance Problems}},
volume = {36},
year = {1968}
}
@article{Gama1995,
author = {Gama, Joao and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gama, Brazdil - 1995 - Characterization of Classification Algorithms.pdf:pdf},
journal = {Progress in Artificial Intelligence},
pages = {189--200},
title = {{Characterization of Classification Algorithms}},
url = {http://link.springer.com/chapter/10.1007/3-540-60428-6{\_}16},
year = {1995}
}
@article{Peltonen2014,
author = {Peltonen, Jaakko and Lin, Ziyuan},
doi = {10.1007/s10994-014-5464-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peltonen, Lin - 2014 - Information retrieval approach to meta-visualization.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {and masashi sugiyama,b,bob williamson,cheng soon ong,editors,j,lin,meta-visualization,neighbor embedding,nonlinear dimensionality reduction,peltonen,tu bao ho,wray buntine,z},
month = {oct},
number = {January},
title = {{Information retrieval approach to meta-visualization}},
url = {http://link.springer.com/10.1007/s10994-014-5464-x},
year = {2014}
}
@article{Liu2013,
author = {Liu, J. and Gelman, Andrew and Hill, J. and Su, Y.-S. and Kropko, J.},
doi = {10.1093/biomet/ast044},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2013 - On the stationary distribution of iterative imputations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {nov},
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast044},
volume = {101},
year = {2013}
}
@article{Ye2007a,
address = {New York, New York, USA},
author = {Ye, Jieping},
doi = {10.1145/1273496.1273633},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ye - 2007 - Least squares linear discriminant analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning},
keywords = {18,3,8,are linear combinations of,class separability,derived features in lda,dimension reduction,least squares,linear discriminant anal-,linear regression,the,the data achieves maximum,the orig-,ysis},
pages = {1087--1093},
publisher = {ACM Press},
title = {{Least squares linear discriminant analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273633},
year = {2007}
}
@inproceedings{Balcan2013,
author = {Balcan, Maria-Florina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2013 - Efficient Semi-supervised and Active Learning of Disjunctions.pdf:pdf},
pages = {633--641},
title = {{Efficient Semi-supervised and Active Learning of Disjunctions}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}balcan13},
year = {2013}
}
@article{Rasmussen2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2005}
}
@article{Wang2009,
author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
doi = {10.1016/j.neucom.2008.10.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Smith-Miles, Hyndman - 2009 - Rule induction for forecasting method selection Meta-learning the characteristics of univariate time.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {jun},
number = {10-12},
pages = {2581--2594},
title = {{Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
volume = {72},
year = {2009}
}
@article{Alaoui2016,
abstract = {Given a weighted graph with {\$}N{\$} vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes {\$}n{\$} labeled vertices, and the task is to label the remaining ones. We present a theoretical study of {\$}\backslashell{\_}p{\$}-based Laplacian regularization under a {\$}d{\$}-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as {\$}N{\$} grows to infinity while {\$}n{\$} stays constant, the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate {\$}\backslashhat{\{}f{\}}{\$}. From this formulation we derive several predictions on the limiting behavior the {\$}d{\$}-dimensional function {\$}\backslashhat{\{}f{\}}{\$}, including (a) a phase transition in its smoothness at the threshold {\$}p = d + 1{\$}, and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution {\$}P{\$}. Thus, over the range {\$}p \backslashleq d{\$}, the function estimate {\$}\backslashhat{\{}f{\}}{\$} is degenerate and "spiky," whereas for {\$}p\backslashgeq d+1{\$}, the function estimate {\$}\backslashhat{\{}f{\}}{\$} is smooth. We show that the effect of the underlying density vanishes monotonically with {\$}p{\$}, such that in the limit {\$}p = \backslashinfty{\$}, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate {\$}\backslashhat{\{}f{\}}{\$} is independent of the distribution {\$}P{\$}. Under the assumption of semi-supervised smoothness, ignoring {\$}P{\$} can lead to poor statistical performance, in particular, we construct a specific example for {\$}d=1{\$} to demonstrate that {\$}p=2{\$} has lower risk than {\$}p=\backslashinfty{\$} due to the former penalty adapting to {\$}P{\$} and the latter ignoring it. We also provide simulations that verify the accuracy of our predictions for finite sample sizes. Together, these properties show that {\$}p = d+1{\$} is an optimal choice, yielding a function estimate {\$}\backslashhat{\{}f{\}}{\$} that is both smooth and non-degenerate, while remaining maximally sensitive to {\$}P{\$}.},
archivePrefix = {arXiv},
arxivId = {1603.00564},
author = {Alaoui, Ahmed El and Cheng, Xiang and Ramdas, Aaditya and Wainwright, Martin J. and Jordan, Michael I.},
eprint = {1603.00564},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Alaoui et al. - 2016 - Asymptotic behavior of {\$}ell{\_}p{\$}-based Laplacian regularization in semi-supervised learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {absolutely minimal lipschitz extension,asymptotic behav-,geometric random graph model,ior,phase transition,regularization,semi-supervised learning,â„“ p -based laplacian},
title = {{Asymptotic behavior of {\$}\backslashell{\_}p{\$}-based Laplacian regularization in semi-supervised learning}},
url = {http://arxiv.org/abs/1603.00564},
year = {2016}
}
@phdthesis{Macia2011,
author = {Macia, Nuria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia - 2011 - Data Complexity in Supervised Learning A Far-Reaching Implication.pdf:pdf},
title = {{Data Complexity in Supervised Learning: A Far-Reaching Implication}},
year = {2011}
}
@article{Sion1958,
author = {Sion, Maurice},
doi = {1103040253},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sion - 1958 - On general minimax theorems.pdf:pdf},
issn = {0030-8730},
journal = {Pacific J. Math},
number = {1},
pages = {171--176},
title = {{On general minimax theorems}},
volume = {8},
year = {1958}
}
@article{Spirtes2010,
author = {Spirtes, Peter},
doi = {10.2202/1557-4679.1203},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spirtes - 2010 - Introduction to causal inference.pdf:pdf},
issn = {1557-4679},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {1643--1662},
pmid = {20305706},
title = {{Introduction to causal inference}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836213{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Goodman2016,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
eprint = {1606.08813},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
keywords = {machine learning},
pages = {1--6},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
url = {http://arxiv.org/abs/1606.08813},
year = {2016}
}
@inproceedings{Niu2012,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2012 - Information-theoretic Semi-supervised Metric Learning via Entropy Regularization.pdf:pdf},
number = {c},
title = {{Information-theoretic Semi-supervised Metric Learning via Entropy Regularization}},
url = {http://arxiv.org/abs/1206.4614},
year = {2012}
}
@article{Loogc,
author = {Loog, M. and van Ginneken, B.},
doi = {10.1109/ICPR.2002.1048456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, van Ginneken - Unknown - Supervised segmentation by iterated contextual pixel classification.pdf:pdf},
isbn = {0-7695-1695-X},
journal = {Object recognition supported by user interaction for service robots},
pages = {925--928},
publisher = {IEEE Comput. Soc},
title = {{Supervised segmentation by iterated contextual pixel classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1048456},
volume = {2}
}
@article{Chwialkowski2016,
abstract = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
archivePrefix = {arXiv},
arxivId = {1602.02964},
author = {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
eprint = {1602.02964},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chwialkowski, Strathmann, Gretton - 2016 - A Kernel Test of Goodness of Fit.pdf:pdf},
title = {{A Kernel Test of Goodness of Fit}},
url = {http://arxiv.org/abs/1602.02964},
year = {2016}
}
@inproceedings{White2012,
author = {White, Martha and Schuurmans, Dale},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White, Schuurmans - 2012 - Generalized optimal reverse prediction.pdf:pdf},
pages = {1305--1313},
title = {{Generalized optimal reverse prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}WhiteS12.pdf},
year = {2012}
}
@article{Niyogi2013,
author = {Niyogi, Partha},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niyogi - 2013 - Manifold Regularization and Semi-supervised Learning Some Theoretical Analyses.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph laplacian,manifold regularization,minimax rates,semi-supervised learning},
pages = {1229--1250},
title = {{Manifold Regularization and Semi-supervised Learning : Some Theoretical Analyses}},
volume = {14},
year = {2013}
}
@article{Li2015b,
abstract = {The scarcity of data annotated at the desired level of granularity is a recurring issue in many applications. Significant amounts of effort have been devoted to developing weakly supervised methods tailored to each individual setting, which are often carefully designed to take advantage of the particular properties of weak supervision regimes, form of available data and prior knowledge of the task at hand. Unfortunately, it is difficult to adapt these methods to new tasks and/or forms of data, which often require different weak supervision regimes or models. We present a general-purpose method that can solve any weakly supervised learning problem irrespective of the weak supervision regime or the model. The proposed method turns any off-the-shelf strongly supervised classifier into a weakly supervised classifier and allows the user to specify any arbitrary weakly supervision regime via a loss function. We apply the method to several different weak supervision regimes and demonstrate competitive results compared to methods specifically engineered for those settings.},
archivePrefix = {arXiv},
arxivId = {1509.06807},
author = {Li, Ke and Malik, Jitendra},
eprint = {1509.06807},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Malik - 2015 - Bandit Label Inference for Weakly Supervised Learning.pdf:pdf},
pages = {1--10},
title = {{Bandit Label Inference for Weakly Supervised Learning}},
url = {http://arxiv.org/abs/1509.06807},
year = {2015}
}
@inproceedings{Rohrbach2013,
author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rohrbach, Ebert, Schiele - 2013 - Transfer Learning in a Transductive Setting.pdf:pdf},
pages = {46--54},
title = {{Transfer Learning in a Transductive Setting}},
url = {http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting},
year = {2013}
}
@article{Kleinberg1996,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 1996 - An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition.pdf:pdf},
number = {6},
pages = {2319--2349},
title = {{An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition}},
volume = {24},
year = {1996}
}
@inproceedings{Zhou2005a,
author = {Zhou, Zhi-hua and Li, Ming},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Semi-Supervised Regression with Co-Training.pdf:pdf},
title = {{Semi-Supervised Regression with Co-Training.}},
url = {http://ijcai.org/Past Proceedings/IJCAI-05/PDF/0689.pdf},
year = {2005}
}
@article{Krijthe2016,
author = {Krijthe, Jesse Hendrik and Loog, Marco},
journal = {arXiv preprint arXiv:1602.07865},
title = {{Projected Estimators for Robust Semi-supervised Classification}},
year = {2016}
}
@unpublished{Scholkopf2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.06794v1},
author = {Sch{\"{o}}lkopf, Bernhard and Muandet, Krikamol and Fukumizu, Kenji and Peters, Jonas},
eprint = {arXiv:1501.06794v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf et al. - 2015 - Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations.pdf:pdf},
pages = {1--20},
title = {{Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations}},
year = {2015}
}
@article{Efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron, Morris - 1977 - Stein's paradox in statistics.pdf:pdf},
journal = {Scientific American},
pages = {119--127},
title = {{Stein's paradox in statistics}},
url = {https://www.cs.nyu.edu/{~}roweis/csc2515-2006/readings/stein{\_}sciam.pdf},
year = {1977}
}
@inproceedings{Goldman2000,
author = {Goldman, Sally and Zhou, Yan},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldman, Zhou - 2000 - Enhancing supervised learning with unlabeled data.pdf:pdf},
pages = {327--334},
title = {{Enhancing supervised learning with unlabeled data}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Enhancing+Supervised+Learning+with+Unlabeled+Data{\#}0},
volume = {3},
year = {2000}
}
@article{Hanselmann2008,
abstract = {Imaging mass spectrometry (IMS) is a promising technology which allows for detailed analysis of spatial distributions of (bio)molecules in organic samples. In many current applications, IMS relies heavily on (semi)automated exploratory data analysis procedures to decompose the data into characteristic component spectra and corresponding abundance maps, visualizing spectral and spatial structure. The most commonly used techniques are principal component analysis (PCA) and independent component analysis (ICA). Both methods operate in an unsupervised manner. However, their decomposition estimates usually feature negative counts and are not amenable to direct physical interpretation. We propose probabilistic latent semantic analysis (pLSA) for non-negative decomposition and the elucidation of interpretable component spectra and abundance maps. We compare this algorithm to PCA, ICA, and non-negative PARAFAC (parallel factors analysis) and show on simulated and real-world data that pLSA and non-negative PARAFAC are superior to PCA or ICA in terms of complementarity of the resulting components and reconstruction accuracy. We further combine pLSA decomposition with a statistical complexity estimation scheme based on the Akaike information criterion (AIC) to automatically estimate the number of components present in a tissue sample data set and show that this results in sensible complexity estimates.},
author = {Hanselmann, Michael and Kirchner, Marc and Renard, Bernhard Y and Amstalden, Erika R and Glunde, Kristine and Heeren, Ron M a and Hamprecht, Fred a},
doi = {10.1021/ac801303x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2008 - Concise representation of mass spectrometry images by probabilistic latent semantic analysis.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: pathology,Computer Simulation,Female,Humans,Image Processing, Computer-Assisted,Mass Spectrometry,Principal Component Analysis,Signal Processing, Computer-Assisted},
month = {dec},
number = {24},
pages = {9649--58},
pmid = {18989936},
title = {{Concise representation of mass spectrometry images by probabilistic latent semantic analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18989936},
volume = {80},
year = {2008}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
eprint = {1606.05908},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--22},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}
@inproceedings{Blum1998,
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279962},
year = {1998}
}
@article{Adams2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01664v1},
author = {Adams, Niall M},
eprint = {arXiv:1502.01664v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - 2014 - Estimating Optimal Active Learning via Model Retraining.pdf:pdf},
keywords = {active learning,classification,estimation framework,ex-,model retraining improvement,pected loss reduction},
pages = {1--36},
title = {{Estimating Optimal Active Learning via Model Retraining}},
volume = {15},
year = {2014}
}
@article{Loog2014a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2014 - Semi-supervised linear discriminant analysis through moment-constraint parameter estimation.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {linear discriminant analysis,semi-supervised learning},
pages = {24--31},
publisher = {Elsevier B.V.},
title = {{Semi-supervised linear discriminant analysis through moment-constraint parameter estimation}},
volume = {37},
year = {2014}
}
@phdthesis{Hillebrand2012,
author = {Hillebrand, Arne},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hillebrand - 2012 - Separating a polygonal environment into a multi-layered environment.pdf:pdf},
keywords = {branch,explicit corridor map,ge-,graphs,local search,multi-layered environment,multicut,netic algorithm,price},
school = {Utrecht University},
title = {{Separating a polygonal environment into a multi-layered environment}},
year = {2012}
}
@inproceedings{Szummer2002,
author = {Szummer, Martin and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2002 - Information regularization with partially labeled data.pdf:pdf},
pages = {1025--1032},
title = {{Information regularization with partially labeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA69.pdf},
year = {2002}
}
@article{Wickenberg-Bolin2006,
abstract = {Supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. In practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. A suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. However, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. Therefore different methods for small sample performance estimation such as a recently proposed procedure called Repeated Random Sampling (RSS) is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. Here we explore such biases and develop a refined algorithm called Repeated Independent Design and Test (RIDT).},
author = {Wickenberg-Bolin, Ulrika and G{\"{o}}ransson, Hanna and Frykn{\"{a}}s, M{\aa}rten and Gustafsson, Mats G and Isaksson, Anders},
doi = {10.1186/1471-2105-7-127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wickenberg-Bolin et al. - 2006 - Improved variance estimation of classification performance via reduction of bias caused by small sample.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Analysis of Variance,Artificial Intelligence,Bias (Epidemiology),Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis},
month = {jan},
pages = {127},
pmid = {16533392},
title = {{Improved variance estimation of classification performance via reduction of bias caused by small sample size.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1435937{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Buhlmann2014a,
author = {Buhlmann, Peter and Meier, Lukas and van de Geer, Sara},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Meier, van de Geer - 2014 - Discussion â€œa significance test for the lassoâ€.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {469--477},
title = {{Discussion: â€œa significance test for the lassoâ€}},
volume = {42},
year = {2014}
}
@inproceedings{Tax2005,
author = {Tax, David M.J. and Duin, Robert P.W.},
booktitle = {Proceedings of the Sixteenth Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tax, Duin - 2005 - Characterizing one-class datasets.pdf:pdf},
number = {4},
pages = {21--26},
title = {{Characterizing one-class datasets}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/TaxDui2005.pdf},
volume = {1},
year = {2005}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Ranzato, Marc Aurelio and Aviv, Tel and Park, Menlo},
booktitle = {Computer Vision and Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
title = {{DeepFace : Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@article{Goldberg2009,
author = {Goldberg, Andrew B. and Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu - 2009 - Keepin'it real semi-supervised learning with realistic tuning.pdf:pdf},
journal = {NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing},
title = {{Keepin'it real: semi-supervised learning with realistic tuning}},
year = {2009}
}
@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M.},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neal - 2011 - Handbook of Markov Chain Monte Carlo.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{Shore1980,
author = {Shore, John E. and Johnson, Rodney W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shore, Johnson - 1980 - Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {26--37},
title = {{Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1056144},
volume = {26},
year = {1980}
}
@article{Li2015,
author = {Li, Yu-Feng and Zhou, Zhi-Hua},
doi = {10.1109/TPAMI.2014.2299812},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2015 - Towards Making Unlabeled Data Never Hurt.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {175--188},
title = {{Towards Making Unlabeled Data Never Hurt}},
volume = {37},
year = {2015}
}
@article{Soares2004,
author = {Soares, Carlos and Brazdil, Pavel B. and Kuba, Petr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soares, Brazdil, Kuba - 2004 - A Meta-Learning Method to Select the KernelWidth in Support Vector Regression.pdf:pdf},
journal = {Machine learning},
keywords = {gaussian kernel,learning rankings,meta-learning,parameter setting,support vector machines},
pages = {195--209},
title = {{A Meta-Learning Method to Select the KernelWidth in Support Vector Regression}},
url = {http://link.springer.com/article/10.1023/b:mach.0000015879.28004.9b},
volume = {54},
year = {2004}
}
@inproceedings{Hoi2006,
abstract = {The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be ineï¬ƒcient since the classiï¬cation model has to be retrained for every labeled example. In this paper, we present a framework for â€œbatch mode active learningâ€ that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to eï¬ƒciently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an eï¬ƒcient greedy algorithm that is based on the property of submodular functions. Our empirical studies with ï¬ve UCI datasets and one realworld medical image classiï¬cation show that the proposed batch mode active learning algorithm is more eï¬€ective than the state-ofthe-art algorithms for active learning},
annote = {Badly written},
author = {Hoi, Steven C H and Jin, Rong and Zhu, Jianke and Lyu, Michael R},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
doi = {10.1145/1143844.1143897},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoi et al. - 2006 - Batch mode active learning and its application to medical image classification.pdf:pdf},
isbn = {1595933832},
pages = {417--424},
title = {{Batch mode active learning and its application to medical image classification}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143897},
year = {2006}
}
@article{Gratiet2014,
author = {Gratiet, Loic and Garnier, Josselin},
doi = {10.1007/s10994-014-5437-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gratiet, Garnier - 2014 - Asymptotic analysis of the learning curve for Gaussian process regression.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {asymptotic mean squared error,convergence rate,gaussian process regression,generalization error,learning curves},
month = {mar},
title = {{Asymptotic analysis of the learning curve for Gaussian process regression}},
url = {http://link.springer.com/10.1007/s10994-014-5437-0},
year = {2014}
}
@article{Minka2005,
abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
author = {Minka, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 2005 - Divergence measures and message passing.pdf:pdf},
pages = {MSR--TR--2005--173},
title = {{Divergence measures and message passing}},
year = {2005}
}
@article{Marchand2004,
author = {Marchand, E and Strawderman, WE},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Strawderman - 2004 - Estimation in Restricted Parameter Spaces A Review.pdf:pdf},
journal = {Institute of Mathematical Statistics Lecture Notes-Monograph Series},
number = {2004},
pages = {21--44},
title = {{Estimation in Restricted Parameter Spaces: A Review}},
url = {http://www.jstor.org/stable/10.2307/4356296},
volume = {45},
year = {2004}
}
@article{Ho2002a,
author = {Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2002 - A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors.pdf:pdf},
journal = {Pattern Analysis and Applications},
number = {2},
pages = {102--112},
title = {{A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors}},
volume = {5},
year = {2002}
}
@article{Wangb,
abstract = {â€”While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.},
archivePrefix = {arXiv},
arxivId = {1604.01662},
author = {Wang, Hao and Yeung, Dit-Yan},
eprint = {1604.01662},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Yeung - Unknown - Towards Bayesian Deep Learning A Survey.pdf:pdf},
keywords = {Artificial Intelligence !,Data Mining,Deep Learning,Index Termsâ€”Bayesian Networks,Machine Learning,Neural Networks},
pages = {1--17},
title = {{Towards Bayesian Deep Learning: A Survey}}
}
@article{Hilario2006,
abstract = {Among the many applications of mass spectrometry, biomarker pattern discovery from protein mass spectra has aroused considerable interest in the past few years. While research efforts have raised hopes of early and less invasive diagnosis, they have also brought to light the many issues to be tackled before mass-spectra-based proteomic patterns become routine clinical tools. Known issues cover the entire pipeline leading from sample collection through mass spectrometry analytics to biomarker pattern extraction, validation, and interpretation. This study focuses on the data-analytical phase, which takes as input mass spectra of biological specimens and discovers patterns of peak masses and intensities that discriminate between different pathological states. We survey current work and investigate computational issues concerning the different stages of the knowledge discovery process: exploratory analysis, quality control, and diverse transforms of mass spectra, followed by further dimensionality reduction, classification, and model evaluation. We conclude after a brief discussion of the critical biomedical task of analyzing discovered discriminatory patterns to identify their component proteins as well as interpret and validate their biological implications.},
author = {Hilario, Melanie and Kalousis, Alexandros and Pellegrini, Christian and M{\"{u}}ller, Markus},
doi = {10.1002/mas.20072},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hilario et al. - 2006 - Processing and classification of protein mass spectra.pdf:pdf},
issn = {0277-7037},
journal = {Mass spectrometry reviews},
keywords = {Algorithms,Animals,Biological Markers,Computational Biology,Humans,Mass Spectrometry,Mass Spectrometry: classification,Mass Spectrometry: methods,Models, Chemical,Peptide Mapping,Proteins,Proteins: analysis,Proteomics},
number = {3},
pages = {409--49},
pmid = {16463283},
title = {{Processing and classification of protein mass spectra.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16463283},
volume = {25},
year = {2006}
}
@inproceedings{Ben-David2011,
author = {Ben-David, Shai and Srebro, Nathan and Urner, R},
booktitle = {Philosophy and Machine Learning - Workshop at NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Srebro, Urner - 2011 - Universal learning vs. no free lunch results.pdf:pdf},
title = {{Universal learning vs. no free lunch results}},
url = {http://www.dsi.unive.it/PhiMaLe2011/Abstract/Ben-David{\_}Srebro{\_}Urner.pdf},
year = {2011}
}
@inproceedings{Zhang2000a,
author = {Zhang, Tong and Oles, Frank J.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Oles - 2000 - A Probability Analysis on the Value of Unlabeled Data for Classification Problems.pdf:pdf},
pages = {1191--1198},
title = {{A Probability Analysis on the Value of Unlabeled Data for Classification Problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Joachims2003,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 2003 - Transductive learning via spectral graph partitioning.pdf:pdf},
pages = {290--297},
title = {{Transductive learning via spectral graph partitioning}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-040.pdf},
year = {2003}
}
@article{Rojas-Carulla2015,
abstract = {From training data from several related domains (or tasks), methods of domain adaptation try to combine knowledge to improve performance. This paper discusses an approach to domain adaptation which is inspired by a causal interpretation of the multi-task problem. We assume that a covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant with respect to shifts in those predictors (covariates). We propose to learn the corresponding conditional expectation in the training domains and use it for estimation in the target domain. We further introduce a method which allows for automatic inference of the above subset in regression and classification. We study the performance of this approach in an adversarial setting, in the case where no additional examples are available in the test domain. If a labeled sample is available, we provide a method for using both the transferred invariant conditional and task specific information. We present results on synthetic data sets and a sentiment analysis problem.},
archivePrefix = {arXiv},
arxivId = {1507.05333},
author = {Rojas-Carulla, Mateo and Sch{\"{o}}lkopf, Bernhard and Turner, Richard and Peters, Jonas},
eprint = {1507.05333},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rojas-Carulla et al. - 2015 - A Causal Perspective on Domain Adaptation.pdf:pdf},
pages = {1--14},
title = {{A Causal Perspective on Domain Adaptation}},
url = {http://arxiv.org/abs/1507.05333},
year = {2015}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
url = {http://home.olemiss.edu/{~}xdang/676/Greedy{\_}function{\_}approximation{\_}a{\_}gradient{\_}bossting{\_}machine.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Greedy+Function+Approximation:+A+Gradient+Boosting+Machine{\#}3},
volume = {29},
year = {2001}
}
@article{Masnadi-shirazi2015,
author = {Masnadi-shirazi, Hamed},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Masnadi-shirazi - 2015 - A View of Margin Losses as Regularizers of Probability Estimates.pdf:pdf},
keywords = {boosting,classification,margin losses,probability elicitation,regularization},
pages = {2751--2795},
title = {{A View of Margin Losses as Regularizers of Probability Estimates}},
volume = {16},
year = {2015}
}
@article{Duchi2016,
abstract = {We provide a unifying view of statistical information measures, multi-class classification problems, multi-way Bayesian hypothesis testing, and loss functions, elaborating equivalence results between all of these objects. In particular, we consider a particular generalization of {\$}f{\$}-divergences to multiple distributions, and we show that there is a constructive equivalence between {\$}f{\$}-divergences, statistical information (in the sense of uncertainty as elaborated by DeGroot), and loss functions for multi-category classification. We also study an extension of our results to multi-class classification problems in which we must both infer a discriminant function {\$}\backslashgamma{\$} and a data representation (or, in the setting of a hypothesis testing problem, an experimental design), represented by a quantizer {\$}\backslashmathsf{\{}q{\}}{\$} from a family of possible quantizers {\$}\backslashmathsf{\{}Q{\}}{\$}. There, we give a complete characterization of the equivalence between loss functions, meaning that optimizing either of two losses yields the same optimal discriminant and quantizer {\$}\backslashmathsf{\{}q{\}}{\$}. A main consequence of our results is to describe those convex loss functions that are Fisher consistent for jointly choosing a data representation and minimizing the (weighted) probability of error in multi-category classification and hypothesis testing problems.},
archivePrefix = {arXiv},
arxivId = {1603.00126},
author = {Duchi, John C. and Khosravi, Khashayar and Ruan, Feng},
eprint = {1603.00126},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duchi, Khosravi, Ruan - 2016 - Information Measures, Experiments, Multi-category Hypothesis Tests, and Surrogate Losses.pdf:pdf},
title = {{Information Measures, Experiments, Multi-category Hypothesis Tests, and Surrogate Losses}},
url = {http://arxiv.org/abs/1603.00126},
year = {2016}
}
@article{Schaffer1993,
author = {Schaffer, Cullen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1993 - Selecting a classification method by cross-validation.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,cross-validation,decision trees,neural networks},
pages = {135--143},
title = {{Selecting a classification method by cross-validation}},
url = {http://link.springer.com/article/10.1007/BF00993106},
volume = {13},
year = {1993}
}
@article{Hollenbach2014,
abstract = {Gold-standard approaches to missing data imputation are complicated and computationally expensive. We present a principled solution to this situation, using copula distributions from which missing data may be quickly drawn. We compare this approach to other imputation techniques and show that it performs at least as well as less computationally efficient approaches. Our results demonstrate that most applied researchers can achieve great speed improvements implementing a copula-based imputation approach, while still maintaining the performance of other approaches to multiple imputation. Moreover, this approach can be easily implemented at the point of need in Bayesian analyses.},
archivePrefix = {arXiv},
arxivId = {1411.0647},
author = {Hollenbach, Florian M. and Metternich, Nils W. and Minhas, Shahryar and Ward, Michael D.},
eprint = {1411.0647},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hollenbach et al. - 2014 - Fast {\&} Easy Imputation of Missing Social Science Data.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Fast {\&} Easy Imputation of Missing Social Science Data}},
url = {http://arxiv.org/abs/1411.0647},
year = {2014}
}
@incollection{Cozman2006,
author = {Cozman, F and Cohen, Ira},
booktitle = {Semi-Supervised Learning},
chapter = {4},
editor = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, A},
pages = {56--72},
publisher = {MIT press},
title = {{Risks of Semi-Supervised Learning}},
year = {2006}
}
@article{Gelman2006,
abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant. It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant.},
author = {Gelman, Andrew and Stern, Hal},
doi = {10.1198/000313006X152649},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Stern - 2006 - The Difference Between â€œSignificantâ€ and â€œNot Significantâ€ is not Itself Statistically Significant.pdf:pdf},
isbn = {0602440371100},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {comparison,hypothesis testing,meta-analysis,pairwise,replication},
number = {4},
pages = {328--331},
title = {{The Difference Between â€œSignificantâ€ and â€œNot Significantâ€ is not Itself Statistically Significant}},
volume = {60},
year = {2006}
}
@article{Wilson2015,
abstract = {Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive, for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.},
archivePrefix = {arXiv},
arxivId = {1510.07389},
author = {Wilson, Andrew Gordon and Dann, Christoph and Lucas, Christopher G. and Xing, Eric P.},
eprint = {1510.07389},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson et al. - 2015 - The Human Kernel.pdf:pdf},
pages = {11},
title = {{The Human Kernel}},
url = {http://arxiv.org/abs/1510.07389},
year = {2015}
}
@article{Patel2015,
abstract = {A grand challenge in machine learning is the development of computational al-gorithms that match or outperform humans in perceptual inference tasks such as visual object and speech recognition. The key factor complicating such tasks is the presence of numerous nuisance variables, for instance, the unknown object position, orientation, and scale in object recognition or the unknown voice pronunciation, pitch, and speed in speech recognition. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks; they are constructed from many layers of alternating linear and nonlin-ear processing units and are trained using large-scale algorithms and massive amounts of training data. The recent success of deep learning systems is im-pressive â€” they now routinely yield pattern recognition systems with near-or super-human capabilities â€” but a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on a Bayesian generative probabilistic model that explicitly cap-tures variation due to nuisance variables. The graphical structure of the model enables it to be learned from data using classical expectation-maximization techniques. Furthermore, by relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks (DCNs) and random decision forests (RDFs), providing insights into their successes and shortcomings as well as a princi-pled route to their improvement.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00641v1},
author = {Patel, Ankit B and Nguyen, Tan and Baraniuk, Richard G},
eprint = {arXiv:1504.00641v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patel, Nguyen, Baraniuk - 2015 - A Probabilistic Theory of Deep Learning.pdf:pdf},
pages = {1--56},
title = {{A Probabilistic Theory of Deep Learning}},
year = {2015}
}
@article{Chatterjee2007,
author = {Chatterjee, Sangit and Firat, Aykut},
doi = {10.1198/000313007X220057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Firat - 2007 - Generating Data with Identical Statistics but Dissimilar Graphics.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American Statistician},
month = {aug},
number = {3},
pages = {248--254},
title = {{Generating Data with Identical Statistics but Dissimilar Graphics}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X220057},
volume = {61},
year = {2007}
}
@article{Roweis1999,
abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
author = {Roweis, Sam and Ghahramani, Zoubin},
doi = {10.1162/089976699300016674},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roweis, Ghahramani - 1999 - A unifying review of linear gaussian models.pdf:pdf},
isbn = {0899766993000},
issn = {0899-7667},
journal = {Neural computation},
number = {1995},
pages = {305--345},
pmid = {9950734},
title = {{A unifying review of linear gaussian models.}},
volume = {11},
year = {1999}
}
@article{Wang2015b,
abstract = {How should one statistically analyze privacy-enhanced data? In theory, one could process it exactly as if it were normal data since many differentially private algorithms asymptotically converge exponentially fast to their non-private counterparts and/or have error that asymptotically decreases as fast as sampling error. In practice, convergence often requires enormous amounts of data. Thus making differential privacy practical requires the development of techniques that specifically account for the noise that is added for the sake of providing privacy guarantees. Such techniques are especially needed for statistical hypothesis testing. Previous approaches either ignored the added noise (resulting in highly biased {\$}p{\$}-values), accounted for the noise but had high variance, or accounted for the noise while having small variance but were restricted to very specific types of data sets. In this paper, we propose statistical tests of independence that address all three problems simultaneously -- they add small amounts of noise, account for this noise to produce accurate {\$}p{\$}-values, and have no restrictions on the types of tables to which they are applicable. Along with these tests, we propose an alternative methodology for computing the asymptotic distributions of test statistics that results in better finite-sample approximations when using differential privacy to protect data.},
archivePrefix = {arXiv},
arxivId = {1511.03376},
author = {Wang, Yue and Lee, Jaewoo and Kifer, Daniel},
eprint = {1511.03376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Lee, Kifer - 2015 - Differentially Private Hypothesis Testing, Revisited.pdf:pdf},
title = {{Differentially Private Hypothesis Testing, Revisited}},
url = {http://arxiv.org/abs/1511.03376},
year = {2015}
}
@book{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bertsekas - 1982 - Constrained optimization and Lagrange multiplier methods.pdf:pdf},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
url = {http://adsabs.harvard.edu/abs/1982colm.book.....b},
year = {1982}
}
@article{Mealli2015,
author = {Mealli, Fabrizia and Rubin, Donald B.},
doi = {10.1093/biomet/asv035},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mealli, Rubin - 2015 - Clarifying missing at random and related definitions, and implications when coupled with exchangeability Table 1.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {September},
pages = {asv035},
title = {{Clarifying missing at random and related definitions, and implications when coupled with exchangeability: Table 1.}},
url = {http://biomet.oxfordjournals.org/lookup/doi/10.1093/biomet/asv035},
year = {2015}
}
@article{Moeller2016,
abstract = {Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to learn not only a classifier/regressor but also the best kernel for the training task, usually from a combination of existing kernel functions. Most MKL methods seek the combined kernel that performs best over every training example, sacrificing performance in some areas to seek a global optimum. Localized kernel learning (LKL) overcomes this limitation by allowing the training algorithm to match a component kernel to the examples that can exploit it best. Several approaches to the localized kernel learning problem have been explored in the last several years. We unify many of these approaches under one simple system and design a new algorithm with improved performance. We also develop enhanced versions of existing algorithms, with an eye on scalability and performance.},
archivePrefix = {arXiv},
arxivId = {1603.01374},
author = {Moeller, John and Swaminathan, Sarathkrishna and Venkatasubramanian, Suresh},
eprint = {1603.01374},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Moeller, Swaminathan, Venkatasubramanian - 2016 - A Unified View of Localized Kernel Learning.pdf:pdf},
pages = {1--14},
title = {{A Unified View of Localized Kernel Learning}},
url = {http://arxiv.org/abs/1603.01374},
year = {2016}
}
@article{Abadi2015,
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
year = {2015}
}
@article{Niu2016,
abstract = {In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds of estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.},
archivePrefix = {arXiv},
arxivId = {1603.03130},
author = {Niu, Gang and du Plessis, Marthinus Christoffel and Sakai, Tomoya and Ma, Yao and Sugiyama, Masashi},
eprint = {1603.03130},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2016 - Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning.pdf:pdf},
keywords = {()},
number = {Icml},
title = {{Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning}},
url = {http://arxiv.org/abs/1603.03130},
year = {2016}
}
@inproceedings{Chaubey2003,
author = {Chaubey, Yogendra P. and Nebebe, Fassil and Sen, Debaraj},
booktitle = {Joint Statistical Meetings},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaubey, Nebebe, Sen - 2003 - Estimation of Joint Distribution from Marginal Distributions.pdf:pdf},
keywords = {bayesian prediction,because they require multidimensional,ble,contingency ta- methods,dirichlet prior,however,is preferred as it,merical integration,nu-,of the,readily presents an estimate,the bayesian method},
pages = {883--889},
title = {{Estimation of Joint Distribution from Marginal Distributions}},
url = {http://www.amstat.org/sections/SRMS/Proceedings/y2003/Files/JSM2003-000794.pdf},
year = {2003}
}
@book{Kuncheva2004,
author = {Kuncheva, Ludmila I},
booktitle = {Methods and Algorithms. Wiley, Chichester},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2004 - Combining Pattern Classifers.pdf:pdf},
isbn = {9786468600},
title = {{Combining Pattern Classifers}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2005.s320 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Combining+Pattern+Classifiers{\#}3},
year = {2004}
}
@article{Gonzalez-Covarrubias2013,
abstract = {Middle-aged offspring of nonagenarians, as compared to their spouses (controls), show a favorable lipid metabolism marked by larger LDL particle size in men and lower total triglyceride levels in women. To investigate which specific lipids associate with familial longevity, we explore the plasma lipidome by measuring 128 lipid species using liquid chromatography coupled to mass spectrometry in 1526 offspring of nonagenarians (59 years Â± 6.6) and 675 (59 years Â± 7.4) controls from the Leiden Longevity Study. In men, no significant differences were observed between offspring and controls. In women, however, 19 lipid species associated with familial longevity. Female offspring showed higher levels of ether phosphocholine (PC) and sphingomyelin (SM) species (3.5-8.7{\%}) and lower levels of phosphoethanolamine PE (38:6) and long-chain triglycerides (TG) (9.4-12.4{\%}). The association with familial longevity of two ether PC and four SM species was independent of total triglyceride levels. In addition, the longevity-associated lipid profile was characterized by a higher ratio of monounsaturated (MUFA) over polyunsaturated (PUFA) lipid species, suggesting that female offspring have a plasma lipidome less prone to oxidative stress. Ether PC and SM species were identified as novel longevity markers in females, independent of total triglycerides levels. Several longevity-associated lipids correlated with a lower risk of hypertension and diabetes in the Leiden Longevity Study cohort. This sex-specific lipid signature marks familial longevity and may suggest a plasma lipidome with a better antioxidant capacity, lower lipid peroxidation and inflammatory precursors, and an efficient beta-oxidation function.},
author = {Gonzalez-Covarrubias, Vanessa and Beekman, Marian and Uh, Hae Won and Dane, Adrie and Troost, Jorne and Paliukhovich, Iryna and van der Kloet, Frans M. and Houwing-Duistermaat, Jeanine and Vreeken, Rob J. and Hankemeier, Thomas and Slagboom, Eline P.},
doi = {10.1111/acel.12064},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gonzalez-Covarrubias et al. - 2013 - Lipidomics of familial longevity.pdf:pdf},
isbn = {1474-9726 (Electronic)$\backslash$r1474-9718 (Linking)},
issn = {14749718},
journal = {Aging Cell},
keywords = {Aging,Gender differences,Human,Longevity,Mass spectrometry,Oxidative stress},
number = {3},
pages = {426--434},
pmid = {23451766},
title = {{Lipidomics of familial longevity}},
volume = {12},
year = {2013}
}
@article{Smaldino2016,
archivePrefix = {arXiv},
arxivId = {1605.09511},
author = {Smaldino, Paul E and Mcelreath, Richard},
eprint = {1605.09511},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smaldino, Mcelreath - 2016 - The natural selection of bad science 2.pdf:pdf},
keywords = {campbell,cultural evolution,incentives,metascience,replication,s,statistical power},
pages = {1--20},
title = {{The natural selection of bad science 2}},
year = {2016}
}
@article{Xiao2015,
author = {Xiao, Min and Guo, Yuhong},
doi = {10.1109/TPAMI.2014.2343216},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiao, Guo - 2015 - Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {54--66},
title = {{Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6866177},
volume = {37},
year = {2015}
}
@article{Gu2012a,
author = {Gu, Quanquan and Zhang, Tong and Ding, Chris and Han, Jiawei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gu et al. - 2012 - Selective Labeling via Error Bound Minimization.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 25},
pages = {332--340},
title = {{Selective Labeling via Error Bound Minimization}},
url = {http://books.nips.cc/papers/files/nips25/NIPS2012{\_}0180.pdf},
year = {2012}
}
@inproceedings{Hoekstra1996,
author = {Hoekstra, Aarnoud and Duin, Robert P.W.},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoekstra, Duin - 1996 - On the nonlinearity of pattern classifiers.pdf:pdf},
pages = {271--275},
title = {{On the nonlinearity of pattern classifiers}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547429},
year = {1996}
}
@article{Huggins2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04984v1},
author = {Huggins, Jonathan H and Tenenbaum, Joshua B},
eprint = {arXiv:1505.04984v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huggins, Tenenbaum - 2015 - Risk and Regret of Hierarchical Bayesian Learners.pdf:pdf},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
title = {{Risk and Regret of Hierarchical Bayesian Learners}},
volume = {37},
year = {2015}
}
@article{Bouckaert2004,
abstract = {Empirical research in learning algorithms for classification tasks gen- erally requires the use of significance tests.The quality of a test is typically judged on Type I error (how often the test indicates a difference when it should not) and Type II error (how often it indicates no difference when it should). In this paper we argue that the replicability of a test is also of importance.We say that a test has low replicability if its outcome strongly depends on the particular random parti- tioning of the data that is used to perform it. We present empirical measures of replicability and use them to compare the performance of several popular tests in a realistic setting involving standard learning algorithms and benchmark datasets. Based on our results we give recommendations on which test to use.},
author = {Bouckaert, Remco R and Frank, Eibe},
doi = {10.1007/978-3-540-24775-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bouckaert, Frank - 2004 - Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms.pdf:pdf},
isbn = {3-540-22064-X},
issn = {0302-9743},
journal = {Advances in knowledge discovery and data mining},
pages = {3--12},
title = {{Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms}},
year = {2004}
}
@inproceedings{Sokolovska2011,
address = {Greece},
author = {Sokolovska, Nataliya},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska - 2011 - Aspects of semi-supervised and active learning in conditional random fields.pdf:pdf},
keywords = {active learn-,conditional random fields,ing,probability of observations,semi-supervised learning},
title = {{Aspects of semi-supervised and active learning in conditional random fields}},
url = {http://www.springerlink.com/index/3308764R6251J70P.pdf},
year = {2011}
}
@inproceedings{Zhou2007,
author = {Zhou, Zhi-hua and Zhan, De-Chuan and Yang, Qiang},
booktitle = {Proceedings of the 22nd national conference on Artificial intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Zhan, Yang - 2007 - Semi-supervised learning with very few labeled training examples.pdf:pdf},
title = {{Semi-supervised learning with very few labeled training examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
year = {2007}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arlot, Celisse - 2010 - A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Loog2014b,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2014 - Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {5},
pages = {995 -- 1006},
title = {{Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood}},
volume = {26},
year = {2014}
}
@article{Wang2016a,
abstract = {In adaptive data analysis, the user makes a sequence of queries on the data, where at each step the choice of query may depend on the results in previous steps. The releases are often randomized in order to reduce overfitting for such adaptively chosen queries. In this paper, we propose a minimax framework for adaptive data analysis. Assuming Gaussianity of queries, we establish the first sharp minimax lower bound on the squared error in the order of {\$}O(\backslashfrac{\{}\backslashsqrt{\{}k{\}}\backslashsigma{\^{}}2{\}}{\{}n{\}}){\$}, where {\$}k{\$} is the number of queries asked, and {\$}\backslashsigma{\^{}}2/n{\$} is the ordinary signal-to-noise ratio for a single query. Our lower bound is based on the construction of an approximately least favorable adversary who picks a sequence of queries that are most likely to be affected by overfitting. This approximately least favorable adversary uses only one level of adaptivity, suggesting that the minimax risk for 1-step adaptivity with k-1 initial releases and that for {\$}k{\$}-step adaptivity are on the same order. The key technical component of the lower bound proof is a reduction to finding the convoluting distribution that optimally obfuscates the sign of a Gaussian signal. Our lower bound construction also reveals a transparent and elementary proof of the matching upper bound as an alternative approach to Russo and Zou (2015), who used information-theoretic tools to provide the same upper bound. We believe that the proposed framework opens up opportunities to obtain theoretical insights for many other settings of adaptive data analysis, which would extend the idea to more practical realms.},
archivePrefix = {arXiv},
arxivId = {1602.04287},
author = {Wang, Yu-Xiang and Lei, Jing and Fienberg, Stephen E.},
eprint = {1602.04287},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Lei, Fienberg - 2016 - A Minimax Theory for Adaptive Data Analysis.pdf:pdf},
title = {{A Minimax Theory for Adaptive Data Analysis}},
url = {http://arxiv.org/abs/1602.04287},
year = {2016}
}
@misc{Mitchell1980,
author = {Mitchell, Tom M.},
booktitle = {Psychology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mitchell - 1980 - The need for biases in learning generalizations.pdf:pdf},
title = {{The need for biases in learning generalizations}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/Need for Bias.pdf},
year = {1980}
}
@article{Cheplygina2010,
author = {Cheplygina, Veronika},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheplygina - 2010 - Random Subspace Method for One-Class Classifiers.pdf:pdf},
title = {{Random Subspace Method for One-Class Classifiers}},
year = {2010}
}
@book{Glymour2001,
author = {Glymour, Clark},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Glymour - 2001 - The Mind's Arrow.pdf:pdf},
isbn = {0262072203},
title = {{The Mind's Arrow}},
year = {2001}
}
@inproceedings{Fan2008,
author = {Fan, Bin and Lei, Zhen and Li, Stan Z.},
booktitle = {International Conference on Automatic Face {\&} Gesture Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Lei, Li - 2008 - Normalized LDA for Semi-supervised Learning.pdf:pdf},
pages = {1--6},
title = {{Normalized LDA for Semi-supervised Learning}},
year = {2008}
}
@article{Wang2012a,
abstract = {This paper proposes a method to select a set of genes from a large number of genes with the ability of classifying types of diseases. The proposed gene selection method is designed according to correlation analysis and the concept of 95{\%} reference range. The method is very simple and uses the information of all genes. We have used the method in leukemia patients and achieved good classification results.},
author = {Wang, Xiaodong and Tian, Jun},
doi = {10.1155/2012/586246},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Tian - 2012 - A gene selection method for cancer classification.pdf:pdf},
issn = {1748-6718},
journal = {Computational and mathematical methods in medicine},
keywords = {array,cancer classification,diagnosis,diagnostic tests,dna micro-,drug discovery,feature selection,gene selection,genomics,proteomics,recursive feature elimination,rna expression,support vector machines},
month = {jan},
pages = {586246},
pmid = {23251228},
title = {{A gene selection method for cancer classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23289441},
volume = {2012},
year = {2012}
}
@article{Reid2009,
abstract = {We present tight surrogate regret bounds for the
class of proper (i.e., Fisher consistent) losses.
The bounds generalise the margin-based bounds
due to Bartlett et al. (2006). The proof uses Taylor's
theorem and leads to new representations
for loss and regret and a simple proof of the integral
representation of proper losses. We also
present a different formulation of a duality result
of Bregman divergences which leads to a simple
demonstration of the convexity of composite
losses using canonical link functions},
author = {Reid, Mark and Williamson, Bob},
doi = {10.1145/1553374.1553489},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2009 - Surrogate Regret Bounds for Proper Losses.pdf:pdf},
isbn = {9781605585161},
keywords = {Learning/Statistics {\&} Optimisation},
number = {Theorem 3},
pages = {897--904},
title = {{Surrogate Regret Bounds for Proper Losses}},
url = {http://eprints.pascal-network.org/archive/00008977/},
year = {2009}
}
@article{Devroye1982,
abstract = {Consider the basic discrimination problem based on a sample of size n drawn from the distribution of (X, Y) on the Borel sets of Rdx {\{}O, 1{\}}. If 0 {\textless} R*{\textless} is a given number, and 'n - 0 is an arbitrary positive sequence, then for any discrimination rule one can find a distribution for (X, Y), not depending upon n, with Bayes probability of error R* such that the probability of error (Rn) of the discrimination rule is larger than R* + 'On for infinitely many n. We give a formal proof of this result, which is a generalization of a result by Cover [1].},
author = {Devroye, L},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Devroye - 1982 - Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {feb},
number = {2},
pages = {154--7},
pmid = {21869021},
title = {{Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869021},
volume = {4},
year = {1982}
}
@book{Wickham2015,
author = {Wickham, Hadley},
publisher = {O'Reilly Media},
title = {{R packages}},
year = {2015}
}
@article{VanRooden2010,
abstract = {The clinical variability between patients with Parkinson's disease (PD) may point at the existence of subtypes of the disease. Identification of subtypes is important, since a focus on homogeneous groups may enhance the chance of success of research on mechanisms of disease and may also lead to tailored treatment strategies. Cluster analysis (CA) is an objective method to classify patients into subtypes. We systematically reviewed the methodology and results of CA studies in PD to gain a better understanding of the robustness of identified subtypes. We found seven studies that fulfilled the inclusion criteria. Studies were limited by incomplete reporting and methodological limitations. Differences between studies rendered comparisons of the results difficult. However, it appeared that studies which applied a comparable design identified similar subtypes. The cluster profiles "old age-at-onset and rapid disease progression" and "young age-at-onset and slow disease progression" emerged from the majority of studies. Other cluster profiles were less consistent across studies. Future studies with a rigorous study design that is standardized with respect to the included variables, data processing, and CA technique may advance the knowledge on subtypes in PD.},
author = {van Rooden, Stephanie M and Heiser, Willem J and Kok, Joost N and Verbaan, Dagmar and van Hilten, Jacobus J and Marinus, Johan},
doi = {10.1002/mds.23116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2010 - The identification of Parkinson's disease subtypes using cluster analysis a systematic review.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Algorithms,Cluster Analysis,Humans,Parkinson Disease,Parkinson Disease: classification,PubMed,PubMed: statistics {\&} numerical data},
month = {jun},
number = {8},
pages = {969--78},
pmid = {20535823},
title = {{The identification of Parkinson's disease subtypes using cluster analysis: a systematic review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20535823},
volume = {25},
year = {2010}
}
@article{Giraud-carrier2004,
author = {Giraud-carrier, Christophe and Vilalta, Ricardo and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Vilalta, Brazdil - 2004 - Introduction to the special issue on meta-learning.pdf:pdf},
journal = {Machine learning},
keywords = {dynamic bias selection,inductive bias,meta-knowledge,meta-learning},
pages = {187--193},
title = {{Introduction to the special issue on meta-learning}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015878.60765.42},
volume = {54},
year = {2004}
}
@article{Hoffman2013,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
eprint = {1206.7051},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
number = {2},
pages = {1303--1347},
title = {{Stochastic Variational Inference}},
url = {http://jmlr.org/papers/v14/hoffman13a.html$\backslash$nhttp://arxiv.org/abs/1206.7051},
volume = {14},
year = {2013}
}
@phdthesis{Mika2002,
author = {Mika, Sebastian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mika - 2002 - Kernel fisher discriminants.pdf:pdf},
title = {{Kernel fisher discriminants}},
url = {http://opus.kobv.de/tuberlin/volltexte/2003/477/},
year = {2002}
}
@book{Brazdil2010,
author = {Brazdil, Pavel B. and Bernstein, Abraham},
editor = {Brazdil, Pavel and Bernstein, Abraham},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Bernstein - 2010 - Proceedings of 3rd Planning to Learn Workshop at ECAI 2010.pdf:pdf},
number = {Ecai},
title = {{Proceedings of 3rd Planning to Learn Workshop at ECAI 2010}},
year = {2010}
}
@article{Papadopoulos2016,
abstract = {Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning algorithm. Our scheme iterates between re-training the detector, re-localizing objects in the training images, and human verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Extensive experiments on PASCAL VOC 2007 show that (1) using human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detectors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme substantially reduces total annotation time by a factor 6x-9x.},
archivePrefix = {arXiv},
arxivId = {1602.08405},
author = {Papadopoulos, Dim P. and Uijlings, Jasper R. R. and Keller, Frank and Ferrari, Vittorio},
eprint = {1602.08405},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Papadopoulos et al. - 2016 - We don't need no bounding-boxes Training object class detectors using only human verification.pdf:pdf},
number = {1},
title = {{We don't need no bounding-boxes: Training object class detectors using only human verification}},
url = {http://arxiv.org/abs/1602.08405},
year = {2016}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1111/j.1467-9868.2010.00740.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meinshausen, B{\"{u}}hlmann - 2010 - Stability selection.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
month = {jul},
number = {4},
pages = {417--473},
title = {{Stability selection}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2010.00740.x},
volume = {72},
year = {2010}
}
@inproceedings{Steck2003,
author = {Steck, Harald and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steck, Jaakkola - 2003 - Bias-corrected bootstrap and model uncertainty.pdf:pdf},
title = {{Bias-corrected bootstrap and model uncertainty}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA66.pdf},
year = {2003}
}
@inproceedings{Roli2002,
author = {Roli, Fabio and Raudys, {\v{S}}arÅ«nas and Marcialis, Gian Luca},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roli, Raudys, Marcialis - 2002 - An experimental comparison of fixed and trained fusion rules for crisp classifier outputs.pdf:pdf},
title = {{An experimental comparison of fixed and trained fusion rules for crisp classifier outputs}},
url = {http://link.springer.com/chapter/10.1007/3-540-45428-4{\_}23},
year = {2002}
}
@article{Little1992,
abstract = {The literature of regression analysis with missing values of the independent variables is reviewed. Six classes of procedures are distinguished: complete case analysis, available case methods, least squares on imputed data, maximum likelihood, Bayesian methods, and multiple imputation. Methods are compared and illustrated when missing data are confined to one independent variable, and extensions to more general patterns are indicated. Attention is paid to the performance of methods when the missing data are not missing completely at random. Least squares methods that fill in missing X's using only data on the X's are contrasted with likelihood-based methods that use data on the X's and Y. The latter approach is preferred and provides methods for elaboration of the basic normal linear regression model. It is suggested that more widely distributed software is needed that advances beyond complete-case analysis, available-case analysis, and naive imputation methods. Bayesian simulation methods and multiple imputation are reviewed; these provide fruitful avenues for future research.},
author = {Little, Roderick J. a.},
doi = {10.2307/2290664},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little - 1992 - Regression with missing x's A review.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian inference,imputation,incomplete data,multiple imputation},
number = {420},
pages = {1227--1237},
pmid = {318},
title = {{Regression with missing x's: A review}},
url = {http://www.jstor.org.libproxy1.nus.edu.sg/stable/2290664},
volume = {87},
year = {1992}
}
@article{Seaman2013,
author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
doi = {10.1214/13-STS415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seaman et al. - 2013 - What Is Meant by â€œMissing at Randomâ€.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Ignorability, direct-likelihood inference, frequen,and phrases,direct-likelihood inference,frequen-,ignorability,missing completely at random,repeated sampling,tist inference},
month = {may},
number = {2},
pages = {257--268},
title = {{What Is Meant by â€œMissing at Randomâ€?}},
url = {http://projecteuclid.org/euclid.ss/1369147915},
volume = {28},
year = {2013}
}
@article{Raudys1998,
author = {Raudys, Sarunas and Duin, Robert P W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raudys, Duin - 1998 - Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {Dimensionality,fisher linear discriminant,generalization error,pseudo-inversion,sample size,scissors effect,statistical classification},
month = {apr},
number = {5-6},
pages = {385--392},
publisher = {Elsevier},
title = {{Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix}},
volume = {19},
year = {1998}
}
@article{Hernandez-Gonzalez2015,
author = {Hern{\'{a}}ndez-Gonz{\'{a}}lez, Jer{\'{o}}nimo and naki Inza and Lozano, Jose A.},
doi = {10.1016/j.patrec.2015.10.008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Gonz{\'{a}}lez, Inza, Lozano - 2015 - Weak supervision and other non-standard classification problems a taxonomy.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {degrees of supervision,partially supervised classification,weakly supervised classification},
pages = {49--55},
title = {{Weak supervision and other non-standard classification problems: a taxonomy}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515003505},
volume = {69},
year = {2015}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression shrinkage and selection via the lasso}},
volume = {58},
year = {1996}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We further propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). The usefulness of explanations is shown via novel experiments, both simulated and with human subjects. Our explanations empower users in various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1602.04938},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@article{Molinaro2005,
abstract = {In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Molinaro, Simon, Pfeiffer - 2005 - Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software},
month = {aug},
number = {15},
pages = {3301--7},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
pages = {1027--1035},
title = {{k-means ++ : The Advantages of Careful Seeding}},
year = {2007}
}
@article{McLachlan1977,
author = {McLachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1977 - Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observatio.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {358},
pages = {403--406},
title = {{Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations}},
volume = {72},
year = {1977}
}
@article{Lamb2016,
abstract = {We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders.},
archivePrefix = {arXiv},
arxivId = {1602.03220},
author = {Lamb, Alex and Dumoulin, Vincent and Courville, Aaron},
eprint = {1602.03220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lamb, Dumoulin, Courville - 2016 - Discriminative Regularization for Generative Models.pdf:pdf},
number = {Icml},
title = {{Discriminative Regularization for Generative Models}},
url = {http://arxiv.org/abs/1602.03220},
year = {2016}
}
@article{Horton2007,
abstract = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Development of statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be utilized in practice.},
author = {Horton, Nicholas J and Kleinman, Ken P},
doi = {10.1198/000313007X172556},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Horton, Kleinman - 2007 - Much ado about nothing A comparison of missing data methods and software to fit incomplete data regression mod.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American statistician},
keywords = {conditional gaussian,health services research,maximum likelihood,multiple imputation,psychiatric epidemi-},
month = {feb},
number = {1},
pages = {79--90},
pmid = {17401454},
title = {{Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839993{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {61},
year = {2007}
}
@article{Markatou2005,
author = {Markatou, Marianthi and Tian, H},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Markatou, Tian - 2005 - Analysis of variance of cross-validation estimators of the generalization error.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine {\ldots}},
keywords = {cross-validation,generalization error,moment approximation,prediction,variance},
pages = {1127--1168},
title = {{Analysis of variance of cross-validation estimators of the generalization error}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/MarkatouTBH05.pdf},
volume = {6},
year = {2005}
}
@article{Wang2007a,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2007 - On Transductive Support Vector Machines.pdf:pdf},
journal = {Contemporary Mathematics},
pages = {7--19},
title = {{On Transductive Support Vector Machines}},
volume = {443},
year = {2007}
}
@article{Brazdil2003a,
author = {Brazdil, Pavel B. and Soares, Carlos and Costa, JP Da},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Soares, Costa - 2003 - Ranking learning algorithms Using IBL and meta-learning on accuracy and time results.pdf:pdf},
journal = {Machine Learning},
keywords = {algorithm recommendation,data characterization,meta-learning,ranking},
pages = {251--277},
title = {{Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results}},
url = {http://link.springer.com/article/10.1023/A:1021713901879},
volume = {50},
year = {2003}
}
@article{Lopez-Paz2015,
abstract = {We describe generalized distillation, a framework to learn from multiple representations in a semisupervised fashion. We show that distillation (Hinton et al., 2015) and privileged information (Vapnik {\&} Izmailov, 2015) are particular instances of generalized distillation, give insight about why and when generalized distillation works, and provide numerical simulations to assess its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1511.03643},
author = {Lopez-Paz, David and Bottou, L{\'{e}}on and Sch{\"{o}}lkopf, Bernhard and Vapnik, Vladimir},
eprint = {1511.03643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-Paz et al. - 2015 - Unifying distillation and privileged information.pdf:pdf},
number = {1},
pages = {1--9},
title = {{Unifying distillation and privileged information}},
url = {http://arxiv.org/abs/1511.03643},
year = {2015}
}
@book{Quinonero-Candela2009,
author = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Quinonero-Candela et al. - 2009 - Dataset shift in machine learning.pdf:pdf},
isbn = {9780262170055},
title = {{Dataset shift in machine learning}},
url = {http://dl.acm.org/citation.cfm?id=1462129},
year = {2009}
}
@article{Nguyen2015,
author = {Nguyen, Thanh and Khosravi, Abbas and Creighton, Douglas and Nahavandi, Saeid},
doi = {10.1016/j.patrec.2015.03.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen et al. - 2015 - A novel aggregate gene selection method for microarray data classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Analytic hierarchy process,Classification,Gene expression profiles,Gene selection,Microarray data},
pages = {16--23},
publisher = {Elsevier Ltd.},
title = {{A novel aggregate gene selection method for microarray data classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001105},
volume = {60-61},
year = {2015}
}
@inproceedings{Goldberg2007,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Wright, Stephen},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu, Wright - 2007 - Dissimilarity in graph-based semi-supervised classification.pdf:pdf},
number = {1},
pages = {55--162},
title = {{Dissimilarity in graph-based semi-supervised classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS07{\_}GoldbergZW.pdf},
year = {2007}
}
@article{Yarowsky1995,
address = {Morristown, NJ, USA},
author = {Yarowsky, David},
doi = {10.3115/981658.981684},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
journal = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
pages = {189--196},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
year = {1995}
}
@article{Wang2007,
author = {Wang, Junhui and Shen, Xiaotong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen - 2007 - Large margin Semi-supervised Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalization,grouping,sequential quadratic programming,support vectors},
pages = {1867--1891},
title = {{Large margin Semi-supervised Learning}},
volume = {8},
year = {2007}
}
@article{Huopaniemi2010,
abstract = {Analysis of variance (ANOVA)-type methods are the default tool for the analysis of data with multiple covariates. These tools have been generalized to the multivariate analysis of high-throughput biological datasets, where the main challenge is the problem of small sample size and high dimensionality. However, the existing multi-way analysis methods are not designed for the currently increasingly important experiments where data is obtained from multiple sources. Common examples of such settings include integrated analysis of metabolic and gene expression profiles, or metabolic profiles from several tissues in our case, in a controlled multi-way experimental setup where disease status, medical treatment, gender and time-series are usual covariates.},
author = {Huopaniemi, Ilkka and Suvitaival, Tommi and Nikkil{\"{a}}, Janne and Oresic, Matej and Kaski, Samuel},
doi = {10.1093/bioinformatics/btq174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huopaniemi et al. - 2010 - Multivariate multi-way analysis of multi-source data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Analysis of Variance,Data Collection,Gene Expression Profiling,Gene Expression Profiling: methods,Multivariate Analysis},
month = {jun},
number = {12},
pages = {i391--8},
pmid = {20529933},
title = {{Multivariate multi-way analysis of multi-source data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2881359{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {26},
year = {2010}
}
@article{Cule2011,
abstract = {BACKGROUND: Technological developments have increased the feasibility of large scale genetic association studies. Densely typed genetic markers are obtained using SNP arrays, next-generation sequencing technologies and imputation. However, SNPs typed using these methods can be highly correlated due to linkage disequilibrium among them, and standard multiple regression techniques fail with these data sets due to their high dimensionality and correlation structure. There has been increasing interest in using penalised regression in the analysis of high dimensional data. Ridge regression is one such penalised regression technique which does not perform variable selection, instead estimating a regression coefficient for each predictor variable. It is therefore desirable to obtain an estimate of the significance of each ridge regression coefficient.$\backslash$n$\backslash$nRESULTS: We develop and evaluate a test of significance for ridge regression coefficients. Using simulation studies, we demonstrate that the performance of the test is comparable to that of a permutation test, with the advantage of a much-reduced computational cost. We introduce the p-value trace, a plot of the negative logarithm of the p-values of ridge regression coefficients with increasing shrinkage parameter, which enables the visualisation of the change in p-value of the regression coefficients with increasing penalisation. We apply the proposed method to a lung cancer case-control data set from EPIC, the European Prospective Investigation into Cancer and Nutrition.$\backslash$n$\backslash$nCONCLUSIONS: The proposed test is a useful alternative to a permutation test for the estimation of the significance of ridge regression coefficients, at a much-reduced computational cost. The p-value trace is an informative graphical tool for evaluating the results of a test of significance of ridge regression coefficients as the shrinkage parameter increases, and the proposed test makes its production computationally feasible.},
author = {Cule, Erika and Vineis, Paolo and {De Iorio}, Maria},
doi = {10.1186/1471-2105-12-372},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cule, Vineis, De Iorio - 2011 - Significance testing in ridge regression for genetic data.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {372},
pmid = {21929786},
publisher = {BioMed Central Ltd},
title = {{Significance testing in ridge regression for genetic data}},
url = {http://www.biomedcentral.com/1471-2105/12/372},
volume = {12},
year = {2011}
}
@article{Chandrasekaran2013,
abstract = {Modern massive datasets create a fundamental problem at the intersection of the computational and statistical sciences: how to provide guarantees on the quality of statistical inference given bounds on computational resources, such as time or space. Our approach to this problem is to define a notion of "algorithmic weakening," in which a hierarchy of algorithms is ordered by both computational efficiency and statistical efficiency, allowing the growing strength of the data at scale to be traded off against the need for sophisticated processing. We illustrate this approach in the setting of denoising problems, using convex relaxation as the core inferential tool. Hierarchies of convex relaxations have been widely used in theoretical computer science to yield tractable approximation algorithms to many computationally intractable tasks. In the current paper, we show how to endow such hierarchies with a statistical characterization and thereby obtain concrete tradeoffs relating algorithmic runtime to amount of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.1073v2},
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
doi = {10.1073/pnas.1302293110},
eprint = {arXiv:1211.1073v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {convex geometry,convex relaxation,high-dimensional statistics,massive datasets},
month = {mar},
number = {13},
pages = {E1181--90},
pmid = {23479655},
title = {{Computational and statistical tradeoffs via convex relaxation.}},
volume = {110},
year = {2013}
}
@article{Webb1996,
author = {Webb, Geoffrey I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb - 1996 - Further Experimental Evidence against the Utility of Occam's Razor.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {397--417},
title = {{Further Experimental Evidence against the Utility of Occam's Razor}},
url = {http://arxiv.org/abs/cs/9605101},
volume = {4},
year = {1996}
}
@article{Bodo2015,
abstract = {{\textless}p{\textgreater}Semi-supervised learning has become an important and thoroughly studied subdomain of machine learning in the past few years, because gathering large unlabeled data is almost costless, and the costly human labeling process can be minimized by semi-supervision. Label propagation is a transductive semi-supervised learning method that operates on theâ€”most of the time undirectedâ€”data graph. It was introduced in [8] and since many variants were proposed. However, the base algorithm has two variants: the first variant presented in [8] and its slightly modified version used afterwards, e.g. in [7]. This paper presents and compares the two algorithmsâ€”both theoretically and experimentallyâ€”and also tries to make a recommendation which variant to use.{\textless}/p{\textgreater}},
author = {Bod{\'{o}}, Zal{\'{a}}n and Csat{\'{o}}, Lehel},
doi = {10.1515/ausi-2015-0010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bod{\'{o}}, Csat{\'{o}} - 2015 - A note on label propagation for semi-supervised learning.pdf:pdf},
issn = {2066-7760},
journal = {Acta Universitatis Sapientiae, Informatica},
keywords = {and phrases,label propagation,semi-supervised learning},
number = {1},
pages = {18--30},
title = {{A note on label propagation for semi-supervised learning}},
url = {http://www.degruyter.com/view/j/ausi.2015.7.issue-1/ausi-2015-0010/ausi-2015-0010.xml},
volume = {7},
year = {2015}
}
@article{Mann2010,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2010 - Generalized expectation criteria for semi-supervised learning with weakly labeled data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {condi-,generalized expectation criteria,logistic regression,semi-supervised learning,tional random fields},
pages = {955--984},
title = {{Generalized expectation criteria for semi-supervised learning with weakly labeled data}},
volume = {11},
year = {2010}
}
@inproceedings{Plessis2012,
author = {du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Plessis, Sugiyama - 2012 - Semi-supervised learning of class balance under class-prior change by distribution matching.pdf:pdf},
title = {{Semi-supervised learning of class balance under class-prior change by distribution matching}},
url = {http://arxiv.org/abs/1206.4677},
year = {2012}
}
@article{Wilkinson1958,
author = {Wilkinson, G. N.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilkinson - 1958 - Estimation of Missing Values for the Analysis of Incomplete Data.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
number = {2},
pages = {257--286},
title = {{Estimation of Missing Values for the Analysis of Incomplete Data}},
volume = {14},
year = {1958}
}
@inproceedings{Scholkopf2012,
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf, Janzing, Peters - 2012 - On Causal and Anticausal Learning.pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@unpublished{Tan2013,
author = {Tan, Yimin and Zhu, X},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tan, Zhu - 2013 - Dragging Density-Ratio Bagging.pdf:pdf},
pages = {1--10},
title = {{Dragging: Density-Ratio Bagging}},
url = {https://minds.wisconsin.edu/bitstream/handle/1793/65831/TR1795.pdf?sequence=1},
year = {2013}
}
@article{Amini2002,
author = {Amini, Massih-Reza and Gallinari, Patrick},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amini, Gallinari - 2002 - Semi-supervised logistic regression.pdf:pdf},
journal = {15th European Conference on Artificial Intelligence},
pages = {390--394},
title = {{Semi-supervised logistic regression}},
year = {2002}
}
@article{Wyman1990,
author = {Wyman, Frank J. and Young, Dean M. and Turner, Danny W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wyman, Young, Turner - 1990 - A Comparison of Asymptotic Error Rate Expansions for the Sample Linear Discriminant Function.pdf:pdf},
journal = {Pattern Recogtion},
number = {7},
pages = {775--783},
title = {{A Comparison of Asymptotic Error Rate Expansions for the Sample Linear Discriminant Function}},
volume = {23},
year = {1990}
}
@article{Hanczar2010,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.2174/157489310790596376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Dougherty - 2010 - On the Comparison of Classifiers for Microarray Data.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {classifier comparison,error estimation,microarray classification,variance study},
month = {mar},
number = {1},
pages = {29--39},
title = {{On the Comparison of Classifiers for Microarray Data}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1574-8936{\&}volume=5{\&}issue=1{\&}spage=29},
volume = {5},
year = {2010}
}
@article{Shen2016a,
abstract = {Structural equation models (SEMs) have been widely adopted for inference of causal interactions in complex networks. Recent examples include unveiling topologies of hidden causal networks over which processes such as spreading diseases, or rumors propagate. The appeal of SEMs in these settings stems from their simplicity and tractability, since they typically assume linear dependencies among observable variables. Acknowledging the limitations inherent to adopting linear models, the present paper advocates nonlinear SEMs, which account for (possible) nonlinear dependencies among network nodes. The advocated approach leverages kernels as a powerful encompassing framework for nonlinear modeling, and an efficient estimator with affordable tradeoffs is put forth. Interestingly, pursuit of the novel kernel-based approach yields a convex regularized estimator that promotes edge sparsity, and is amenable to proximal-splitting optimization methods. To this end, solvers with complementary merits are developed by leveraging the alternating direction method of multipliers, and proximal gradient iterations. Experiments conducted on simulated data demonstrate that the novel approach outperforms linear SEMs with respect to edge detection errors. Furthermore, tests on a real gene expression dataset unveil interesting new edges that were not revealed by linear SEMs, which could shed more light on regulatory behavior of human genes.},
archivePrefix = {arXiv},
arxivId = {1605.03122},
author = {Shen, Yanning and Baingana, Brian and Giannakis, Georgios B.},
eprint = {1605.03122},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shen, Baingana, Giannakis - 2016 - Kernel-Based Structural Equation Models for Topology Identification of Directed Networks.pdf:pdf},
pages = {1--13},
title = {{Kernel-Based Structural Equation Models for Topology Identification of Directed Networks}},
url = {http://arxiv.org/abs/1605.03122},
volume = {2016},
year = {2016}
}
@article{Abney2004,
author = {Abney, Steven},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2004 - Understanding the yarowsky algorithm.pdf:pdf},
journal = {Computational Linguistics},
number = {3},
pages = {365--395},
title = {{Understanding the yarowsky algorithm}},
volume = {30},
year = {2004}
}
@article{Huggins2016,
abstract = {The use of Bayesian models in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. The proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size.},
archivePrefix = {arXiv},
arxivId = {1605.06423},
author = {Huggins, Jonathan H. and Campbell, Trevor and Broderick, Tamara},
eprint = {1605.06423},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huggins, Campbell, Broderick - 2016 - Coresets for Scalable Bayesian Logistic Regression.pdf:pdf},
pages = {1--20},
title = {{Coresets for Scalable Bayesian Logistic Regression}},
url = {http://arxiv.org/abs/1605.06423},
year = {2016}
}
@article{Xie2014,
author = {Xie, Yihui},
chapter = {1},
editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
journal = {Implementing Reproducible Research},
publisher = {CRC Press},
title = {{knitr: A Comprehensive Tool for Reproducible Research in R}},
year = {2014}
}
@book{Mohri2012,
abstract = {In distributed learning, the goal is to perform a learning task over data distributed across multiple nodes with minimal (expensive) communication. Prior work (Daume III et al., 2012) proposes a general model that bounds the communication required for learning classifiers while allowing for {\$}\backslasheps{\$} training error on linearly separable data adversarially distributed across nodes. In this work, we develop key improvements and extensions to this basic model. Our first result is a two-party multiplicative-weight-update based protocol that uses {\$}O(d{\^{}}2 \backslashlog{\{}1/\backslasheps{\}}){\$} words of communication to classify distributed data in arbitrary dimension {\$}d{\$}, {\$}\backslasheps{\$}-optimally. This readily extends to classification over {\$}k{\$} nodes with {\$}O(kd{\^{}}2 \backslashlog{\{}1/\backslasheps{\}}){\$} words of communication. Our proposed protocol is simple to implement and is considerably more efficient than baselines compared, as demonstrated by our empirical results. In addition, we illustrate general algorithm design paradigms for doing efficient learning over distributed data. We show how to solve fixed-dimensional and high dimensional linear programming efficiently in a distributed setting where constraints may be distributed across nodes. Since many learning problems can be viewed as convex optimization problems where constraints are generated by individual points, this models many typical distributed learning scenarios. Our techniques make use of a novel connection from multipass streaming, as well as adapting the multiplicative-weight-update framework more generally to a distributed setting. As a consequence, our methods extend to the wide range of problems solvable using these techniques.},
archivePrefix = {arXiv},
arxivId = {1204.3523},
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
eprint = {1204.3523},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mohri, Rostamizadeh, Talwalkar - 2012 - Foundations of Machine Learning.pdf:pdf},
isbn = {978-0-262-01825-8},
issn = {03029743},
number = {4},
pages = {1--8},
pmid = {18772260},
publisher = {The MIT Press},
title = {{Foundations of Machine Learning}},
volume = {17},
year = {2012}
}
@article{Liang2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0710.4618v1},
author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
doi = {10.1214/088342307000000032},
eprint = {arXiv:0710.4618v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liang, Mukherjee, West - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayesian analysis,bayesian kernel regression,latent factor models,mixture models,pervised learning,predictive distribution,semisu-,unlabeled data},
month = {may},
number = {2},
pages = {189--205},
title = {{The Use of Unlabeled Data in Predictive Modeling}},
url = {http://projecteuclid.org/euclid.ss/1190905518},
volume = {22},
year = {2007}
}
@article{Stahlecker1996,
author = {Stahlecker, Peter and Knautz, Henning and Trenkler, Gotz},
doi = {10.1007/BF00046994},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stahlecker, Knautz, Trenkler - 1996 - Minimax adjustment technique in a parameter restricted linear model.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {linear regression,minimax adjustment,projection estimator},
month = {apr},
number = {1},
pages = {139--144},
title = {{Minimax adjustment technique in a parameter restricted linear model}},
url = {http://link.springer.com/10.1007/BF00046994},
volume = {43},
year = {1996}
}
@article{Marchand,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08329v1},
author = {Marchand, Mario and Roy, Jean-francis},
eprint = {arXiv:1503.08329v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Roy - Unknown - PAC-Bayesian Bounds for the Risk of the Majority Vote.pdf:pdf},
keywords = {ensemble methods,learning theory,majority vote,pac-bayesian theory},
number = {2006},
title = {{PAC-Bayesian Bounds for the Risk of the Majority Vote}}
}
@article{Rooyena,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01520v1},
author = {Rooyen, Brendan Van and Menon, Aditya Krishna and Williamson, Robert C},
eprint = {arXiv:1506.01520v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rooyen, Menon, Williamson - Unknown - An Average Classification Algorithm.pdf:pdf},
pages = {1--16},
title = {{An Average Classification Algorithm}}
}
@article{Zhu2014,
abstract = {In many situations we have some measurement of confidence on " positiveness " for a binary label. The " positiveness " is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called expectation loss SVM (e-SVM) that is devoted to the problems where only the " positiveness " instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further vali-date this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.},
author = {Zhu, Jun and Mao, Junhua and Yuille, Alan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Mao, Yuille - 2014 - Learning From Weakly Supervised Data by The Expectation Loss SVM ( e-SVM ) Algorithm.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Learning From Weakly Supervised Data by The Expectation Loss SVM ( e-SVM ) Algorithm}},
year = {2014}
}
@article{Lee2013,
abstract = {Abstract We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo -Labels, just ... $\backslash$n},
author = {Lee, D H},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lee - 2013 - Pseudo-label The simple and efficient semi-supervised learning method for deep neural networks.pdf:pdf},
journal = {ICML Workshop on Challenges in Representation Learning},
title = {{Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks}},
url = {https://www.kaggle.com/blobs/download/forum-message-attachment-files/746/pseudo{\_}label{\_}final.pdf$\backslash$npapers3://publication/uuid/FAC81169-4273-4668-90FA-2B849AAA67C1},
year = {2013}
}
@article{Hoogerbrugge1983,
author = {Hoogerbrugge, Ronald and Willig, Simon J. and Kistemaker, Piet G.},
doi = {10.1021/ac00261a016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoogerbrugge, Willig, Kistemaker - 1983 - Discriminant analysis by double stage principal component analysis.pdf:pdf},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = {sep},
number = {11},
pages = {1710--1712},
title = {{Discriminant analysis by double stage principal component analysis}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00261a016},
volume = {55},
year = {1983}
}
@inproceedings{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-hua},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2011 - Towards making unlabeled data never hurt.pdf:pdf},
pages = {1081--1088},
title = {{Towards making unlabeled data never hurt}},
year = {2011}
}
@inproceedings{Yu2013,
author = {Yu, Felix X. and Liu, Dong and Kumar, Sanjiv and Jebara, Tony and Chang, Shih-Fu},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu et al. - 2013 - {\$}{\$}backslash{\$} propto {\$} SVM for Learning with Label Proportions.pdf:pdf},
pages = {504--512},
title = {{{\$}{\$}$\backslash$backslash{\$} propto {\$} SVM for Learning with Label Proportions}},
year = {2013}
}
@article{Bengio2007,
author = {Bengio, Yoshua and LeCun, Yann},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, LeCun - 2007 - Scaling Learning Algorithms towards AI.pdf:pdf},
journal = {Large-Scale Kernel Machines},
number = {1},
pages = {1--41},
title = {{Scaling Learning Algorithms towards AI}},
url = {http://www.iro.umontreal.ca/{~}lisa/bib/pub{\_}subject/language/pointeurs/bengio+lecun-chapter2007.pdf},
year = {2007}
}
@article{Castelli1994,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1995 - On the exponential value of labeled samples.pdf:pdf},
journal = {Pattern Recognition Letters},
pages = {105--111},
title = {{On the exponential value of labeled samples}},
volume = {16},
year = {1995}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.0020124},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2005}
}
@inproceedings{Bennett1998,
author = {Bennett, Kristin P. and Demiriz, Ayhan},
booktitle = {Advances in Neural Information Processing Systems 11},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bennett, Demiriz - 1998 - Semi-supervised support vector machines.pdf:pdf},
pages = {368--374},
title = {{Semi-supervised support vector machines}},
year = {1998}
}
@article{Bloodgood2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06329v1},
author = {Bloodgood, Michael and Grothendieck, John},
eprint = {arXiv:1504.06329v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bloodgood, Grothendieck - 2013 - Analysis of Stopping Active Learning based on Stabilizing Predictions.pdf:pdf},
journal = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
pages = {10--19},
title = {{Analysis of Stopping Active Learning based on Stabilizing Predictions}},
url = {http://www.aclweb.org/anthology/W13-3502},
year = {2013}
}
@book{RamonyCajal1897a,
abstract = {Santiago Ramon y Cajal was a mythic figure in science. Hailed as the father of modernanatomy and neurobiology, he was largely responsible for the modern conception of the brain. Hisgroundbreaking works were New Ideas on the Structure of the Nervous System and Histology of the Nervous System in Man and Vertebrates. In addition to leaving alegacy of unparalleled scientific research, Cajal sought to educate the novice scientist about howscience was done and how he thought it should be done. This recently rediscovered classic, firstpublished in 1897, is an anecdotal guide for the perplexed new investigator as well as a refreshingresource for the old pro.Cajal was a pragmatist, aware of the pitfalls of beingtoo idealistic -- and he had a sense of humor, particularly evident in his diagnoses of variousstereotypes of eccentric scientists. The book covers everything from valuable personality traits foran investigator to social factors conducive to scientific work.},
author = {{Ram{\'{o}}n y Cajal}, Santiago},
booktitle = {Advice for a Young Investigator},
doi = {10.1016/S0166-2236(00)01546-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ram{\'{o}}n y Cajal - 1897 - Advice for a Young Investigator (translated by Neely Swanson and Larry W. Swanson).pdf:pdf},
isbn = {0262181916},
issn = {01662236},
number = {7},
pages = {1--150},
title = {{Advice for a Young Investigator (translated by Neely Swanson and Larry W. Swanson)}},
volume = {23},
year = {1897}
}
@article{Benavoli2016,
abstract = {The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.},
archivePrefix = {arXiv},
arxivId = {1606.04316},
author = {Benavoli, Alessio and Corani, Giorgio and Demsar, Janez and Zaffalon, Marco},
eprint = {1606.04316},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Benavoli et al. - 2016 - Time for a change a tutorial for comparing multiple classifiers through Bayesian analysis.pdf:pdf},
number = {2006},
pages = {1--35},
title = {{Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis}},
url = {http://arxiv.org/abs/1606.04316},
year = {2016}
}
@inproceedings{Hernandez-reyes2005,
author = {Hern{\'{a}}ndez-reyes, Edith and Carrasco-Ochoa, J.A. and Mart{\'{i}}nez-trinidad, J Fco},
booktitle = {Proceedings of the 10th Iberoamerican Congress on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-reyes, Carrasco-Ochoa, Mart{\'{i}}nez-trinidad - 2005 - Classifier Selection Based on Data Complexity Measures.pdf:pdf},
pages = {586--592},
title = {{Classifier Selection Based on Data Complexity Measures}},
year = {2005}
}
@article{Fumera,
author = {Fumera, Giorgio and Roli, Fabio and Serrau, Alessandra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fumera, Roli, Serrau - Unknown - A Theoretical Analysis of Bagging as a Linear Combination of Classifiers.pdf:pdf},
pages = {1--19},
title = {{A Theoretical Analysis of Bagging as a Linear Combination of Classifiers}}
}
@book{Rothenberg1973,
author = {Rothenberg, Thomas J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothenberg - 1973 - Efficient Estimation with A Priori Information.pdf:pdf},
publisher = {Yale University Press},
title = {{Efficient Estimation with A Priori Information}},
url = {http://www.getcited.org/pub/101421013},
year = {1973}
}
@article{Minka1998,
author = {Minka, Thomas P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 1998 - Escpectation-Maximization as lower bound maximization.pdf:pdf},
number = {1977},
pages = {1--8},
title = {{Escpectation-Maximization as lower bound maximization}},
year = {1998}
}
@article{Burman1989,
author = {Burman, Prabir},
doi = {10.2307/2336116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Burman - 1989 - A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {sep},
number = {3},
pages = {503},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {http://www.jstor.org/stable/2336116?origin=crossref},
volume = {76},
year = {1989}
}
@book{Pearl2014,
author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl, Glymour, Jewell - 2016 - Causal Inference in Statistics A Primer.pdf:pdf},
publisher = {Wiley},
title = {{Causal Inference in Statistics: A Primer}},
year = {2016}
}
@techreport{Wolpert1996,
author = {Wolpert, David H and Macready, W},
booktitle = {Santa Fe Institute Technical Report},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert, Macready - 1996 - Combining Stacking With Bagging To Improve A Learning Algorithm.pdf:pdf},
pages = {1--28},
title = {{Combining Stacking With Bagging To Improve A Learning Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9933{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@article{Henmi2004,
author = {Henmi, Masayuki and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Eguchi - 2004 - A paradox concerning nuisance parameters and projected estimating functions.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {929--941},
title = {{A paradox concerning nuisance parameters and projected estimating functions}},
volume = {91},
year = {2004}
}
@article{Castelli1996,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1996 - The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {2102},
title = {{The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter}},
volume = {42},
year = {1996}
}
@article{Kuncheva2002,
author = {Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2002 - A Theoretical Study on Six Classifier Fusion Strategies.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {281--286},
title = {{A Theoretical Study on Six Classifier Fusion Strategies}},
volume = {24},
year = {2002}
}
@article{White1982,
author = {White, Halbert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White - 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {1--25},
title = {{Maximum Likelihood Estimation of Misspecified Models}},
url = {http://www.jstor.org/stable/10.2307/1912526},
volume = {50},
year = {1982}
}
@article{Buhlmann2014,
author = {B{\"{u}}hlmann, Peter},
doi = {10.1214/13-STS460},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/B{\"{u}}hlmann - 2014 - Discussion of Big Bayes Stories and BayesBag.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
month = {feb},
number = {1},
pages = {91--94},
title = {{Discussion of Big Bayes Stories and BayesBag}},
url = {http://projecteuclid.org/euclid.ss/1399645732},
volume = {29},
year = {2014}
}
@article{Ireland1968a,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Contingence tables with given marginals.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {179--188},
title = {{Contingence tables with given marginals}},
volume = {55},
year = {1968}
}
@article{VanRooden2011,
abstract = {The clinical heterogeneity of Parkinson's disease (PD) may point at the existence of subtypes. Because subtypes likely reflect distinct underlying etiologies, their identification may facilitate future genetic and pharmacotherapeutic studies. Aim of this study was to identify subtypes by a data-driven approach applied to a broad spectrum of motor and nonmotor features of PD. Data of motor and nonmotor PD symptoms were collected in 802 patients in two different European prevalent cohorts. A model-based cluster analysis was conducted on baseline data of 344 patients of a Dutch cohort (PROPARK). Reproducibility of these results was tested in data of the second annual assessment of the same cohort and validated in an independent Spanish cohort (ELEP) of 357 patients. The subtypes were subsequently characterized on clinical and demographic variables. Four similar PD subtypes were identified in two different populations and are largely characterized by differences in the severity of nondopaminergic features and motor complications: Subtype 1 was mildly affected in all domains, Subtype 2 was predominantly characterized by severe motor complications, Subtype 3 was affected mainly on nondopaminergic domains without prominent motor complications, while Subtype 4 was severely affected on all domains. The subtypes had largely similar mean disease durations (nonsignificant differences between three clusters) but showed considerable differences with respect to their association with demographic and clinical variables. In prevalent disease, PD subtypes are largely characterized by the severity of nondopaminergic features and motor complications and likely reflect complex interactions between disease mechanisms, treatment, aging, and gender.},
author = {van Rooden, Stephanie M and Colas, Fabrice P. R. and Mart{\'{i}}nez-Mart{\'{i}}n, Pablo and Visser, Martine and Verbaan, Dagmar and Marinus, Johan and Chaudhuri, Ray K and Kok, Joost N and van Hilten, Jacobus J},
doi = {10.1002/mds.23346},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2011 - Clinical subtypes of Parkinson's disease.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Aged,Cluster Analysis,Cohort Studies,Disease Progression,Female,Germany,Humans,Male,Middle Aged,Neurologic Examination,Parkinson Disease,Parkinson Disease: classification,Parkinson Disease: physiopathology,Reproducibility of Results,Spain,Time Factors},
month = {jan},
number = {1},
pages = {51--8},
pmid = {21322019},
title = {{Clinical subtypes of Parkinson's disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21322019},
volume = {26},
year = {2011}
}
@book{Rasmussen2004,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmussen, Williams - 2006 - Gaussian processes for machine learning.pdf:pdf},
issn = {0129-},
month = {apr},
title = {{Gaussian processes for machine learning.}},
year = {2006}
}
@book{Dresher1961,
author = {Dresher, Melvin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dresher - 1961 - Games of Strategy Theory and Applications.pdf:pdf},
pages = {1--186},
publisher = {Prentice Hall},
title = {{Games of Strategy: Theory and Applications}},
year = {1961}
}
@inproceedings{Brazdil1994,
author = {Brazdil, Pavel B. and Gama, Joao and Henery, Bob},
booktitle = {Proceedings of the European conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Gama, Henery - 1994 - Characterizing the applicability of classification algorithms using meta-level learning.pdf:pdf},
pages = {83--102},
title = {{Characterizing the applicability of classification algorithms using meta-level learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-57868-4{\_}52},
year = {1994}
}
@inproceedings{Singh2008,
author = {Singh, Aarti and Nowak, Robert D. and Zhu, Xiaojin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Singh, Nowak, Zhu - 2008 - Unlabeled data Now it helps, now it doesn't.pdf:pdf},
pages = {1513--1520},
title = {{Unlabeled data: Now it helps, now it doesn't}},
year = {2008}
}
@article{Varma2006,
abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Bias (Epidemiology),Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {jan},
pages = {91},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1397873{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@inproceedings{Elworthy1994,
archivePrefix = {arXiv},
arxivId = {arXiv:cmp-lg/9410012v2},
author = {Elworthy, David},
booktitle = {Proceedings of the fourth conference on Applied natural language processing},
eprint = {9410012v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Elworthy - 1994 - Does Baum-Welch re-estimation help taggers.pdf:pdf},
pages = {53--58},
primaryClass = {arXiv:cmp-lg},
title = {{Does Baum-Welch re-estimation help taggers?}},
year = {1994}
}
@article{Hartley1968,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - A new estimation for sample theory surveys.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {547--557},
title = {{A new estimation for sample theory surveys}},
volume = {55},
year = {1968}
}
@inproceedings{Ben-David2012,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David et al. - 2012 - Minimizing the misclassification error rate using a surrogate convex loss.pdf:pdf},
pages = {1863--1870},
title = {{Minimizing the misclassification error rate using a surrogate convex loss}},
year = {2012}
}
@article{Johnson2015,
abstract = {We demonstrate the usefulness of submodularity in statistics. Greedy algorithms such as forward stepwise regression and the lasso perform well in situations that can be characterized by submodularity. In particular, submodularity of the coefficient of determination, R{\$}{\^{}}2{\$}, provides a natural way to analyze the effects of collinearity on model selection. In model selection, we encounter the search problem of identifying a subset of k covariates with predictive loss close to that of the best model of k covariates. Submodularity arises naturally in this setting due to its deep roots within combinatorial optimization. It provides structural results for discrete convexity as well as guarantees for the success of greedy algorithms. In statistics, submodularity isolates cases in which collinearity makes the choice of model features difficult from those in which this task is routine. Submodularity of R{\$}{\^{}}2{\$} is closely related to other statistical assumptions used in variable screening and proving the performance of the Lasso. This has important implications for the use of model selection procedures: the situations in which forward stepwise and Lasso are successful are closely related.},
archivePrefix = {arXiv},
arxivId = {1510.06301},
author = {Johnson, Kory D. and Stine, Robert A. and Foster, Dean P.},
eprint = {1510.06301},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson, Stine, Foster - 2015 - Submodularity in Statistics Comparing the Success of Model Selection Methods.pdf:pdf},
number = {1},
pages = {1--12},
title = {{Submodularity in Statistics: Comparing the Success of Model Selection Methods}},
url = {http://arxiv.org/abs/1510.06301},
year = {2015}
}
@inproceedings{Cortes2010,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mansour, Mohri - 2010 - Learning bounds for importance weighting.pdf:pdf},
pages = {442--450},
title = {{Learning bounds for importance weighting}},
url = {http://www.cs.nyu.edu/{~}mohri/pub/importance.pdf},
year = {2010}
}
@unpublished{Bresson2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0699v1},
author = {Bresson, Xavier and Zhang, Ruiliang},
booktitle = {arXiv preprint},
eprint = {arXiv:1210.0699v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bresson, Zhang - 2012 - TV-SVM Total Variation Support Vector Machine for Semi-Supervised Data Classification.pdf:pdf},
title = {{TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification}},
url = {http://arxiv.org/abs/1210.0699},
year = {2012}
}
@article{Gaffke1989,
author = {Gaffke, Norbert and Heiligers, Berthold},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gaffke, Heiligers - 1989 - Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space.pdf:pdf},
journal = {Statistics: A Journal of Theoretical and Applied Statistics},
pages = {487--508},
title = {{Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space}},
volume = {4},
year = {1989}
}
@book{Zhu2009,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
editor = {Brachman, Ronald J. and Dietterich, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Goldberg - 2009 - Introduction to Semi-Supervised Learning.pdf:pdf},
isbn = {9781598295474},
number = {1},
pages = {1--130},
publisher = {Morgan {\&} Claypool},
title = {{Introduction to Semi-Supervised Learning}},
volume = {3},
year = {2009}
}
@inproceedings{Corduneanu2002,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
booktitle = {Proceedings of the 19th conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Corduneanu, Jaakkola - 2002 - On information regularization.pdf:pdf},
pages = {151--158},
title = {{On information regularization}},
url = {http://dl.acm.org/citation.cfm?id=2100602},
year = {2002}
}
@article{Caticha2011,
author = {Caticha, Ariel and Mohammad-Djafari, Ali and Bercher, Jean-FrancÌ§ois and BessieÌre, Pierre},
doi = {10.1063/1.3573619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha et al. - 2011 - Entropic Inference.pdf:pdf},
isbn = {9780735408609},
keywords = {bayes rule,entropy,information,maximum entropy},
number = {1},
pages = {20--29},
title = {{Entropic Inference}},
url = {http://link.aip.org/link/APCPCS/v1305/i1/p20/s1{\&}Agg=doi},
volume = {20},
year = {2011}
}
@article{Lucic2015,
abstract = {Coresets are efficient representations of datasets such that models trained on a coreset are provably competitive with models trained on the original dataset. As such, they have been successfully used to scale up clustering models such as K-Means and Gaussian mixture models to massive datasets. However, until now, the algorithms and corresponding theory were usually specific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for a large class of hard and soft clustering problems based on Bregman divergences. This class includes hard clustering with popular distortion measures such as the Squared Euclidean distance, the Mahalanobis distance, KL-divergence, Itakura-Saito distance and relative entropy. The corresponding soft clustering problems are directly related to popular mixture models due to a dual relationship between Bregman divergences and Exponential family distributions. Our results recover existing coreset constructions for K-Means and Gaussian mixture models and imply polynomial time approximations schemes for various hard clustering problems.},
archivePrefix = {arXiv},
arxivId = {1508.05243},
author = {Lucic, Mario and Bachem, Olivier and Krause, Andreas},
eprint = {1508.05243},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lucic, Bachem, Krause - 2015 - Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures.pdf:pdf},
title = {{Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures}},
url = {http://arxiv.org/abs/1508.05243},
year = {2015}
}
@article{Minh2014,
abstract = {This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi-view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classification, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semi-supervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging datasets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1401.8066},
author = {Minh, Ha Quang and Bazzani, Loris and Murino, Vittorio},
eprint = {1401.8066},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minh, Bazzani, Murino - 2014 - A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and C.pdf:pdf},
keywords = {kernel methods,learning,manifold regularization,multi-class classification,multi-kernel learning,multi-modality,multi-view learning,vector-valued rkhs},
pages = {1--72},
title = {{A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning}},
url = {http://arxiv.org/abs/1401.8066},
volume = {17},
year = {2014}
}
@article{Hanselmann2013,
abstract = {Digital staining for the automated annotation of mass spectrometry imaging (MSI) data has previously been achieved using state-of-the-art classifiers such as random forests or support vector machines (SVMs). However, the training of such classifiers requires an expert to label exemplary data in advance. This process is time-consuming and hence costly, especially if the tissue is heterogeneous. In theory, it may be sufficient to only label a few highly representative pixels of an MS image, but it is not known a priori which pixels to select. This motivates active learning strategies in which the algorithm itself queries the expert by automatically suggesting promising candidate pixels of an MS image for labeling. Given a suitable querying strategy, the number of required training labels can be significantly reduced while maintaining classification accuracy. In this work, we propose active learning for convenient annotation of MSI data. We generalize a recently proposed active learning method to the multiclass case and combine it with the random forest classifier. Its superior performance over random sampling is demonstrated on secondary ion mass spectrometry data, making it an interesting approach for the classification of MS images.},
author = {Hanselmann, Michael and R{\"{o}}der, Jens and K{\"{o}}the, Ullrich and Renard, Bernhard Y and Heeren, Ron M.A. and Hamprecht, Fred A.},
doi = {10.1021/ac3023313},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2013 - Active learning for convenient annotation and classification of secondary ion mass spectrometry images.pdf:pdf},
issn = {1520-6882},
journal = {Analytical Chemistry},
month = {jan},
number = {1},
pages = {147--55},
pmid = {23157438},
title = {{Active learning for convenient annotation and classification of secondary ion mass spectrometry images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23157438},
volume = {85},
year = {2013}
}
@article{Dwork2015,
author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
doi = {10.1126/science.aaa9375},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dwork et al. - 2015 - The reusable holdout Preserving validity in adaptive data analysis(2).pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Dwork et al. - 2015 - The reusable holdout Preserving validity in adaptive data analysis.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6248},
pages = {636--638},
title = {{The reusable holdout: Preserving validity in adaptive data analysis}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa9375},
volume = {349},
year = {2015}
}
@article{Bickel2009,
author = {Bickel, Steffen and Br{\"{u}}ckner, Michael and Scheffer, Tobias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bickel, Br{\"{u}}ckner, Scheffer - 2009 - Discriminative learning under covariate shift.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {covariate shift,discriminative learning,transfer learning},
pages = {2137--2155},
title = {{Discriminative learning under covariate shift}},
url = {http://dl.acm.org/citation.cfm?id=1755858},
volume = {10},
year = {2009}
}
@article{Shevade2003,
author = {Shevade, S. K. and Keerthi, S. S.},
doi = {10.1093/bioinformatics/btg308},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shevade, Keerthi - 2003 - A simple and efficient algorithm for gene selection using sparse logistic regression.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {17},
pages = {2246--2253},
title = {{A simple and efficient algorithm for gene selection using sparse logistic regression}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btg308},
volume = {19},
year = {2003}
}
@article{Li2013a,
abstract = {In this paper, we study the heterogeneous domain adaptation (HDA) problem, in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. By introducing two different projection matrices, we first transform the data from two domains into a common subspace such that the similarity between samples across different domains can be measured. We then develop two new feature mapping functions for two domains, which respectively augments the transformed source and target samples with their original features and padding zeros. Existing supervised learning methods (e.g., SVM and SVR) can be readily employed by incorporating our newly proposed augmented feature representations for supervised HDA. As a showcase, we propose a novel method called Heterogeneous Feature Augmentation (HFA) based on SVM. We show that the proposed formulation can be equivalently derived as a standard Multiple Kernel Learning (MKL) problem, which is convex and thus the global solution can be guaranteed. To additionally utilize the unlabeled data in the target domain, we further propose the semi-supervised HFA (SHFA) which can simultaneously learn the target classifier as well as infer the labels of unlabeled target samples. Comprehensive experiments on three different applications clearly demonstrate that our SHFA and HFA outperform the existing HDA methods.},
author = {Li, Wen and Duan, Lixin and Xu, Dong and Tsang, Ivor W},
doi = {3E685107-2977-47E4-B935-16F0A8864540},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {aug},
number = {6},
pages = {1134--1148},
pmid = {23999386},
title = {{Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23999386},
volume = {36},
year = {2013}
}
@article{Kalousis2001,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Hilario - 2001 - Model selection via meta-learning a comparative study.pdf:pdf},
journal = {International Journal on Artificial Intelligence Tools},
number = {4},
title = {{Model selection via meta-learning: a comparative study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000647},
volume = {10},
year = {2001}
}
@phdthesis{Druck2011,
author = {Druck, Gregory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck - 2011 - Generalized Expectation Criteria for Lightly Supervised Learning.pdf:pdf},
number = {September},
title = {{Generalized Expectation Criteria for Lightly Supervised Learning}},
url = {http://scholarworks.umass.edu/open{\_}access{\_}dissertations/440/},
year = {2011}
}
@article{Davies1995,
abstract = {This article attempts to provide a formal framework for a data based inference which explicitly and consistently recognizes the approximate nature of probability models. It is based on the idea that a stochastic model is adequate if samples generated under the model are very much like the sample actually obtained. The formalization is based on the concept of data feature. Examples are given of applying the ideas to different areas of statistics including location-scale models, densities, non-parametric regression, interlaboratory test, auto-regressive processes and the analysis of variance. The four cornerstones of the approach are direct comparison, approximation, weak topologies and parsimony. The approach is contrasted to that of much of conventional statistics many of whose concepts are pathologically discontinuous with respect to the topology of data analysis and common sense.},
author = {Davies, Pl},
doi = {10.1111/j.1467-9574.1995.tb01464.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Davies - 1995 - Data features '.pdf:pdf},
issn = {0039-0402},
journal = {Statistica Neerlandica},
keywords = {adequacy,continuity,data analysis,data features,inference,phrases,smooth functionals,topologies,weak and strong},
number = {2},
pages = {185--245},
title = {{Data features '}},
volume = {49},
year = {1995}
}
@article{Brown2015,
author = {Brown, Gavin},
doi = {10.1016/j.patrec.2015.04.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brown - 2015 - On Unifiers, Diversifiers, and the Nature of Pattern Recognition.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Nature of pattern recognition,Unifying,Diversifyin,nature of pattern recognition},
pages = {1--10},
publisher = {Elsevier Ltd.},
title = {{On Unifiers, Diversifiers, and the Nature of Pattern Recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001312},
volume = {000},
year = {2015}
}
@article{Ho2002,
author = {Ho, Tin Kam and Basu, Mitra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Basu - 2002 - Complexity Measures of Supervised Classification Problems.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity Measures of Supervised Classification Problems}},
volume = {24},
year = {2002}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baraniuk - 2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4286571 http://omni.isr.ist.utl.pt/{~}aguiar/CS{\_}notes.pdf},
year = {2007}
}
@article{Orhan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04155v1},
author = {Orhan, Cem},
eprint = {arXiv:1507.04155v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Orhan - 2015 - ALEVS Active Learning by Statistical Leverage Sampling.pdf:pdf},
keywords = {Active learning, Statistical leverage, Classificat},
title = {{ALEVS: Active Learning by Statistical Leverage Sampling}},
year = {2015}
}
@article{Cortes2014,
abstract = {We present a series of new theoretical, algorithmic, and empirical results for domain adaptation and sample bias correction in regression. We prove that the discrepancy is a distance for the squared loss when the hypothesis set is the reproducing kernel Hilbert space induced by a universal kernel such as the Gaussian kernel. We give new pointwise loss guarantees based on the discrepancy of the empirical source and target distributions for the general class of kernel-based regularization algorithms. These bounds have a simpler form than previous results and hold for a broader class of convex loss functions not necessarily differentiable, including Lq losses and the hinge loss. We also give finer bounds based on the discrepancy and a weighted feature discrepancy parameter. We extend the discrepancy minimization adaptation algorithm to the more significant case where kernels are used and show that the problem can be cast as an SDP similar to the one in the feature space. We also show that techniques from smooth optimization can be used to derive an efficient algorithm for solving such SDPs even for very high-dimensional feature spaces and large samples. We have implemented this algorithm and report the results of experiments both with artificial and real-world data sets demonstrating its benefits both for general scenario of adaptation and the more specific scenario of sample bias correction. Our results show that it can scale to large data sets of tens of thousands or more points and demonstrate its performance improvement benefits. ?? 2013 Elsevier B.V.},
author = {Cortes, Corinna and Mohri, Mehryar},
doi = {10.1016/j.tcs.2013.09.027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2014 - Domain adaptation and sample bias correction theory and algorithm for regression.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Domain adaptation,Learning theory,Machine learning,Optimization},
number = {June 2013},
pages = {103--126},
title = {{Domain adaptation and sample bias correction theory and algorithm for regression}},
url = {http://dx.doi.org/10.1016/j.tcs.2013.09.027},
volume = {519},
year = {2014}
}
@incollection{Loog2016a,
author = {Loog, Marco and Krijthe, Jesse Hendrik and Jensen, A C},
booktitle = {Handbook of Pattern Recognition and Computer Vision},
chapter = {1.3},
edition = {5},
editor = {Chen, C H},
publisher = {World Scientific},
title = {{On Measuring and Quantifying Performance: Error Rates, Surrogate Loss, and an Example in SSL}},
year = {2016}
}
@article{Tolstikhin2016,
abstract = {In this paper, we study the minimax estimation of the Bochner integral {\$}{\$}$\backslash$mu{\_}k(P):=$\backslash$int{\_}{\{}$\backslash$mathcal{\{}X{\}}{\}} k($\backslash$cdot,x)$\backslash$,dP(x),{\$}{\$} also called as the $\backslash$emph{\{}kernel mean embedding{\}}, based on random samples drawn i.i.d.{\~{}}from {\$}P{\$}, where {\$}k:\backslashmathcal{\{}X{\}}\backslashtimes\backslashmathcal{\{}X{\}}\backslashrightarrow\backslashmathbb{\{}R{\}}{\$} is a positive definite kernel. Various estimators (including the empirical estimator), {\$}\backslashhat{\{}\backslashtheta{\}}{\_}n{\$} of {\$}\backslashmu{\_}k(P){\$} are studied in the literature wherein all of them satisfy {\$}\backslashbigl\backslash| \backslashhat{\{}\backslashtheta{\}}{\_}n-\backslashmu{\_}k(P)\backslashbigr\backslash|{\_}{\{}\backslashmathcal{\{}H{\}}{\_}k{\}}=O{\_}P(n{\^{}}{\{}-1/2{\}}){\$} with {\$}\backslashmathcal{\{}H{\}}{\_}k{\$} being the reproducing kernel Hilbert space induced by {\$}k{\$}. The main contribution of the paper is in showing that the above mentioned rate of {\$}n{\^{}}{\{}-1/2{\}}{\$} is minimax in {\$}\backslash|\backslashcdot\backslash|{\_}{\{}\backslashmathcal{\{}H{\}}{\_}k{\}}{\$} and {\$}\backslash|\backslashcdot\backslash|{\_}{\{}L{\^{}}2(\backslashmathbb{\{}R{\}}{\^{}}d){\}}{\$}-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with {\$}k{\$} being a continuous translation-invariant kernel on {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$}. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of {\$}P{\$} (if it exists). This result has practical consequences in statistical applications as the mean embedding has been widely employed in non-parametric hypothesis testing, density estimation, causal inference and feature selection, through its relation to energy distance (and distance covariance).},
archivePrefix = {arXiv},
arxivId = {1602.04361},
author = {Tolstikhin, Ilya and Sriperumbudur, Bharath and Muandet, Krikamol},
eprint = {1602.04361},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Sriperumbudur, Muandet - 2016 - Minimax Estimation of Kernel Mean Embeddings.pdf:pdf},
keywords = {and phrases,bochner integral,empirical estimator,imax lower bounds,kernel mean embeddings,min-,nonparametric function estimation,reproducing kernel hilbert space},
number = {1},
pages = {1--44},
title = {{Minimax Estimation of Kernel Mean Embeddings}},
url = {http://arxiv.org/abs/1602.04361},
year = {2016}
}
@article{Dai2006,
author = {Dai, Yu-Hong},
doi = {10.1137/040613305},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dai - 2006 - Fast Algorithms for Projection on an Ellipsoid.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {040613305,1,10,1137,65k05,90c06,ams subject classifications,doi,ellipsoid,hybrid algorithm,introduction,large-scale,linear convergence,on a general convex,projection,set,the problem of projection},
month = {jan},
number = {4},
pages = {986--1006},
title = {{Fast Algorithms for Projection on an Ellipsoid}},
url = {http://epubs.siam.org/doi/abs/10.1137/040613305},
volume = {16},
year = {2006}
}
@inproceedings{Druck2010,
author = {Druck, Gregory and McCallum, Andrew Kachites},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck, McCallum - 2010 - High-performance semi-supervised learning using discriminatively constrained generative models.pdf:pdf},
pages = {319--326},
title = {{High-performance semi-supervised learning using discriminatively constrained generative models}},
url = {http://www.cs.umass.edu/{~}gdruck/pubs/druck10high.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}DruckM10.pdf},
year = {2010}
}
@article{Nishimura2015a,
abstract = {Hamiltonian Monte Carlo and related algorithms have become routinely used in Bayesian computation. The utility of such approaches is highlighted in the software package STAN, which provides a platform for automatic implementation of general Bayesian models. Hence, methods for improving the efficiency of general Hamiltonian Monte Carlo algorithms can have a substantial impact on practice. We propose such a method in this article by recycling the intermediate leap-frog steps used in approximating the Hamiltonian trajectories. Current algorithms use only the final step, and wastefully discard all the intermediate steps. We propose a simple and provably accurate approach for using these intermediate samples, boosting the effective sample size with little programming effort and essentially no extra computational cost. We show that our recycled Hamiltonian Monte Carlo algorithm can lead to substantial gains in computational efficiency in a variety of experiments.},
archivePrefix = {arXiv},
arxivId = {1511.06925},
author = {Nishimura, Akihiko and Dunson, David},
eprint = {1511.06925},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nishimura, Dunson - 2015 - Recycling intermediate steps to improve Hamiltonian Monte Carlo.pdf:pdf},
number = {1},
title = {{Recycling intermediate steps to improve Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1511.06925},
year = {2015}
}
@book{Gelman,
author = {Gelman, Andrew and Hill, Jennifer},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill - 2007 - Data Analysis Using Regression and MiltilevelHierarchical Models.pdf:pdf},
title = {{Data Analysis Using Regression and Miltilevel/Hierarchical Models}},
year = {2007}
}
@article{Shipp2001,
author = {Shipp, Catherine A. and Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shipp, Kuncheva - 2001 - Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling.pdf:pdf},
isbn = {0000000000000},
journal = {Proc. CIMA},
title = {{Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling}},
url = {http://www.bangor.ac.uk/{~}mas00a/papers/cslkAIDA01.pdf},
year = {2001}
}
@article{VanErven2015,
abstract = {The pursuit of fast rates in online and statistical learning has led to the conception of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that, under surprisingly weak conditions, both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov-Mammen margin condition, which has played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov, and Vovk's notion of mixability. Our unifying conditions thus provide a significant step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
archivePrefix = {arXiv},
arxivId = {1507.02592},
author = {van Erven, Tim and Gr{\"{u}}nwald, Peter D. and Mehta, Nishant a. and Reid, Mark D. and Williamson, Robert C.},
eprint = {1507.02592},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Erven et al. - 2015 - Fast rates in statistical and online learning.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/van Erven et al. - 2015 - Fast rates in statistical and online learning(2).pdf:pdf},
pages = {1793--1861},
title = {{Fast rates in statistical and online learning}},
url = {http://arxiv.org/abs/1507.02592},
volume = {2014},
year = {2015}
}
@unpublished{Chena,
archivePrefix = {arXiv},
arxivId = {arXiv:0000.0000},
author = {Chen, Aiyou and Owen, Art B. and Shi, Minghui},
eprint = {arXiv:0000.0000},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Owen, Shi - Unknown - Data Enriched Linear Regression.pdf:pdf},
pages = {1--37},
title = {{Data Enriched Linear Regression}}
}
@article{Loog2012a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2012 - Semi-supervised linear discriminant analysis using moment constraints.pdf:pdf},
journal = {Partially Supervised Learning, LNCS},
pages = {32--41},
title = {{Semi-supervised linear discriminant analysis using moment constraints}},
volume = {7081},
year = {2012}
}
@article{Adams,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01344v1},
author = {Adams, Ryan P},
eprint = {arXiv:1504.01344v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - Unknown - Early Stopping is Nonparametric Variational Inference.pdf:pdf},
title = {{Early Stopping is Nonparametric Variational Inference}}
}
@article{Bottou2012,
author = {Bottou, L{\'{e}}on},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@article{VanderKooi2013,
abstract = {OBJECTIVES: We investigated how much the Human Development Index (HDI), a global measure of development, modifies the effect of education on self-reported health.

METHODS: We analyzed cross-sectional World Health Survey data on 217,642 individuals from 49 countries, collected in 2002 to 2005, with random-intercept multilevel linear regression models.

RESULTS: We observed greater positive associations between educational levels and self-reported good health with increasing HDI. The magnitude of this effect modification of the education-health relation tended to increase with educational attainment. For example, before adjustment for effect modification, at comparable HDI, on average, finishing primary school was associated with better general health (b = 1.49; 95{\%} confidence interval [CI] = 1.18, 1.80). With adjustment for effect modification by HDI, the impact became 4.63 (95{\%} CI = 3.63, 5.62) for every 0.1 increase in HDI. Among those who completed high school, these associations were, respectively, 5.59 (95{\%} CI = 5.20, 5.98) and 9.95 (95{\%} CI = 8.89, 11.00).

CONCLUSIONS: The health benefits of educational attainment are greater in countries with greater human development. Health inequalities attributable to education are, therefore, larger in more developed countries.},
author = {van der Kooi, Anne L F and Stronks, Karien and Thompson, Caroline a and DerSarkissian, Maral and Arah, Onyebuchi a},
doi = {10.2105/AJPH.2013.301593},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Kooi et al. - 2013 - The modifying influence of country development on the effect of individual educational attainment on self-r.pdf:pdf},
issn = {1541-0048},
journal = {American journal of public health},
keywords = {Adult,Aged,Cross-Sectional Studies,Developed Countries,Developed Countries: statistics {\&} numerical data,Developing Countries,Developing Countries: statistics {\&} numerical data,Educational Status,Female,Health Status,Health Status Disparities,Health Surveys,Humans,Male,Middle Aged,Self Report,Young Adult},
month = {nov},
number = {11},
pages = {e49--54},
pmid = {24028233},
title = {{The modifying influence of country development on the effect of individual educational attainment on self-rated health.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24028233},
volume = {103},
year = {2013}
}
@phdthesis{Rifkin2002,
author = {Rifkin, Ryan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin - 2002 - Everything Old Is New Again A Fresh Look at Historical Approaches in Machine Learning.pdf:pdf},
title = {{Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning}},
year = {2002}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02531v1},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {arXiv:1503.02531v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hinton, Vinyals, Dean - Unknown - Distilling the Knowledge in a Neural Network.pdf:pdf},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}}
}
@article{Jain2016,
abstract = {The problem of developing binary classifiers from positive and unlabeled data is often encountered in machine learning. A common requirement in this setting is to approximate posterior probabilities of positive and negative classes for a previously unseen data point. This problem can be decomposed into two steps: (i) the development of accurate predictors that discriminate between positive and unlabeled data, and (ii) the accurate estimation of the prior probabilities of positive and negative examples. In this work we primarily focus on the latter subproblem. We study nonparametric class prior estimation and formulate this problem as an estimation of mixing proportions in two-component mixture models, given a sample from one of the components and another sample from the mixture itself. We show that estimation of mixing proportions is generally ill-defined and propose a canonical form to obtain identifiability while maintaining the flexibility to model any distribution. We use insights from this theory to elucidate the optimization surface of the class priors and propose an algorithm for estimating them. To address the problems of high-dimensional density estimation, we provide practical transformations to low-dimensional spaces that preserve class priors. Finally, we demonstrate the efficacy of our method on univariate and multivariate data.},
archivePrefix = {arXiv},
arxivId = {1601.01944},
author = {Jain, Shantanu and White, Martha and Trosset, Michael W. and Radivojac, Predrag},
eprint = {1601.01944},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain et al. - 2016 - Nonparametric semi-supervised learning of class proportions.pdf:pdf},
title = {{Nonparametric semi-supervised learning of class proportions}},
url = {http://arxiv.org/abs/1601.01944},
year = {2016}
}
@article{Keogh2005,
author = {Keogh, Eamonn and Lin, Jessica},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keogh, Lin - 2005 - Clustering of time-series subsequences is meaningless implications for previous and future research.pdf:pdf},
journal = {Knowledge and information systems},
number = {2},
pages = {154--177},
title = {{Clustering of time-series subsequences is meaningless: implications for previous and future research}},
url = {http://link.springer.com/article/10.1007/s10115-004-0172-7},
volume = {8},
year = {2005}
}
@article{Johnson,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01255v1},
author = {Johnson, Rie},
eprint = {arXiv:1504.01255v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - Unknown - Semi-Supervised Learning with Multi-View Embedding Theory and Application with Convolutional Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Semi-Supervised Learning with Multi-View Embedding : Theory and Application with Convolutional Neural Networks}}
}
@article{Gigerenzer2004,
abstract = {For instance, consider HIV screening for people who are in no known  group ( , 2002). In this population, the a priori probability p(H },
author = {Gigerenzer, Gerd and Krauss, Sv},
doi = {10.4135/9781412986311.n21},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gigerenzer, Krauss - 2004 - The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask.pdf:pdf},
isbn = {9780761923596},
journal = {The Sage Handbook of Methodology for the Social {\ldots}},
pages = {391--408},
title = {{The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Null+Ritual+What+You+Always+Wanted+to+Know+About+Signifi+cance+Testing+but+Were+Afraid+to+Ask{\#}3$\backslash$nhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Null+Ritual+What+You+Alway},
year = {2004}
}
@inproceedings{Duin1995,
author = {Duin, Robert P W},
booktitle = {Proceedings of the Scandinavian Conference on Image Analysis},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 1995 - Small sample size generalization.pdf:pdf},
keywords = {classification error,linear discriminants,small sample size},
pages = {957--964},
title = {{Small sample size generalization}},
year = {1995}
}
@inproceedings{Sindhwani2006,
address = {New York, New York, USA},
author = {Sindhwani, Vikas and Keerthi, S. S.},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Keerthi - 2006 - Large scale semi-supervised linear SVMs.pdf:pdf},
isbn = {1595933697},
keywords = {global optimiza-,support vector machines,text categorization,tion,unlabeled data},
pages = {477},
publisher = {ACM Press},
title = {{Large scale semi-supervised linear SVMs}},
year = {2006}
}
@article{Taskesen2016,
author = {Taskesen, Erdogan and Huisman, Sjoerd M H and Mahfouz, Ahmed and Krijthe, Jesse H and de Ridder, Jeroen and van de Stolpe, Anja and van den Akker, Erik and Verheagh, Wim and Reinders, Marcel J T},
doi = {10.1038/srep24949},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taskesen et al. - 2016 - Pan-cancer subtyping in a 2D-map shows substructures that are driven by specific combinations of molecular char.pdf:pdf},
journal = {Scientific Reports},
month = {apr},
pages = {24949},
publisher = {The Author(s)},
title = {{Pan-cancer subtyping in a 2D-map shows substructures that are driven by specific combinations of molecular characteristics}},
volume = {6},
year = {2016}
}
@article{Ajakan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.07818v1},
author = {Ajakan, Hana and Larochelle, Hugo and Marchand, Mario and Lempitsky, Victor},
eprint = {arXiv:1505.07818v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ajakan et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
journal = {arXiv},
title = {{Domain-Adversarial Training of Neural Networks}},
year = {2015}
}
@inproceedings{Shaffer1994,
author = {Schaffer, Cullen},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1994 - A conservation law for generalization performance.pdf:pdf},
title = {{A conservation law for generalization performance}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/LCG.pdf},
year = {1994}
}
@article{Wilson,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4245v2},
author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
eprint = {arXiv:1302.4245v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Adams - Unknown - Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation.pdf:pdf},
title = {{Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation}}
}
@article{Denis2015,
abstract = {Confident prediction is highly relevant in machine learning; for example, in applications such as medical diagnoses, wrong prediction can be fatal. For classification, there already exist procedures that allow to not classify data when the confidence in their prediction is weak. This approach is known as classification with reject option. In the present paper, we provide new methodology for this approach. Predicting a new instance via a confidence set, we ensure an exact control of the probability of classification. Moreover, we show that this methodology is easily implementable and entails attractive theoretical and numerical properties.},
archivePrefix = {arXiv},
arxivId = {1507.07235},
author = {Denis, Christophe and Hebiri, Mohamed},
eprint = {1507.07235},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Denis, Hebiri - 2015 - Consistency of plug-in confidence sets for classification in semi-supervised learning.pdf:pdf},
keywords = {classification,classification with reject option,confidence sets,conformal predictors,plug-in confidence sets},
pages = {1--26},
title = {{Consistency of plug-in confidence sets for classification in semi-supervised learning}},
url = {http://arxiv.org/abs/1507.07235},
year = {2015}
}
@article{Tian2016a,
author = {Tian, Ye and Luo, Jian},
doi = {10.1007/s00500-016-2089-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tian, Luo - 2016 - A new branch-and-bound approach to semi-supervised support vector machine.pdf:pdf},
isbn = {0050001620},
issn = {14337479},
journal = {Soft Computing},
keywords = {Branch-and-bound scheme,Lower bound estimator,MIQP reformulation,Soft margin {\$}{\$}$\backslash$mathrm {\{}S{\^{}}3VM{\}}{\$}{\$}S3VMmodel},
number = {2001},
pages = {1--10},
publisher = {Springer Berlin Heidelberg},
title = {{A new branch-and-bound approach to semi-supervised support vector machine}},
url = {"http://dx.doi.org/10.1007/s00500-016-2089-y},
year = {2016}
}
@article{Hamsici2008,
abstract = {We present an algorithm which provides the one-dimensional subspace where the Bayes error is minimized for the C class problem with homoscedastic Gaussian distributions. Our main result shows that the set of possible one-dimensional spaces v, for which the order of the projected class means is identical, defines a convex region with associated convex Bayes error function g(v). This allows for the minimization of the error function using standard convex optimization algorithms. Our algorithm is then extended to the minimization of the Bayes error in the more general case of heteroscedastic distributions. This is done by means of an appropriate kernel mapping function. This result is further extended to obtain the d-dimensional solution for any given d, by iteratively applying our algorithm to the null space of the (d - 1)-dimensional solution. We also show how this result can be used to improve up on the outcomes provided by existing algorithms, and derive a low-computational cost, linear approximation. Extensive experimental validations are provided to demonstrate the use of these algorithms in classification, data analysis and visualization.},
author = {Hamsici, Onur C and Martinez, Aleix M},
doi = {10.1109/TPAMI.2007.70717},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamsici, Martinez - 2008 - Bayes optimality in linear discriminant analysis.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Computer Simulation,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Linear Models,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {apr},
number = {4},
pages = {647--57},
pmid = {18276970},
title = {{Bayes optimality in linear discriminant analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18276970},
volume = {30},
year = {2008}
}
@article{Leisch2004,
author = {Leisch, F},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leisch - 2004 - {\{}FlexMix{\}} A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}.pdf:pdf},
keywords = {finite mixture models,latent class regression,model based clustering,r},
number = {8},
pages = {1--18},
title = {{{\{}FlexMix{\}}: A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}}},
volume = {11},
year = {2004}
}
@article{Goel2003,
author = {Goel, Prem K. and Ginebra, Josep},
doi = {10.1046/j.1467-9884.2003.00376.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goel, Ginebra - 2003 - When is one experiment 'always better than' another.pdf:pdf},
issn = {00390526},
journal = {Journal of the Royal Statistical Society Series D: The Statistician},
keywords = {Comparison of experiments,Location experiments,Loewner ordering,Optimal design,Statistical information,Stochastic ordering,Sufficiency},
number = {4},
pages = {515--537},
title = {{When is one experiment 'always better than' another?}},
volume = {52},
year = {2003}
}
@inproceedings{Lafferty2007,
author = {Lafferty, John D. and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems 20},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lafferty, Wasserman - 2007 - Statistical analysis of semi-supervised regression.pdf:pdf},
pages = {801----808},
title = {{Statistical analysis of semi-supervised regression}},
year = {2007}
}
@inproceedings{Wager2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.1493v2},
author = {Wager, Stefan and Wang, Sida and Liang, Percy},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1307.1493v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wager, Wang, Liang - 2013 - Dropout training as adaptive regularization.pdf:pdf},
pages = {351--359},
title = {{Dropout training as adaptive regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
year = {2013}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learn-ing a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a com-pression cost, known as the variational free en-ergy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforce-ment learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05424v2},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {arXiv:1505.05424v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
title = {{Weight Uncertainty in Neural Networks}},
volume = {37},
year = {2015}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrd et al. - 1995 - A limited memory algorithm for bound constrained optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
number = {5},
pages = {1190--1208},
title = {{A limited memory algorithm for bound constrained optimization}},
volume = {16},
year = {1995}
}
@article{Oneto2015a,
author = {Oneto, Luca and Ridella, Sandro and Anguita, Davide},
doi = {10.1007/s10994-015-5540-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Oneto, Ridella, Anguita - 2015 - Tikhonov, Ivanov and Morozov regularization for support vector machine learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Ivanov regularization,Morozov regularization,Structural risk minimization,Support vector machine,Tikhonov regularization,ivanov regularization,morozov regularization,structural risk minimization,support vector machine,tikhonov regularization},
publisher = {Springer US},
title = {{Tikhonov, Ivanov and Morozov regularization for support vector machine learning}},
url = {http://link.springer.com/10.1007/s10994-015-5540-x},
year = {2015}
}
@article{Shafer2008,
author = {Shafer, Glenn and Vovk, Vladimir},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shafer, Vovk - 2008 - A tutorial on conformal prediction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {confidence,on-line compression modeling,on-line learning,prediction regions},
pages = {371--421},
title = {{A tutorial on conformal prediction}},
url = {http://dl.acm.org/citation.cfm?id=1390693},
volume = {9},
year = {2008}
}
@article{Tomar2016,
archivePrefix = {arXiv},
arxivId = {1606.05925},
author = {Tomar, Vikrant Singh and Rose, Richard},
eprint = {1606.05925},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tomar, Rose - 2016 - Manifold regularized deep neural networks for automatic speech recognition.pdf:pdf},
journal = {Icassp},
keywords = {ifold regularized deep neural,networks for automatic speech},
title = {{Manifold regularized deep neural networks for automatic speech recognition}},
year = {2016}
}
@inproceedings{Joulin2012,
author = {Joulin, Armand and Bach, Francis},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joulin, Bach - 2012 - A convex relaxation for weakly supervised classifiers.pdf:pdf},
keywords = {MIL,convex relaxation,weak supervision},
pages = {1279--1286},
title = {{A convex relaxation for weakly supervised classifiers}},
url = {http://arxiv.org/abs/1206.6413},
year = {2012}
}
@article{Fan2014,
author = {Fan, Jianqing and Ke, Zheng Tracy},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Ke - 2014 - Discussion â€œa significance test for the lassoâ€.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {483--492},
title = {{Discussion: â€œa significance test for the lassoâ€}},
volume = {42},
year = {2014}
}
@article{Meier2008,
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1111/j.1467-9868.2007.00627.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meier, Van De Geer, B{\"{u}}hlmann - 2008 - The group lasso for logistic regression.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {categorical data,co-ordinate descent algorithm,dna splice site,group variable,high dimensional generalized linear,model,penalized likelihood,selection},
month = {jan},
number = {1},
pages = {53--71},
title = {{The group lasso for logistic regression}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
volume = {70},
year = {2008}
}
@article{Mirowski2008,
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
doi = {10.1109/MLSP.2008.4685487},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mirowski et al. - 2008 - Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG.pdf:pdf},
isbn = {978-1-4244-2375-0},
journal = {2008 IEEE Workshop on Machine Learning for Signal Processing},
month = {oct},
pages = {244--249},
publisher = {Ieee},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4685487},
year = {2008}
}
@article{Kakade2008,
abstract = {This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliï¬ed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniï¬ed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2.},
author = {Kakade, Sm and Sridharan, Karthik and Tewari, Ambuj},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kakade, Sridharan, Tewari - 2008 - On the Complexity of Linear Prediction risk bounds, margin bounds, and regularization.pdf:pdf},
isbn = {9781605609492},
journal = {Nips},
pages = {1--11},
title = {{On the Complexity of Linear Prediction : risk bounds, margin bounds, and regularization}},
url = {https://papers.nips.cc/paper/3510-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization.pdf},
year = {2008}
}
@inproceedings{Lawrence2004,
author = {Lawrence, Neil D. and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lawrence, Jordan - 2004 - Semi-supervised learning via Gaussian processes.pdf:pdf},
pages = {753--760},
title = {{Semi-supervised learning via Gaussian processes}},
year = {2004}
}
@article{Mann2007,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2007 - Efficient computation of entropy gradient for semi-supervised conditional random fields.pdf:pdf},
journal = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Efficient computation of entropy gradient for semi-supervised conditional random fields}},
url = {http://dl.acm.org/citation.cfm?id=1614136},
year = {2007}
}
@article{Gretton,
author = {Gretton, Arthur and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gretton, Sch{\"{o}}lkopf - Unknown - A Kernel Method for the Two-Sample-Problem.pdf:pdf},
title = {{A Kernel Method for the Two-Sample-Problem}}
}
@inproceedings{Huang2006,
author = {Huang, Jiayuan and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2006 - Correcting sample selection bias by unlabeled data.pdf:pdf},
pages = {601--608},
title = {{Correcting sample selection bias by unlabeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2006{\_}915.pdf},
year = {2006}
}
@inproceedings{Suzuki2008,
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {ACL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Isozaki - 2008 - Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.pdf:pdf},
pages = {665--673},
title = {{Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.5597{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@article{Maaten2014,
author = {Maaten, Laurens Van Der},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maaten - 2014 - Accelerating t-SNE using Tree-Based Algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {barnes-hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-sne},
pages = {3221âˆ’3245},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
volume = {15},
year = {2014}
}
@article{Xue2015,
author = {Xue, Jing-hao and Hall, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xue, Hall - 2015 - Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {5},
pages = {1109--1112},
title = {{Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis?}},
volume = {37},
year = {2015}
}
@article{Wolpert1996a,
abstract = {This is the first of two papers that use off-training set {\{}(OTS){\}} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no {\{}$\backslash$textbackslash{\}}textita priori distinctions between learning algorithms. {\{}(The{\}} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are â€œas manyâ€ targets (or priors over targets) for which A has lower expected {\{}OTS{\}} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is â€œanti-cross-validationâ€ (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the {\{}Vapnik-Chervonenkis{\}} dimension of your generalizer is small; and the training set is large, then with high probability your {\{}OTS{\}} error is small. Other implications for â€œmembership queriesâ€ algorithms and â€œpuntingâ€ algorithms are also discussed.},
author = {Wolpert, David H},
doi = {10.1162/neco.1996.8.7.1341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 1996 - The Lack of A Priori Distinctions Between Learning Algorithms.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {7},
pages = {1341--1390},
title = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341},
volume = {8},
year = {1996}
}
@article{Kawano2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1108.5244v3},
author = {Kawano, Shuichi},
eprint = {arXiv:1108.5244v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawano - 2012 - Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,covariate shift,em algorithm,model selection,reg-,semi-supervised learning,ularization},
pages = {1--19},
title = {{Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions}},
year = {2012}
}
@inproceedings{Niu2013,
author = {Niu, G and Jitkrittum, W and Dai, Bo and Hachiya, H and Sugiyama, M},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2013 - Squared-loss Mutual Information Regularization A Novel Information-theoretic Approach to Semi-supervised Learning.pdf:pdf},
pages = {10--18},
title = {{Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}gang/paper/niu{\_}icml13.pdf},
year = {2013}
}
@article{Bertsekas1999,
author = {Bertsekas, Dimitri P},
publisher = {Athena scientific},
title = {{Nonlinear programming}},
year = {1999}
}
@inproceedings{Bernad2004,
author = {Bernad{\'{o}}-Mansilla, Ester and Ho, Tin Kam},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bernad{\'{o}}-Mansilla, Ho - 2004 - On classifier domains of competence.pdf:pdf},
title = {{On classifier domains of competence}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1334026},
year = {2004}
}
@inproceedings{Ghahramani1994,
author = {Ghahramani, Zoubin and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani, Jordan - 1994 - Supervised Learning from incomplete data via an EM approach.pdf:pdf},
title = {{Supervised Learning from incomplete data via an EM approach}},
year = {1994}
}
@article{Todorovski2003,
author = {Todorovski, Ljupco and D{\v{z}}eroski, Saso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Todorovski, D{\v{z}}eroski - 2003 - Combining classifiers with meta decision trees.pdf:pdf},
journal = {Machine learning},
keywords = {combining classifiers,decision trees,ensembles of classifiers,meta-level learning,stacking},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
url = {http://link.springer.com/article/10.1023/A:1021709817809},
year = {2003}
}
@article{Greenland1985,
abstract = {The authors examine some recently proposed criteria for determining when to adjust for covariates related to misclassification, and show these criteria to be incorrect. In particular, they show that when misclassification is present, covariate control can sometimes increase net bias, even when the covariate would have been a confounder under perfect classification, and even if the covariate is a determinant of classification. Thus, bias due to misclassification cannot be adequately dealt with by the methods used for control of confounding. The examples presented also show that the "change-in-estimate" criterion for deciding whether to control a covariate can be systematically misleading when misclassification is present. These results demonstrate that it is necessary to consider the degree of misclassification when deciding whether to control a covariate.},
author = {Greenland, S and Robins, J M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Greenland, Robins - 1985 - Confounding and misclassification.pdf:pdf},
issn = {0002-9262},
journal = {American journal of epidemiology},
number = {3},
pages = {495--506},
pmid = {4025298},
title = {{Confounding and misclassification.}},
volume = {122},
year = {1985}
}
@inproceedings{Pranckeviciene2006,
author = {Pranckeviciene, Erinija and Ho, Tin Kam and Somorjai, Ray},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pranckeviciene, Ho, Somorjai - 2006 - Class separability in spaces reduced by feature selection.pdf:pdf},
title = {{Class separability in spaces reduced by feature selection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1699514},
year = {2006}
}
@article{Krause2014,
abstract = {Submodularity1 is a property of set functions with deep theoretical consequences and farâ€“ reaching applications. At first glance it appears very similar to concavity, in other ways it resembles convexity. It appears in a wide variety of applications: in Computer Science it ... $\backslash$n},
author = {Krause, Andreas and Golovin, Daniel},
doi = {10.1017/CBO9781139177801.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krause, Golovin - 2014 - Submodular function maximization.pdf:pdf},
isbn = {9781139177801},
issn = {{\textless}null{\textgreater}},
journal = {Tractability: Practical Approaches to Hard Problems},
pages = {71--104},
title = {{Submodular function maximization}},
volume = {3},
year = {2014}
}
@inproceedings{Loog2012,
author = {Loog, Marco and Duin, Robert P W},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Duin - 2012 - The dipping phenomenon.pdf:pdf},
pages = {310--317},
title = {{The dipping phenomenon}},
year = {2012}
}
@article{Schuurmans2002,
author = {Schuurmans, Dale and Southey, Finnegan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schuurmans, Southey - 2002 - Metric-Based Methods for Adaptive Model Selection and Regularization.pdf:pdf},
journal = {Machine Learning},
keywords = {model selection,regularization,unlabeled examples},
pages = {51--84},
title = {{Metric-Based Methods for Adaptive Model Selection and Regularization}},
volume = {48},
year = {2002}
}
@article{Herndon2014,
author = {Herndon, Thomas and Ash, Michael and Pollin, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herndon, Ash, Pollin - 2014 - Does High Public Debt Consistently Stifle Economic Growth A Critique of Reinhart and Rogoff.pdf:pdf},
journal = {Cambridge Journal of Economics},
number = {2},
pages = {257--279},
title = {{Does High Public Debt Consistently Stifle Economic Growth ? A Critique of Reinhart and Rogoff}},
volume = {38},
year = {2014}
}
@article{Cai2014,
author = {Cai, T. Tony and Yuan, Ming},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, Yuan - 2014 - Discussion â€œa significance test for the lassoâ€.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {478--482},
title = {{Discussion: â€œa significance test for the lassoâ€}},
volume = {42},
year = {2014}
}
@inproceedings{Sa1994,
author = {Sa, Virginia R De},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sa - 1994 - Learning Classification with Unlabeled Data.pdf:pdf},
pages = {112--112},
title = {{Learning Classification with Unlabeled Data}},
year = {1994}
}
@article{Grunwald2004,
abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss.  Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other.  Although Topsoe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. 

We here generalize this theory to apply to arbitrary decision problems and loss functions.  We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context.  This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts.  We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions.  This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the "redundancy-capacity theorem'' of information theory).  For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions.},
archivePrefix = {arXiv},
arxivId = {math/0410076},
author = {Gr{\"{u}}nwald, Peter D. and {Philip Dawid}, a.},
doi = {10.1214/009053604000000553},
eprint = {0410076},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Philip Dawid - 2004 - Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Additive model,Bayes act,Bregman divergence,Brier score,Convexity,Duality,Equalizer rule,Exponential family,Gamma-minimax,Generalized exponential family,Kullback-Leibler divergence,Logarithmic score,Maximin,Mean-value constraints,Minimax},
number = {4},
pages = {1367--1433},
primaryClass = {math},
title = {{Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory}},
volume = {32},
year = {2004}
}
@inproceedings{Druck2007,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew Kachites and Zhu, Xiaojin},
booktitle = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck et al. - 2007 - Semi-supervised classification with hybrid generativediscriminative methods.pdf:pdf},
isbn = {9781595936097},
keywords = {discriminative,hybrid generative,methods,semi-supervised learning,text classification},
pages = {280--289},
title = {{Semi-supervised classification with hybrid generative/discriminative methods}},
url = {http://dl.acm.org/citation.cfm?id=1281225},
year = {2007}
}
@article{Blanchard2010,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blanchard, Lee, Scott - 2010 - Semi-supervised novelty detection.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2973--3009},
title = {{Semi-supervised novelty detection}},
url = {http://dl.acm.org/citation.cfm?id=1953028},
volume = {11},
year = {2010}
}
@article{Gal,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02142v1},
author = {Gal, Yarin},
eprint = {arXiv:1506.02142v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gal - Unknown - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
pages = {1--10},
title = {{Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning}}
}
