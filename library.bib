Automatically generated by Mendeley Desktop 1.16
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Bertsekas1999,
author = {Bertsekas, Dimitri P},
publisher = {Athena scientific},
title = {{Nonlinear programming}},
year = {1999}
}
@article{Isaksson2008,
author = {Isaksson, Anders and Wallman, M. and Goransson, H. and Gustafsson, Mats G},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
month = {oct},
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Kleinberg2000,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 2000 - On the algorithmic implementation of stochastic discrimination.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {473--490},
publisher = {IEEE},
title = {{On the algorithmic implementation of stochastic discrimination}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=857004},
volume = {22},
year = {2000}
}
@article{Lehmann1993,
author = {Lehmann, EL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lehmann - 1993 - The Fisher, Neyman-Pearson theories of testing hypotheses One theory or two.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson theories of testing hypotheses: One theory or two?}},
url = {http://www.jstor.org/stable/10.2307/2291263},
volume = {88},
year = {1993}
}
@inproceedings{Pranckeviciene2006,
author = {Pranckeviciene, Erinija and Ho, Tin Kam and Somorjai, Ray},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pranckeviciene, Ho, Somorjai - 2006 - Class separability in spaces reduced by feature selection.pdf:pdf},
title = {{Class separability in spaces reduced by feature selection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1699514},
year = {2006}
}
@inproceedings{Giraud-Carrier2005,
author = {Giraud-carrier, Christophe and Provost, Foster},
booktitle = {In Proceedings of the ICML-2005 Workshop on Meta-learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Provost - 2005 - Toward a justification of meta-learning Is the no free lunch theorem a show-stopper.pdf:pdf},
pages = {12--19},
title = {{Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper}},
url = {http://dml.cs.byu.edu/{~}cgc/pubs/ICML2005WS.pdf},
year = {2005}
}
@inproceedings{Hernandez-reyes2005,
author = {Hern{\'{a}}ndez-reyes, Edith and Carrasco-Ochoa, J.A. and Mart{\'{i}}nez-trinidad, J Fco},
booktitle = {Proceedings of the 10th Iberoamerican Congress on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-reyes, Carrasco-Ochoa, Mart{\'{i}}nez-trinidad - 2005 - Classifier Selection Based on Data Complexity Measures.pdf:pdf},
pages = {586--592},
title = {{Classifier Selection Based on Data Complexity Measures}},
year = {2005}
}
@inproceedings{Ho2008,
author = {Ho, Tin Kam},
booktitle = {Proceedings of the 2008 Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89689-0{\_}102},
year = {2008}
}
@inproceedings{Walt2007,
author = {Walt, Christiaan Van Der and Barnard, Etienne},
booktitle = {18th Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Walt, Barnard - 2007 - Measures for the characterisation of pattern-recognition data sets.pdf:pdf},
title = {{Measures for the characterisation of pattern-recognition data sets}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/1979},
year = {2007}
}
@book{Brazdil2010,
author = {Brazdil, Pavel B. and Bernstein, Abraham},
editor = {Brazdil, Pavel and Bernstein, Abraham},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Bernstein - 2010 - Proceedings of 3rd Planning to Learn Workshop at ECAI 2010.pdf:pdf},
number = {Ecai},
title = {{Proceedings of 3rd Planning to Learn Workshop at ECAI 2010}},
year = {2010}
}
@article{Sotoca2006,
author = {Sotoca, J M and Mollineda, R A and Sanchez, J.S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sotoca, Mollineda, Sanchez - 2006 - A meta-learning framework for pattern classification by means of data complexity measures.pdf:pdf},
journal = {Inteligencia artificial: Revista Iberoamericana de Inteligencia Artificial},
keywords = {classification,data complexity,feature selection,meta-learning,prototype selection},
number = {29},
pages = {31--38},
title = {{A meta-learning framework for pattern classification by means of data complexity measures}},
volume = {10},
year = {2006}
}
@article{Sohn1999,
author = {Sohn, So Young},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sohn - 1999 - Meta Analysis of Classification Algorithms for Pattern Recognition.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1137--1144},
title = {{Meta Analysis of Classification Algorithms for Pattern Recognition}},
volume = {21},
year = {1999}
}
@article{Bengio2007,
author = {Bengio, Yoshua and LeCun, Yann},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, LeCun - 2007 - Scaling Learning Algorithms towards AI.pdf:pdf},
journal = {Large-Scale Kernel Machines},
number = {1},
pages = {1--41},
title = {{Scaling Learning Algorithms towards AI}},
url = {http://www.iro.umontreal.ca/{~}lisa/bib/pub{\_}subject/language/pointeurs/bengio+lecun-chapter2007.pdf},
year = {2007}
}
@article{Bengio2010,
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
journal = {Computational Intelligence},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.2010.00366.x/full},
volume = {26},
year = {2010}
}
@article{Rendell1990,
author = {Rendell, Larry and Cho, Howard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rendell, Cho - 1990 - Empirical learning as a function of concept character.pdf:pdf},
journal = {Machine Learning},
keywords = {concepts as functions,empirical concept learning,experimental studies},
pages = {267--298},
title = {{Empirical learning as a function of concept character}},
url = {http://www.springerlink.com/index/K5311727465WLH07.pdf},
volume = {5},
year = {1990}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brodley - 1995 - Recursive automatic bias selection for classifier construction.pdf:pdf},
journal = {Machine Learning},
keywords = {automatic algorithm selection,decision trees,hybrid classifiers,inductive bias,learning from},
pages = {63--94},
title = {{Recursive automatic bias selection for classifier construction}},
url = {http://link.springer.com/article/10.1023/A:1022686102325},
volume = {94},
year = {1995}
}
@article{Bengio2009,
author = {Bengio, Yoshua},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
journal = {Foundations and trends{\textregistered} in Machine Learning},
title = {{Learning deep architectures for AI}},
url = {http://dl.acm.org/citation.cfm?id=1658424},
year = {2009}
}
@inproceedings{Furnkranz2001,
author = {F{\"{u}}rnkranz, Johannes and Petrak, Johann},
booktitle = {Proceedings of the ECML/PKDD Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/F{\"{u}}rnkranz, Petrak - 2001 - An Evaluation of Landmarking Variants.pdf:pdf},
pages = {57--68},
title = {{An Evaluation of Landmarking Variants}},
url = {http://ai.ijs.si/branax/iddm-2001-proceedings/paper9.pdf},
year = {2001}
}
@techreport{Bensusan2000,
author = {Bensusan, H. and Giraud-carrier, Christophe and Kennedy, C.J.},
booktitle = {ILP Work-in-progress {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bensusan, Giraud-carrier, Kennedy - 2000 - A Higher-order Approach to Meta-learning.pdf:pdf},
institution = {University of Bristol},
title = {{A Higher-order Approach to Meta-learning}},
url = {http://137.222.102.8/Publications/Papers/1000471.pdf},
year = {2000}
}
@article{Brazdil2003a,
author = {Brazdil, Pavel B. and Soares, Carlos and Costa, JP Da},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Soares, Costa - 2003 - Ranking learning algorithms Using IBL and meta-learning on accuracy and time results.pdf:pdf},
journal = {Machine Learning},
keywords = {algorithm recommendation,data characterization,meta-learning,ranking},
pages = {251--277},
title = {{Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results}},
url = {http://link.springer.com/article/10.1023/A:1021713901879},
volume = {50},
year = {2003}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andre},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
edition = {Second},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {0387848576},
publisher = {Spinger},
title = {{The Elements of Statistical Learning}},
year = {2009}
}
@inproceedings{Steck2003,
author = {Steck, Harald and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steck, Jaakkola - 2003 - Bias-corrected bootstrap and model uncertainty.pdf:pdf},
title = {{Bias-corrected bootstrap and model uncertainty}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA66.pdf},
year = {2003}
}
@article{Shipp2001,
author = {Shipp, Catherine A. and Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shipp, Kuncheva - 2001 - Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling.pdf:pdf},
isbn = {0000000000000},
journal = {Proc. CIMA},
title = {{Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling}},
url = {http://www.bangor.ac.uk/{~}mas00a/papers/cslkAIDA01.pdf},
year = {2001}
}
@inproceedings{Loog2010,
author = {Loog, Marco},
booktitle = {Proceedings of the 2010 European Conference on Machine learning and Knowledge Discovery in Databases},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2010 - Constrained Parameter Estimation for Semi-Supervised Learning The Case of the Nearest Mean Classifier.pdf:pdf},
pages = {291--304},
title = {{Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier}},
year = {2010}
}
@inproceedings{Sokolovska2008,
address = {Helsinki, Finland},
author = {Sokolovska, Nataliya and Capp{\'{e}}, Olivier and Yvon, Francois},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska, Capp{\'{e}}, Yvon - 2008 - The asymptotics of semi-supervised learning in discriminative probabilistic models.pdf:pdf},
pages = {984--991},
publisher = {ACM Press},
title = {{The asymptotics of semi-supervised learning in discriminative probabilistic models}},
year = {2008}
}
@article{Amini2002,
author = {Amini, Massih-Reza and Gallinari, Patrick},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amini, Gallinari - 2002 - Semi-supervised logistic regression.pdf:pdf},
journal = {15th European Conference on Artificial Intelligence},
pages = {390--394},
title = {{Semi-supervised logistic regression}},
year = {2002}
}
@inproceedings{Fujino2005,
author = {Fujino, Akinori and Ueda, Naonori and Saito, Kazumi},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fujino, Ueda, Saito - 2005 - A hybrid generativediscriminative approach to semi-supervised classifier design.pdf:pdf},
number = {2},
pages = {764--769},
title = {{A hybrid generative/discriminative approach to semi-supervised classifier design}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-120.pdf},
volume = {20},
year = {2005}
}
@inproceedings{Druck2007,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew Kachites and Zhu, Xiaojin},
booktitle = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck et al. - 2007 - Semi-supervised classification with hybrid generativediscriminative methods.pdf:pdf},
isbn = {9781595936097},
keywords = {discriminative,hybrid generative,methods,semi-supervised learning,text classification},
pages = {280--289},
title = {{Semi-supervised classification with hybrid generative/discriminative methods}},
url = {http://dl.acm.org/citation.cfm?id=1281225},
year = {2007}
}
@article{Balcan2010,
author = {Balcan, Maria-Florina and Blum, Avrim},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Blum - 2010 - A Discriminative Model for Semi-Supervised Learning.pdf:pdf},
journal = {Journal of the ACM (JACM)},
number = {3},
title = {{A Discriminative Model for Semi-Supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1706599},
volume = {57},
year = {2010}
}
@article{Breiman1996,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 1996 - Bagging predictors.pdf:pdf},
journal = {Machine learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging predictors}},
url = {http://www.springerlink.com/index/L4780124W2874025.pdf},
volume = {140},
year = {1996}
}
@inproceedings{Bernad2004,
author = {Bernad{\'{o}}-Mansilla, Ester and Ho, Tin Kam},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bernad{\'{o}}-Mansilla, Ho - 2004 - On classifier domains of competence.pdf:pdf},
title = {{On classifier domains of competence}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1334026},
year = {2004}
}
@article{Ho2002a,
author = {Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2002 - A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors.pdf:pdf},
journal = {Pattern Analysis and Applications},
number = {2},
pages = {102--112},
title = {{A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors}},
volume = {5},
year = {2002}
}
@inproceedings{Joachims1999,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 16th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 1999 - Transductive inference for text classification using support vector machines.pdf:pdf},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers},
title = {{Transductive inference for text classification using support vector machines}},
year = {1999}
}
@inproceedings{Ho2001a,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2001 - Data Complexity Analysis for Classifier Combination.pdf:pdf},
pages = {53--67},
title = {{Data Complexity Analysis for Classifier Combination}},
year = {2001}
}
@article{Kasabov2003,
author = {Kasabov, Nikola and Pang, Shaoning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kasabov, Pang - 2003 - Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition.pdf:pdf},
journal = {Proceedings of the International Conference on Neural networks and signal processing},
keywords = {inductive svm,motif,promoter,promoter recognition,transductive svm},
number = {2},
pages = {31--38},
title = {{Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition}},
volume = {3},
year = {2003}
}
@techreport{Wolpert1996,
author = {Wolpert, David H and Macready, W},
booktitle = {Santa Fe Institute Technical Report},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert, Macready - 1996 - Combining Stacking With Bagging To Improve A Learning Algorithm.pdf:pdf},
pages = {1--28},
title = {{Combining Stacking With Bagging To Improve A Learning Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9933{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@article{Skurichina2000,
author = {Skurichina, Marina and Duin, Robert P.W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 2000 - The role of combining rules in bagging and boosting.pdf:pdf},
journal = {Advances in Pattern Recognition},
pages = {631--640},
title = {{The role of combining rules in bagging and boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-44522-6{\_}65},
year = {2000}
}
@article{Buhlmann2002,
author = {Buhlmann, Peter and Yu, Bin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Yu - 2002 - Analyzing bagging.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {927--961},
title = {{Analyzing bagging}},
volume = {30},
year = {2002}
}
@inproceedings{Skurichina2002,
author = {Skurichina, Marina and Kuncheva, Ludmila I and Duin, Robert P.W.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Kuncheva, Duin - 2002 - Bagging and Boosting for the Nearest Mean Classifier Effects of Sample Size on Diversity and Accura.pdf:pdf},
pages = {62--71},
publisher = {Springer},
title = {{Bagging and Boosting for the Nearest Mean Classifier : Effects of Sample Size on Diversity and Accuracy}},
year = {2002}
}
@inproceedings{Zhang2000,
author = {Zhang, Tong},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2000 - The value of unlabeled data for classification problems.pdf:pdf},
pages = {1191--1198},
title = {{The value of unlabeled data for classification problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Bennett1998,
author = {Bennett, Kristin P. and Demiriz, Ayhan},
booktitle = {Advances in Neural Information Processing Systems 11},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bennett, Demiriz - 1998 - Semi-supervised support vector machines.pdf:pdf},
pages = {368--374},
title = {{Semi-supervised support vector machines}},
year = {1998}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu - 2005 - Semi-supervised learning literature survey.pdf:pdf},
institution = {University of Wisconsin - Madison},
pages = {1--59},
title = {{Semi-supervised learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@inproceedings{Kuncheva2001,
author = {Kuncheva, Ludmila I and Roli, Fabio and Marcialis, Gian Luca and Shipp, Catherine A.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2001 - Complexity of Data Subsets Generated by the Random Subspace Method An Experimental Investigation.pdf:pdf},
pages = {349--358},
title = {{Complexity of Data Subsets Generated by the Random Subspace Method: An Experimental Investigation}},
year = {2001}
}
@misc{Mitchell1980,
author = {Mitchell, Tom M.},
booktitle = {Psychology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mitchell - 1980 - The need for biases in learning generalizations.pdf:pdf},
title = {{The need for biases in learning generalizations}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/Need for Bias.pdf},
year = {1980}
}
@article{Wang2007a,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2007 - On Transductive Support Vector Machines.pdf:pdf},
journal = {Contemporary Mathematics},
pages = {7--19},
title = {{On Transductive Support Vector Machines}},
volume = {443},
year = {2007}
}
@article{Loog2012a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2012 - Semi-supervised linear discriminant analysis using moment constraints.pdf:pdf},
journal = {Partially Supervised Learning, LNCS},
pages = {32--41},
title = {{Semi-supervised linear discriminant analysis using moment constraints}},
volume = {7081},
year = {2012}
}
@inproceedings{Ho2000,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2000 - Complexity of Classification Problems and Comparative Advantages of Combined Classifiers.pdf:pdf},
pages = {97--106},
title = {{Complexity of Classification Problems and Comparative Advantages of Combined Classifiers}},
year = {2000}
}
@inproceedings{Tax2005,
author = {Tax, David M.J. and Duin, Robert P.W.},
booktitle = {Proceedings of the Sixteenth Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tax, Duin - 2005 - Characterizing one-class datasets.pdf:pdf},
number = {4},
pages = {21--26},
title = {{Characterizing one-class datasets}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/TaxDui2005.pdf},
volume = {1},
year = {2005}
}
@article{Kuncheva2002,
author = {Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2002 - A Theoretical Study on Six Classifier Fusion Strategies.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {281--286},
title = {{A Theoretical Study on Six Classifier Fusion Strategies}},
volume = {24},
year = {2002}
}
@inproceedings{Roli2002,
author = {Roli, Fabio and Raudys, {\v{S}}arÅ«nas and Marcialis, Gian Luca},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roli, Raudys, Marcialis - 2002 - An experimental comparison of fixed and trained fusion rules for crisp classifier outputs.pdf:pdf},
title = {{An experimental comparison of fixed and trained fusion rules for crisp classifier outputs}},
url = {http://link.springer.com/chapter/10.1007/3-540-45428-4{\_}23},
year = {2002}
}
@article{Fumera,
author = {Fumera, Giorgio and Roli, Fabio and Serrau, Alessandra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fumera, Roli, Serrau - Unknown - A Theoretical Analysis of Bagging as a Linear Combination of Classifiers.pdf:pdf},
pages = {1--19},
title = {{A Theoretical Analysis of Bagging as a Linear Combination of Classifiers}}
}
@article{Kuncheva2003,
author = {Kuncheva, Ludmila I and Whitaker, Christopher J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva, Whitaker - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
journal = {Machine Learning},
keywords = {committee of learners,dependency and diversity,multiple classifiers ensemble,pattern recognition},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
volume = {51},
year = {2003}
}
@inproceedings{Duin2002,
author = {Pekalska, Ella and Duin, Robert P.W. and Skurichina, Marina},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pekalska, Duin, Skurichina - 2002 - A discussion on the classifier projection space for classifier combining.pdf:pdf},
pages = {137--148},
title = {{A discussion on the classifier projection space for classifier combining}},
url = {http://www.springerlink.com/index/A98FBKT93AK0YNNE.pdf},
year = {2002}
}
@book{Barber2012,
author = {Barber, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
title = {{Bayesian reasoning and machine learning}},
year = {2012}
}
@inproceedings{Auger2007,
author = {Auger, Anne and Teytaud, Olivier},
booktitle = {Proceedings of the 9th annual conference on Genetic and evolutionary computation},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Auger, Teytaud - 2007 - Continuous lunches are free!.pdf:pdf},
isbn = {9781595936974},
keywords = {free-lunch,kolmogorov,no-free-lunch,s extension theo-},
pages = {916--922},
title = {{Continuous lunches are free!}},
url = {http://dl.acm.org/citation.cfm?id=1277145},
year = {2007}
}
@article{Braga-Neto2004,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Braga-Neto, Dougherty - 2004 - Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Reproducibility of Results,Sample Size,Sensitivity and Specificity},
month = {feb},
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{Soares2004,
author = {Soares, Carlos and Brazdil, Pavel B. and Kuba, Petr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soares, Brazdil, Kuba - 2004 - A Meta-Learning Method to Select the KernelWidth in Support Vector Regression.pdf:pdf},
journal = {Machine learning},
keywords = {gaussian kernel,learning rankings,meta-learning,parameter setting,support vector machines},
pages = {195--209},
title = {{A Meta-Learning Method to Select the KernelWidth in Support Vector Regression}},
url = {http://link.springer.com/article/10.1023/b:mach.0000015879.28004.9b},
volume = {54},
year = {2004}
}
@article{Schaffer1993,
author = {Schaffer, Cullen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1993 - Selecting a classification method by cross-validation.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,cross-validation,decision trees,neural networks},
pages = {135--143},
title = {{Selecting a classification method by cross-validation}},
url = {http://link.springer.com/article/10.1007/BF00993106},
volume = {13},
year = {1993}
}
@article{Kalousis2001,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Hilario - 2001 - Model selection via meta-learning a comparative study.pdf:pdf},
journal = {International Journal on Artificial Intelligence Tools},
number = {4},
title = {{Model selection via meta-learning: a comparative study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000647},
volume = {10},
year = {2001}
}
@article{Dietterich1998,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1998 - Approximate statistical tests for comparing supervised classification learning algorithms.pdf:pdf},
journal = {Neural computation},
title = {{Approximate statistical tests for comparing supervised classification learning algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
year = {1998}
}
@article{Ho2002,
author = {Ho, Tin Kam and Basu, Mitra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Basu - 2002 - Complexity Measures of Supervised Classification Problems.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity Measures of Supervised Classification Problems}},
volume = {24},
year = {2002}
}
@inproceedings{Brazdil1994,
author = {Brazdil, Pavel B. and Gama, Joao and Henery, Bob},
booktitle = {Proceedings of the European conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Gama, Henery - 1994 - Characterizing the applicability of classification algorithms using meta-level learning.pdf:pdf},
pages = {83--102},
title = {{Characterizing the applicability of classification algorithms using meta-level learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-57868-4{\_}52},
year = {1994}
}
@article{Chan1997,
author = {Chan, Philip K. and Stolfo, Salvatore J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chan, Stolfo - 1997 - On the accuracy of meta-learning for scalable data mining.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
title = {{On the accuracy of meta-learning for scalable data mining}},
url = {http://www.springerlink.com/index/M27133K052552242.pdf},
year = {1997}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@article{Norton2003,
author = {Norton, John D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Norton - 2003 - A Material Theory of Induction.pdf:pdf},
journal = {Philosophy of Science},
number = {October},
pages = {647--670},
title = {{A Material Theory of Induction}},
volume = {70},
year = {2003}
}
@article{Spirtes2010,
author = {Spirtes, Peter},
doi = {10.2202/1557-4679.1203},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spirtes - 2010 - Introduction to causal inference.pdf:pdf},
issn = {1557-4679},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {1643--1662},
pmid = {20305706},
title = {{Introduction to causal inference}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836213{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Claassen2010,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2010 - Learning causal network structure from multiple (in) dependence models.pdf:pdf},
journal = {Proceedings of the Fifth European Workshop on Probabilistic Graphical Models},
title = {{Learning causal network structure from multiple (in) dependence models}},
year = {2010}
}
@article{Pearl2009,
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Structural equation models, confounding, graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@article{Claassen2005,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2005 - A Logical Characterization of Constraint-Based Causal Discovery.pdf:pdf},
title = {{A Logical Characterization of Constraint-Based Causal Discovery}},
year = {2005}
}
@article{Mooij,
author = {Mooij, Joris M and Heskes, Tom and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - Unknown - On Causal Discovery with Cyclic Additive Noise Models.pdf:pdf},
pages = {1--9},
title = {{On Causal Discovery with Cyclic Additive Noise Models}}
}
@article{Huopaniemi2010,
abstract = {Analysis of variance (ANOVA)-type methods are the default tool for the analysis of data with multiple covariates. These tools have been generalized to the multivariate analysis of high-throughput biological datasets, where the main challenge is the problem of small sample size and high dimensionality. However, the existing multi-way analysis methods are not designed for the currently increasingly important experiments where data is obtained from multiple sources. Common examples of such settings include integrated analysis of metabolic and gene expression profiles, or metabolic profiles from several tissues in our case, in a controlled multi-way experimental setup where disease status, medical treatment, gender and time-series are usual covariates.},
author = {Huopaniemi, Ilkka and Suvitaival, Tommi and Nikkil{\"{a}}, Janne and Oresic, Matej and Kaski, Samuel},
doi = {10.1093/bioinformatics/btq174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huopaniemi et al. - 2010 - Multivariate multi-way analysis of multi-source data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Analysis of Variance,Data Collection,Gene Expression Profiling,Gene Expression Profiling: methods,Multivariate Analysis},
month = {jun},
number = {12},
pages = {i391--8},
pmid = {20529933},
title = {{Multivariate multi-way analysis of multi-source data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2881359{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {26},
year = {2010}
}
@inproceedings{Kopf2000,
author = {K{\"{o}}pf, Christian and Taylor, Charles and Keller, Jorg},
booktitle = {Proceedings of the PKDD-00 workshop on data mining, decision support, meta-learning and ILP},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{o}}pf, Taylor, Keller - 2000 - Meta-analysis from data characterisation for meta-learning to meta-regression.pdf:pdf},
number = {Ml},
title = {{Meta-analysis: from data characterisation for meta-learning to meta-regression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.8159},
year = {2000}
}
@inproceedings{Giraud-Carrier2008,
author = {Giraud-carrier, Christophe},
booktitle = {Tutorial at the 2008 International Conference on Machine Learning and Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier - 2008 - Metalearning - A Tutorial.pdf:pdf},
number = {December},
title = {{Metalearning - A Tutorial}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/ICMLA2008Tut/ICMLA 2008.pdf},
year = {2008}
}
@unpublished{Amasyali2009,
author = {Amasyali, M Fatih and Ersoy, Okan K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amasyali, Ersoy - 2009 - A Study of Meta Learning for Regression.pdf:pdf},
institution = {Purdue University},
title = {{A Study of Meta Learning for Regression}},
year = {2009}
}
@article{Smith-Miles2008,
author = {Smith-Miles, Kate},
doi = {10.1145/1456650.1456656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smith-Miles - 2008 - Cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {dec},
number = {1},
pages = {1--25},
title = {{Cross-disciplinary perspectives on meta-learning for algorithm selection}},
url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
volume = {41},
year = {2008}
}
@article{Wang2009,
author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
doi = {10.1016/j.neucom.2008.10.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Smith-Miles, Hyndman - 2009 - Rule induction for forecasting method selection Meta-learning the characteristics of univariate time.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {jun},
number = {10-12},
pages = {2581--2594},
title = {{Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
volume = {72},
year = {2009}
}
@article{Varma2006,
abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Bias (Epidemiology),Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {jan},
pages = {91},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1397873{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Vanschoren2012,
author = {Vanschoren, Joaquin and Blockeel, Hendrik and Pfahringer, Bernhard and Holmes, Geoffrey},
doi = {10.1007/s10994-011-5277-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vanschoren et al. - 2012 - Experiment databases.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jan},
number = {2},
pages = {127--158},
title = {{Experiment databases}},
url = {http://www.springerlink.com/index/10.1007/s10994-011-5277-0},
volume = {87},
year = {2012}
}
@inproceedings{Hoekstra1996,
author = {Hoekstra, Aarnoud and Duin, Robert P.W.},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoekstra, Duin - 1996 - On the nonlinearity of pattern classifiers.pdf:pdf},
pages = {271--275},
title = {{On the nonlinearity of pattern classifiers}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547429},
year = {1996}
}
@article{Gelman2011,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Induction and deduction in Bayesian data analysis.pdf:pdf},
journal = {Rationality, Markets and Morals (RMM)},
pages = {67--78},
title = {{Induction and deduction in Bayesian data analysis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/philosophy{\_}online4.pdf},
volume = {2},
year = {2011}
}
@article{Buhmann2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.0375v1},
author = {Buhmann, Joachim M},
eprint = {arXiv:1006.0375v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhmann - 2010 - Information theoretic model validation for clustering.pdf:pdf},
number = {X},
title = {{Information theoretic model validation for clustering}},
volume = {2010},
year = {2010}
}
@inproceedings{Ben-David2011,
author = {Ben-David, Shai and Srebro, Nathan and Urner, R},
booktitle = {Philosophy and Machine Learning - Workshop at NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Srebro, Urner - 2011 - Universal learning vs. no free lunch results.pdf:pdf},
title = {{Universal learning vs. no free lunch results}},
url = {http://www.dsi.unive.it/PhiMaLe2011/Abstract/Ben-David{\_}Srebro{\_}Urner.pdf},
year = {2011}
}
@article{Ben-david2011,
author = {Ben-david, Shai and Srebro, Nati and Urner, Ruth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-david, Srebro, Urner - 2011 - Is learning possible without Prior Knowledge Do Universal Learners exist High level view of ( Statis.pdf:pdf},
title = {{Is learning possible without Prior Knowledge ? Do Universal Learners exist ? High level view of ( Statistical ) Machine Learning}},
year = {2011}
}
@article{Shaffer1991,
author = {Shaffer, Juliet Popper},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shaffer - 1991 - The Gauss-Markov Theorem and Random Regressors.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear unbiased estimators,finite-,linear regression,population sampling,unbiased esti-},
number = {4},
pages = {269--273},
title = {{The Gauss-Markov Theorem and Random Regressors}},
volume = {45},
year = {1991}
}
@article{Dougherty2001,
abstract = {In order to study the molecular biological differences between normal and diseased tissues, it is desirable to perform classification among diseases and stages of disease using microarray-based gene-expression values. Owing to the limited number of microarrays typically used in these studies, serious issues arise with respect to the design, performance and analysis of classifiers based on microarray data. This paper reviews some fundamental issues facing small-sample classification: classification rules, constrained classifiers, error estimation and feature selection. It discusses both unconstrained and constrained classifier design from sample data, and the contributions to classifier error from constrained optimization and lack of optimality owing to design from sample data. The difficulty with estimating classifier error when confined to small samples is addressed, particularly estimating the error from training data. The impact of small samples on the ability to include more than a few variables as classifier features is explained.},
author = {Dougherty, Edward R.},
doi = {10.1002/cfg.62},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dougherty - 2001 - Small sample issues for microarray-based classification.pdf:pdf},
issn = {1531-6912},
journal = {Comparative and functional genomics},
month = {jan},
number = {1},
pages = {28--34},
pmid = {18628896},
title = {{Small sample issues for microarray-based classification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447190{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2001}
}
@article{Wickenberg-Bolin2006,
abstract = {Supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. In practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. A suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. However, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. Therefore different methods for small sample performance estimation such as a recently proposed procedure called Repeated Random Sampling (RSS) is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. Here we explore such biases and develop a refined algorithm called Repeated Independent Design and Test (RIDT).},
author = {Wickenberg-Bolin, Ulrika and G{\"{o}}ransson, Hanna and Frykn{\"{a}}s, M{\aa}rten and Gustafsson, Mats G and Isaksson, Anders},
doi = {10.1186/1471-2105-7-127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wickenberg-Bolin et al. - 2006 - Improved variance estimation of classification performance via reduction of bias caused by small sample.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Analysis of Variance,Artificial Intelligence,Bias (Epidemiology),Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis},
month = {jan},
pages = {127},
pmid = {16533392},
title = {{Improved variance estimation of classification performance via reduction of bias caused by small sample size.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1435937{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Hanczar2010,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.2174/157489310790596376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Dougherty - 2010 - On the Comparison of Classifiers for Microarray Data.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {classifier comparison,error estimation,microarray classification,variance study},
month = {mar},
number = {1},
pages = {29--39},
title = {{On the Comparison of Classifiers for Microarray Data}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1574-8936{\&}volume=5{\&}issue=1{\&}spage=29},
volume = {5},
year = {2010}
}
@article{Burman1989,
author = {Burman, Prabir},
doi = {10.2307/2336116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Burman - 1989 - A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {sep},
number = {3},
pages = {503},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {http://www.jstor.org/stable/2336116?origin=crossref},
volume = {76},
year = {1989}
}
@article{Bengio2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Grandvalet - 2004 - No unbiased estimator of the variance of k-fold cross-validation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of k-fold cross-validation}},
volume = {5},
year = {2004}
}
@phdthesis{Hamers2012,
author = {Hamers, Adrian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamers - 2012 - The Evolution of Coeval Stellar Hierarchical Triple Systems.pdf:pdf},
school = {Utrecht University},
title = {{The Evolution of Coeval Stellar Hierarchical Triple Systems}},
year = {2012}
}
@article{Shannon1948,
author = {Shannon, Claude Elwood},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1948 - A mathematical theory of communication.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {J},
pages = {379--423},
title = {{A mathematical theory of communication}},
url = {http://dl.acm.org/citation.cfm?id=584093},
volume = {27},
year = {1948}
}
@article{Reif2012,
author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
doi = {10.1007/s10044-012-0280-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reif et al. - 2012 - Automatic classifier selection for non-experts.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {classifier recommendation,classifier selection,landmarking,meta-features,meta-learning,regression},
month = {jul},
title = {{Automatic classifier selection for non-experts}},
url = {http://www.springerlink.com/index/10.1007/s10044-012-0280-z},
year = {2012}
}
@book{Zhu2009,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00196ED1V01Y200906AIM006},
editor = {Brachman, Ronald J. and Dietterich, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Goldberg - 2009 - Introduction to Semi-Supervised Learning.pdf:pdf},
isbn = {9781598295474},
issn = {1939-4608},
number = {1},
pages = {1--130},
publisher = {Morgan {\&} Claypool},
title = {{Introduction to Semi-Supervised Learning}},
volume = {3},
year = {2009}
}
@techreport{Seeger2001,
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2001 - Learning with labeled and unlabeled data.pdf:pdf},
pages = {1--62},
title = {{Learning with labeled and unlabeled data}},
year = {2001}
}
@inproceedings{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Algorithmic Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2011 - Domain adaptation in regression.pdf:pdf},
title = {{Domain adaptation in regression}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24412-4{\_}25},
year = {2011}
}
@inproceedings{Cortes2010,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mansour, Mohri - 2010 - Learning bounds for importance weighting.pdf:pdf},
pages = {442--450},
title = {{Learning bounds for importance weighting}},
url = {http://www.cs.nyu.edu/{~}mohri/pub/importance.pdf},
year = {2010}
}
@inproceedings{Loog2012b,
author = {Loog, Marco and Jensen, Are C},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2012 - Constrained log-likelihood-based semi-supervised linear discriminant analysis.pdf:pdf},
pages = {327--335},
title = {{Constrained log-likelihood-based semi-supervised linear discriminant analysis}},
url = {http://www.springerlink.com/index/U16X1L3015777162.pdf},
year = {2012}
}
@book{Quinonero-Candela2009,
author = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Quinonero-Candela et al. - 2009 - Dataset shift in machine learning.pdf:pdf},
isbn = {9780262170055},
title = {{Dataset shift in machine learning}},
url = {http://dl.acm.org/citation.cfm?id=1462129},
year = {2009}
}
@article{Bickel2009,
author = {Bickel, Steffen and Br{\"{u}}ckner, Michael and Scheffer, Tobias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bickel, Br{\"{u}}ckner, Scheffer - 2009 - Discriminative learning under covariate shift.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {covariate shift,discriminative learning,transfer learning},
pages = {2137--2155},
title = {{Discriminative learning under covariate shift}},
url = {http://dl.acm.org/citation.cfm?id=1755858},
volume = {10},
year = {2009}
}
@unpublished{Looga,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - Unknown - A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification.pdf:pdf},
keywords = {constrained estimation,log-likelihood,nearest mean classifier,semi-supervised learning},
number = {1},
title = {{A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification}},
volume = {1}
}
@article{Erren2007,
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Erren, Bourne - 2007 - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = {may},
number = {5},
pages = {e102},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2007}
}
@article{Molinaro2005,
abstract = {In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Molinaro, Simon, Pfeiffer - 2005 - Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software},
month = {aug},
number = {15},
pages = {3301--7},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@article{McLachlan1982,
author = {McLachlan, Geoffrey J. and Ganesalingam, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan, Ganesalingam - 1982 - Updating a discriminant function on the basis of unclassified data.pdf:pdf},
journal = {Communication in Statistics- Simulation and Computation},
title = {{Updating a discriminant function on the basis of unclassified data}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918208812293},
year = {1982}
}
@article{Hartley1968,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - A new estimation for sample theory surveys.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {547--557},
title = {{A new estimation for sample theory surveys}},
volume = {55},
year = {1968}
}
@inproceedings{Singh2008,
author = {Singh, Aarti and Nowak, Robert D. and Zhu, Xiaojin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Singh, Nowak, Zhu - 2008 - Unlabeled data Now it helps, now it doesn't.pdf:pdf},
pages = {1513--1520},
title = {{Unlabeled data: Now it helps, now it doesn't}},
year = {2008}
}
@article{Liang2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0710.4618v1},
author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
doi = {10.1214/088342307000000032},
eprint = {arXiv:0710.4618v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liang, Mukherjee, West - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayesian analysis,bayesian kernel regression,latent factor models,mixture models,pervised learning,predictive distribution,semisu-,unlabeled data},
month = {may},
number = {2},
pages = {189--205},
title = {{The Use of Unlabeled Data in Predictive Modeling}},
url = {http://projecteuclid.org/euclid.ss/1190905518},
volume = {22},
year = {2007}
}
@inproceedings{Zhou2007,
author = {Zhou, Zhi-hua and Zhan, De-Chuan and Yang, Qiang},
booktitle = {Proceedings of the 22nd national conference on Artificial intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Zhan, Yang - 2007 - Semi-supervised learning with very few labeled training examples.pdf:pdf},
title = {{Semi-supervised learning with very few labeled training examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
year = {2007}
}
@article{Hsu2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2363v1},
author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
eprint = {arXiv:1106.2363v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hsu, Kakade, Zhang - 2011 - An analysis of random design linear regression.pdf:pdf},
journal = {arXiv preprint},
title = {{An analysis of random design linear regression}},
url = {http://arxiv.org/abs/1106.2363},
year = {2011}
}
@inproceedings{Zhou2005a,
author = {Zhou, Zhi-hua and Li, Ming},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Semi-Supervised Regression with Co-Training.pdf:pdf},
title = {{Semi-Supervised Regression with Co-Training.}},
url = {http://ijcai.org/Past Proceedings/IJCAI-05/PDF/0689.pdf},
year = {2005}
}
@article{Scott2009,
author = {Scott, Clayton and Blanchard, Gilles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Scott, Blanchard - 2009 - Novelty detection Unlabeled data definitely help.pdf:pdf},
pages = {464--471},
title = {{Novelty detection: Unlabeled data definitely help}},
url = {http://eprints.pascal-network.org/archive/00004475/},
volume = {5},
year = {2009}
}
@inproceedings{Goldman2000,
author = {Goldman, Sally and Zhou, Yan},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldman, Zhou - 2000 - Enhancing supervised learning with unlabeled data.pdf:pdf},
pages = {327--334},
title = {{Enhancing supervised learning with unlabeled data}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Enhancing+Supervised+Learning+with+Unlabeled+Data{\#}0},
volume = {3},
year = {2000}
}
@inproceedings{Huang2006,
author = {Huang, Jiayuan and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2006 - Correcting sample selection bias by unlabeled data.pdf:pdf},
pages = {601--608},
title = {{Correcting sample selection bias by unlabeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2006{\_}915.pdf},
year = {2006}
}
@article{VanRooden2011,
abstract = {The clinical heterogeneity of Parkinson's disease (PD) may point at the existence of subtypes. Because subtypes likely reflect distinct underlying etiologies, their identification may facilitate future genetic and pharmacotherapeutic studies. Aim of this study was to identify subtypes by a data-driven approach applied to a broad spectrum of motor and nonmotor features of PD. Data of motor and nonmotor PD symptoms were collected in 802 patients in two different European prevalent cohorts. A model-based cluster analysis was conducted on baseline data of 344 patients of a Dutch cohort (PROPARK). Reproducibility of these results was tested in data of the second annual assessment of the same cohort and validated in an independent Spanish cohort (ELEP) of 357 patients. The subtypes were subsequently characterized on clinical and demographic variables. Four similar PD subtypes were identified in two different populations and are largely characterized by differences in the severity of nondopaminergic features and motor complications: Subtype 1 was mildly affected in all domains, Subtype 2 was predominantly characterized by severe motor complications, Subtype 3 was affected mainly on nondopaminergic domains without prominent motor complications, while Subtype 4 was severely affected on all domains. The subtypes had largely similar mean disease durations (nonsignificant differences between three clusters) but showed considerable differences with respect to their association with demographic and clinical variables. In prevalent disease, PD subtypes are largely characterized by the severity of nondopaminergic features and motor complications and likely reflect complex interactions between disease mechanisms, treatment, aging, and gender.},
author = {van Rooden, Stephanie M and Colas, Fabrice P. R. and Mart{\'{i}}nez-Mart{\'{i}}n, Pablo and Visser, Martine and Verbaan, Dagmar and Marinus, Johan and Chaudhuri, Ray K and Kok, Joost N and van Hilten, Jacobus J},
doi = {10.1002/mds.23346},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2011 - Clinical subtypes of Parkinson's disease.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Aged,Cluster Analysis,Cohort Studies,Disease Progression,Female,Germany,Humans,Male,Middle Aged,Neurologic Examination,Parkinson Disease,Parkinson Disease: classification,Parkinson Disease: physiopathology,Reproducibility of Results,Spain,Time Factors},
month = {jan},
number = {1},
pages = {51--8},
pmid = {21322019},
title = {{Clinical subtypes of Parkinson's disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21322019},
volume = {26},
year = {2011}
}
@article{VanRooden2010,
abstract = {The clinical variability between patients with Parkinson's disease (PD) may point at the existence of subtypes of the disease. Identification of subtypes is important, since a focus on homogeneous groups may enhance the chance of success of research on mechanisms of disease and may also lead to tailored treatment strategies. Cluster analysis (CA) is an objective method to classify patients into subtypes. We systematically reviewed the methodology and results of CA studies in PD to gain a better understanding of the robustness of identified subtypes. We found seven studies that fulfilled the inclusion criteria. Studies were limited by incomplete reporting and methodological limitations. Differences between studies rendered comparisons of the results difficult. However, it appeared that studies which applied a comparable design identified similar subtypes. The cluster profiles "old age-at-onset and rapid disease progression" and "young age-at-onset and slow disease progression" emerged from the majority of studies. Other cluster profiles were less consistent across studies. Future studies with a rigorous study design that is standardized with respect to the included variables, data processing, and CA technique may advance the knowledge on subtypes in PD.},
author = {van Rooden, Stephanie M and Heiser, Willem J and Kok, Joost N and Verbaan, Dagmar and van Hilten, Jacobus J and Marinus, Johan},
doi = {10.1002/mds.23116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2010 - The identification of Parkinson's disease subtypes using cluster analysis a systematic review.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Algorithms,Cluster Analysis,Humans,Parkinson Disease,Parkinson Disease: classification,PubMed,PubMed: statistics {\&} numerical data},
month = {jun},
number = {8},
pages = {969--78},
pmid = {20535823},
title = {{The identification of Parkinson's disease subtypes using cluster analysis: a systematic review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20535823},
volume = {25},
year = {2010}
}
@article{VonHippel2007,
author = {von Hippel, Paul T.},
doi = {10.1111/j.1467-9531.2007.00180.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/von Hippel - 2007 - Regression With Missing Ys an Improved Strategy for Analyzing Multiply Imputed Data.pdf:pdf},
issn = {0081-1750},
journal = {Sociological Methodology},
month = {dec},
number = {1},
pages = {83--117},
title = {{Regression With Missing Ys: an Improved Strategy for Analyzing Multiply Imputed Data}},
url = {http://smx.sagepub.com/lookup/doi/10.1111/j.1467-9531.2007.00180.x},
volume = {37},
year = {2007}
}
@inproceedings{Lafferty2007,
author = {Lafferty, John D. and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems 20},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lafferty, Wasserman - 2007 - Statistical analysis of semi-supervised regression.pdf:pdf},
pages = {801----808},
title = {{Statistical analysis of semi-supervised regression}},
year = {2007}
}
@phdthesis{Marlin2008,
author = {Marlin, BM},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marlin - 2008 - Missing data problems in machine learning.pdf:pdf},
school = {University of Toronto},
title = {{Missing data problems in machine learning}},
url = {http://www-devel.cs.ubc.ca/{~}bmarlin/research/phd{\_}thesis/marlin-phd-thesis.pdf},
year = {2008}
}
@article{Leemis2008,
author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
doi = {10.1198/000313008X270448},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leemis, McQueston - 2008 - Univariate Distribution Relationships.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {asymptotic relationships,distribution proper-,limiting distributions,stochastic parameters,ties,transforma-},
month = {feb},
number = {1},
pages = {45--53},
title = {{Univariate Distribution Relationships}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
volume = {62},
year = {2008}
}
@article{Jain1999a,
author = {Jain, A.K. and Murty, M.N. and Flynn, P.J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
journal = {ACM computing surveys (CSUR)},
number = {3},
title = {{Data clustering: a review}},
url = {http://dl.acm.org/citation.cfm?id=331504},
volume = {31},
year = {1999}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
pages = {1027--1035},
title = {{k-means ++ : The Advantages of Careful Seeding}},
year = {2007}
}
@article{Breiman2001,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling+:+The+Two+Cultures{\#}2 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+Modeling:+The+Two+Cultures{\#}2},
volume = {16},
year = {2001}
}
@phdthesis{Krijthe2012,
abstract = {In order to choose from the large number of classification methods available for use, cross-validation error estimates are often employed. We present this cross-validation selection strategy in the framework of meta-learning and show that conceptually, meta- learning techniques could provide better classifier selections than traditional cross-validation selection. Using various simulation studies we illustrate and discuss this possibility. Through a collection of datasets resembling real-world data, we investigate whether these improvements could possibly exist in the real-world as well. Although the approach presented here currently requires signifi- cant investment when applied to practical applications, the concept of being able to outperform cross-validation selection opens the door to new classifier selection strategies.},
author = {Krijthe, Jesse Hendrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe - 2012 - Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning.pdf:pdf},
keywords = {Classifier Selection,Error estimation,Meta-Learning},
mendeley-tags = {Classifier Selection,Error estimation,Meta-Learning},
school = {Delft University of Technology},
title = {{Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning}},
year = {2012}
}
@inproceedings{Vandewalle2008,
author = {Vandewalle, Vincent and Biernacki, Christophe and Celeux, Gilles and Govaert, Gerard},
booktitle = {vincent.vandewalle.perso.sfr.fr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle et al. - 2008 - Are unlabeled data useful in semi-supervised model-based classification Combining hypothesis testing and mode.pdf:pdf},
title = {{Are unlabeled data useful in semi-supervised model-based classification? Combining hypothesis testing and model choice}},
url = {http://vincent.vandewalle.perso.sfr.fr/documents/recherche/articles/vbcg.pdf},
year = {2008}
}
@phdthesis{Colas,
author = {Colas, Fabrice P. R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Colas - 2009 - Data Mining Scenarios.pdf:pdf},
isbn = {9789090238883},
title = {{Data Mining Scenarios}},
year = {2009}
}
@article{Webb1996,
author = {Webb, Geoffrey I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb - 1996 - Further Experimental Evidence against the Utility of Occam's Razor.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {397--417},
title = {{Further Experimental Evidence against the Utility of Occam's Razor}},
url = {http://arxiv.org/abs/cs/9605101},
volume = {4},
year = {1996}
}
@article{King1995,
author = {King, R.D. and Feng, C and Sutherland, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/King, Feng, Sutherland - 1995 - Statlog comparison of classification algorithms on large real-world problems.pdf:pdf},
journal = {Applied Artificial Intelligence an International Journal},
number = {3},
pages = {289--333},
title = {{Statlog: comparison of classification algorithms on large real-world problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839519508945477},
volume = {9},
year = {1995}
}
@inproceedings{Macia2010,
author = {Macia, Nuria and Ho, Tin Kam and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia et al. - 2010 - The Landscape Contest at ICPR 2010.pdf:pdf},
pages = {29--45},
title = {{The Landscape Contest at ICPR 2010}},
year = {2010}
}
@article{Todorovski2003,
author = {Todorovski, Ljupco and D{\v{z}}eroski, Saso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Todorovski, D{\v{z}}eroski - 2003 - Combining classifiers with meta decision trees.pdf:pdf},
journal = {Machine learning},
keywords = {combining classifiers,decision trees,ensembles of classifiers,meta-level learning,stacking},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
url = {http://link.springer.com/article/10.1023/A:1021709817809},
year = {2003}
}
@phdthesis{Macia2011,
author = {Macia, Nuria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia - 2011 - Data Complexity in Supervised Learning A Far-Reaching Implication.pdf:pdf},
title = {{Data Complexity in Supervised Learning: A Far-Reaching Implication}},
year = {2011}
}
@article{Kalousis2004,
author = {Kalousis, Alexandros and Gama, Joao and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Gama, Hilario - 2004 - On data and algorithms Understanding inductive performance.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {275--312},
title = {{On data and algorithms: Understanding inductive performance}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015882.38031.85},
volume = {54},
year = {2004}
}
@article{Gama1995,
author = {Gama, Joao and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gama, Brazdil - 1995 - Characterization of Classification Algorithms.pdf:pdf},
journal = {Progress in Artificial Intelligence},
pages = {189--200},
title = {{Characterization of Classification Algorithms}},
url = {http://link.springer.com/chapter/10.1007/3-540-60428-6{\_}16},
year = {1995}
}
@article{Kalousis1999,
author = {Kalousis, Alexis and Theoharis, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Theoharis - 1999 - NOEMON An intelligent Assistant for Classifier Selection.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {classifier comparison,classifier selection,dataset morphology,multidimensional metrics},
number = {5},
pages = {319--337},
title = {{NOEMON: An intelligent Assistant for Classifier Selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.7762},
volume = {3},
year = {1999}
}
@article{Lindner1999,
author = {Lindner, Guido and Studer, Rudi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lindner, Studer - 1999 - AST Support for algorithm selection with a CBR approach.pdf:pdf},
journal = {Principles of Data Mining and Knowledge Discovery},
title = {{AST: Support for algorithm selection with a CBR approach}},
url = {http://www.springerlink.com/index/QBDF3R2GKVW57LUF.pdf http://link.springer.com/chapter/10.1007/978-3-540-48247-5{\_}52},
year = {1999}
}
@article{Michie1994,
author = {Michie, D and Spiegelhalter, D J and Taylor, C C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Michie, Spiegelhalter, Taylor - 1994 - Statlog.pdf:pdf},
title = {{Statlog}},
year = {1994}
}
@article{Lattimore2011,
archivePrefix = {arXiv},
arxivId = {1111.3846},
author = {Lattimore, Tor and Hutter, Marcus},
eprint = {1111.3846},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lattimore, Hutter - 2011 - No Free Lunch versus Occam's Razor in Supervised Learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {kolmogorov complexity,no free lunch,occam,s razor,supervised learning},
title = {{No Free Lunch versus Occam's Razor in Supervised Learning}},
url = {http://arxiv.org/abs/1111.3846},
year = {2011}
}
@inproceedings{Shaffer1994,
author = {Schaffer, Cullen},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1994 - A conservation law for generalization performance.pdf:pdf},
title = {{A conservation law for generalization performance}},
url = {http://dml.cs.byu.edu/{~}cgc/docs/mldm{\_}tools/Reading/LCG.pdf},
year = {1994}
}
@inproceedings{Pfahringer2000,
author = {Pfahringer, Bernhard and Giraud-carrier, Christophe},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pfahringer, Giraud-carrier - 2000 - Meta-Learning by Landmarking Various Learning Algorithms.pdf:pdf},
pages = {743--750},
title = {{Meta-Learning by Landmarking Various Learning Algorithms}},
year = {2000}
}
@inproceedings{Rao1995,
author = {Rao, R. Bharat and Gordon, Diana and Spears, William},
booktitle = {Proceedings of the 12th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rao, Gordon, Spears - 1995 - For Every Generalization Action, Is there really an Equal and Opposite Reaction Analysis of the conservatio.pdf:pdf},
pages = {471--479},
title = {{For Every Generalization Action, Is there really an Equal and Opposite Reaction? Analysis of the conservation Law for Generalization Performance}},
url = {http://www.researchgate.net/publication/2516136{\_}For{\_}Every{\_}Generalization{\_}Action{\_}Is{\_}There{\_}Really{\_}An{\_}Equal{\_}And{\_}Opposite{\_}Reaction{\_}Analysis{\_}of{\_}the{\_}Conservation{\_}Law{\_}for{\_}Generalization{\_}Performance/file/79e4150b866697f897.pdf},
year = {1995}
}
@inproceedings{Carroll2007,
author = {Carroll, James L. and Seppi, Kevin D.},
booktitle = {IJCNN Workshop on Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carroll, Seppi - 2007 - No-free-lunch and Bayesian optimality.pdf:pdf},
title = {{No-free-lunch and Bayesian optimality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.7564{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Wolpert2002,
author = {Wolpert, David H},
booktitle = {Soft Computing and Industry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf:pdf},
pages = {25--42},
title = {{The Supervised Learning No-Free-Lunch Theorems}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0123-9{\_}3},
year = {2002}
}
@book{Kuncheva2004,
author = {Kuncheva, Ludmila I},
booktitle = {Methods and Algorithms. Wiley, Chichester},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2004 - Combining Pattern Classifers.pdf:pdf},
isbn = {9786468600},
title = {{Combining Pattern Classifers}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2005.s320 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Combining+Pattern+Classifiers{\#}3},
year = {2004}
}
@inproceedings{Macia2009,
author = {Macia, Nuria and Orriols-puig, Albert and Bernad{\'{o}}-Mansilla, Ester},
booktitle = {Hybrid Artificial Intelligence Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia, Orriols-puig, Bernad{\'{o}}-Mansilla - 2009 - Beyond Homemade Artificial Data Sets.pdf:pdf},
keywords = {artificial data sets,data complexity,machine learning},
pages = {605--612},
title = {{Beyond Homemade Artificial Data Sets}},
url = {http://www.springerlink.com/index/N23720WL67U355MV.pdf http://link.springer.com/chapter/10.1007/978-3-642-02319-4{\_}73},
year = {2009}
}
@article{Giraud-carrier2004,
author = {Giraud-carrier, Christophe and Vilalta, Ricardo and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Vilalta, Brazdil - 2004 - Introduction to the special issue on meta-learning.pdf:pdf},
journal = {Machine learning},
keywords = {dynamic bias selection,inductive bias,meta-knowledge,meta-learning},
pages = {187--193},
title = {{Introduction to the special issue on meta-learning}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015878.60765.42},
volume = {54},
year = {2004}
}
@article{Peng2002,
author = {Peng, Yonghong and Flach, Peter A. and Soares, Carlos and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peng et al. - 2002 - Improved dataset characterisation for meta-learning.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {141--152},
title = {{Improved dataset characterisation for meta-learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-36182-0{\_}14},
volume = {2534},
year = {2002}
}
@article{Vilalta2002,
author = {Vilalta, Ricardo and Drissi, Youssef},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vilalta, Drissi - 2002 - A perspective view and survey of meta-learning.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {classification,inductive learning,meta-knowledge},
number = {1997},
pages = {77--95},
title = {{A perspective view and survey of meta-learning}},
url = {http://link.springer.com/article/10.1023/A:1019956318069},
year = {2002}
}
@book{Basu2006,
author = {Basu, Mitra and Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Basu, Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
title = {{Data complexity in pattern recognition}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GflBKbzym9oC{\&}oi=fnd{\&}pg=PR11{\&}dq=Data+Complexity+in+Pattern+Recognition{\&}ots=igbI3IXn6d{\&}sig=-7L3L4iU5lzLaNaCVoEux{\_}GbVn4},
year = {2006}
}
@inproceedings{Aha1992,
author = {Aha, David W.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Aha - 1992 - Generalizing from case studies A case study.pdf:pdf},
title = {{Generalizing from case studies: A case study}},
year = {1992}
}
@article{Shore1980,
author = {Shore, John E. and Johnson, Rodney W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shore, Johnson - 1980 - Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {26--37},
title = {{Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1056144},
volume = {26},
year = {1980}
}
@article{Ireland1968a,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Contingence tables with given marginals.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {179--188},
title = {{Contingence tables with given marginals}},
volume = {55},
year = {1968}
}
@article{Ireland1968,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Minimum Discrimination Information Estimation.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {707--713},
title = {{Minimum Discrimination Information Estimation}},
url = {http://www.jstor.org/stable/10.2307/2528330},
volume = {24},
year = {1968}
}
@article{Kullback1968,
author = {Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kullback - 1968 - Probability Densities with Given Marginals.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {1236--1243},
title = {{Probability Densities with Given Marginals}},
url = {http://www.jstor.org/stable/10.2307/2239692},
volume = {39},
year = {1968}
}
@article{Caticha2011,
author = {Caticha, Ariel and Mohammad-Djafari, Ali and Bercher, Jean-FrancÌ§ois and BessieÌre, Pierre},
doi = {10.1063/1.3573619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha et al. - 2011 - Entropic Inference.pdf:pdf},
isbn = {9780735408609},
keywords = {bayes rule,entropy,information,maximum entropy},
number = {1},
pages = {20--29},
title = {{Entropic Inference}},
url = {http://link.aip.org/link/APCPCS/v1305/i1/p20/s1{\&}Agg=doi},
volume = {20},
year = {2011}
}
@inproceedings{Caticha2006,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608185v1},
author = {Caticha, Ariel and Giffin, Adom},
booktitle = {Bayesian Inference and Maximum Entropy Methods In Science and Engineering},
eprint = {0608185v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha, Giffin - 2006 - Updating Probabilities.pdf:pdf},
pages = {31--42},
primaryClass = {arXiv:physics},
title = {{Updating Probabilities}},
url = {http://arxiv.org/abs/physics/0608185},
volume = {872},
year = {2006}
}
@inproceedings{Grunwald2000,
author = {Gr{\"{u}}nwald, Peter},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald - 2000 - Maximum entropy and the glasses you are looking through.pdf:pdf},
pages = {238--246},
title = {{Maximum entropy and the glasses you are looking through}},
url = {http://dl.acm.org/citation.cfm?id=2073975},
year = {2000}
}
@unpublished{Bresson2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0699v1},
author = {Bresson, Xavier and Zhang, Ruiliang},
booktitle = {arXiv preprint},
eprint = {arXiv:1210.0699v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bresson, Zhang - 2012 - TV-SVM Total Variation Support Vector Machine for Semi-Supervised Data Classification.pdf:pdf},
title = {{TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification}},
url = {http://arxiv.org/abs/1210.0699},
year = {2012}
}
@article{Bartlett2003a,
author = {Bartlett, Peter L and Mendelson, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Mendelson - 2003 - Rademacher and Gaussian complexities Risk bounds and structural results.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
url = {http://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2003}
}
@article{Shafer2008,
author = {Shafer, Glenn and Vovk, Vladimir},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shafer, Vovk - 2008 - A tutorial on conformal prediction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {confidence,on-line compression modeling,on-line learning,prediction regions},
pages = {371--421},
title = {{A tutorial on conformal prediction}},
url = {http://dl.acm.org/citation.cfm?id=1390693},
volume = {9},
year = {2008}
}
@inproceedings{Bottou2011,
author = {Bottou, Leon and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems 24},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou, Bousquet - 2011 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {In Advances in Neural Information Processing Syste},
title = {{The Tradeoffs of Large-Scale Learning}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large+Scale+Learning{\&}ots=vbhayjhcGc{\&}sig=kWCMo7N51TgoLQSVSv2f{\_}ILArjo http://books.google.com/books?hl=en{\&}lr={\&}id=JPQx7s2L1A8C{\&}oi=fnd{\&}pg=PA351{\&}dq=The+Tradeoffs+of+Large-Scale+Learning{\&}ots=vbjaAkg8Fe{\&}sig=chdz7lCKXTFdUaLPYAgH{\_}FfgLmA},
year = {2011}
}
@article{Bousquet2004,
author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bousquet, Boucheron, Lugosi - 2004 - Introduction to statistical learning theory.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {169--207},
title = {{Introduction to statistical learning theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@article{Cheplygina2010,
author = {Cheplygina, Veronika},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheplygina - 2010 - Random Subspace Method for One-Class Classifiers.pdf:pdf},
title = {{Random Subspace Method for One-Class Classifiers}},
year = {2010}
}
@inproceedings{Druck2010,
author = {Druck, Gregory and McCallum, Andrew Kachites},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck, McCallum - 2010 - High-performance semi-supervised learning using discriminatively constrained generative models.pdf:pdf},
pages = {319--326},
title = {{High-performance semi-supervised learning using discriminatively constrained generative models}},
url = {http://www.cs.umass.edu/{~}gdruck/pubs/druck10high.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}DruckM10.pdf},
year = {2010}
}
@article{Mann2010,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2010 - Generalized expectation criteria for semi-supervised learning with weakly labeled data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {condi-,generalized expectation criteria,logistic regression,semi-supervised learning,tional random fields},
pages = {955--984},
title = {{Generalized expectation criteria for semi-supervised learning with weakly labeled data}},
volume = {11},
year = {2010}
}
@article{Hamsici2008,
abstract = {We present an algorithm which provides the one-dimensional subspace where the Bayes error is minimized for the C class problem with homoscedastic Gaussian distributions. Our main result shows that the set of possible one-dimensional spaces v, for which the order of the projected class means is identical, defines a convex region with associated convex Bayes error function g(v). This allows for the minimization of the error function using standard convex optimization algorithms. Our algorithm is then extended to the minimization of the Bayes error in the more general case of heteroscedastic distributions. This is done by means of an appropriate kernel mapping function. This result is further extended to obtain the d-dimensional solution for any given d, by iteratively applying our algorithm to the null space of the (d - 1)-dimensional solution. We also show how this result can be used to improve up on the outcomes provided by existing algorithms, and derive a low-computational cost, linear approximation. Extensive experimental validations are provided to demonstrate the use of these algorithms in classification, data analysis and visualization.},
author = {Hamsici, Onur C and Martinez, Aleix M},
doi = {10.1109/TPAMI.2007.70717},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamsici, Martinez - 2008 - Bayes optimality in linear discriminant analysis.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Computer Simulation,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Linear Models,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {apr},
number = {4},
pages = {647--57},
pmid = {18276970},
title = {{Bayes optimality in linear discriminant analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18276970},
volume = {30},
year = {2008}
}
@article{Subramanya2011a,
author = {Subramanya, A and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2011 - Semi-supervised learning with measure propagation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph-based semi-supervised learning,large-scale semi-supervised,learning,non-parametric models,transductive inference},
pages = {3311--3370},
title = {{Semi-supervised learning with measure propagation}},
url = {http://dl.acm.org/citation.cfm?id=2078212},
volume = {12},
year = {2011}
}
@phdthesis{Druck2011,
author = {Druck, Gregory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck - 2011 - Generalized Expectation Criteria for Lightly Supervised Learning.pdf:pdf},
number = {September},
title = {{Generalized Expectation Criteria for Lightly Supervised Learning}},
url = {http://scholarworks.umass.edu/open{\_}access{\_}dissertations/440/},
year = {2011}
}
@article{Haffari2012,
author = {Haffari, Gholamreza and Sarkar, Anoop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haffari, Sarkar - 2012 - Analysis of semi-supervised learning with the yarowsky algorithm.pdf:pdf},
journal = {arXiv preprint},
title = {{Analysis of semi-supervised learning with the yarowsky algorithm}},
url = {http://arxiv.org/abs/1206.5240},
year = {2012}
}
@phdthesis{Hillebrand2012,
author = {Hillebrand, Arne},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hillebrand - 2012 - Separating a polygonal environment into a multi-layered environment.pdf:pdf},
keywords = {branch,explicit corridor map,ge-,graphs,local search,multi-layered environment,multicut,netic algorithm,price},
school = {Utrecht University},
title = {{Separating a polygonal environment into a multi-layered environment}},
year = {2012}
}
@article{Kawano2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1108.5244v3},
author = {Kawano, Shuichi},
eprint = {arXiv:1108.5244v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawano - 2012 - Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,covariate shift,em algorithm,model selection,reg-,semi-supervised learning,ularization},
pages = {1--19},
title = {{Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions}},
year = {2012}
}
@article{Kawakita2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.3965v1},
author = {Kawakita, Masanori and Kanamori, Takafumi},
eprint = {arXiv:1204.3965v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Kanamori - 2013 - Semi-Supervised learning with Density-Ratio Estimation.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {189--209},
title = {{Semi-Supervised learning with Density-Ratio Estimation}},
volume = {91},
year = {2013}
}
@inproceedings{Sokolovska2011,
address = {Greece},
author = {Sokolovska, Nataliya},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska - 2011 - Aspects of semi-supervised and active learning in conditional random fields.pdf:pdf},
keywords = {active learn-,conditional random fields,ing,probability of observations,semi-supervised learning},
title = {{Aspects of semi-supervised and active learning in conditional random fields}},
url = {http://www.springerlink.com/index/3308764R6251J70P.pdf},
year = {2011}
}
@inproceedings{Chaubey2003,
author = {Chaubey, Yogendra P. and Nebebe, Fassil and Sen, Debaraj},
booktitle = {Joint Statistical Meetings},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaubey, Nebebe, Sen - 2003 - Estimation of Joint Distribution from Marginal Distributions.pdf:pdf},
keywords = {bayesian prediction,because they require multidimensional,ble,contingency ta- methods,dirichlet prior,however,is preferred as it,merical integration,nu-,of the,readily presents an estimate,the bayesian method},
pages = {883--889},
title = {{Estimation of Joint Distribution from Marginal Distributions}},
url = {http://www.amstat.org/sections/SRMS/Proceedings/y2003/Files/JSM2003-000794.pdf},
year = {2003}
}
@article{Zhou2005,
author = {Zhou, Zhi-hua and Li, Ming},
doi = {10.1109/TKDE.2005.186},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Tri-training exploiting unlabeled data using three classifiers.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {nov},
number = {11},
pages = {1529--1541},
title = {{Tri-training: exploiting unlabeled data using three classifiers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1512038},
volume = {17},
year = {2005}
}
@article{Gretton,
author = {Gretton, Arthur and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gretton, Sch{\"{o}}lkopf - Unknown - A Kernel Method for the Two-Sample-Problem.pdf:pdf},
title = {{A Kernel Method for the Two-Sample-Problem}}
}
@inproceedings{Matti2006,
author = {Kaariainen, Matti},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kaariainen - 2006 - Semi-Supervised Model Selection Based on Cross-Validation.pdf:pdf},
number = {510},
title = {{Semi-Supervised Model Selection Based on Cross-Validation}},
year = {2006}
}
@article{Nguyen2009,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1214/08-AOS595},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2009 - On surrogate loss functions and f -divergences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {876--904},
title = {{On surrogate loss functions and f -divergences}},
url = {http://projecteuclid.org/euclid.aos/1236693153},
volume = {37},
year = {2009}
}
@article{Gelman2011a,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Ethics and statistics Open data and open methods.pdf:pdf},
journal = {Chance},
pages = {51--53},
title = {{Ethics and statistics: Open data and open methods}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Ethics+and+Statistics+Open+Data+and+Open+Methods{\#}3},
year = {2011}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.0020124},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2005}
}
@article{White1982,
author = {White, Halbert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White - 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {1--25},
title = {{Maximum Likelihood Estimation of Misspecified Models}},
url = {http://www.jstor.org/stable/10.2307/1912526},
volume = {50},
year = {1982}
}
@inproceedings{Grandvalet2005,
address = {Cambridge, MA},
author = {Grandvalet, Yves and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grandvalet, Bengio - 2005 - Semi-supervised learning by entropy minimization.pdf:pdf},
pages = {529--536},
publisher = {MIT Press},
title = {{Semi-supervised learning by entropy minimization}},
year = {2005}
}
@article{Sch,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.2738v1},
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Zhang, Kun},
eprint = {arXiv:1112.2738v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf et al. - Unknown - Robust Learning via Cause-Effect Models.pdf:pdf},
pages = {1--15},
title = {{Robust Learning via Cause-Effect Models}}
}
@inproceedings{Niu2012,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2012 - Information-theoretic Semi-supervised Metric Learning via Entropy Regularization.pdf:pdf},
number = {c},
title = {{Information-theoretic Semi-supervised Metric Learning via Entropy Regularization}},
url = {http://arxiv.org/abs/1206.4614},
year = {2012}
}
@inproceedings{Ben-David2008,
author = {Ben-David, Shai and Lu, Tyler and P{\'{a}}l, David},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Lu, P{\'{a}}l - 2008 - Does Unlabeled Data Provably Help Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.pdf:pdf},
pages = {33--44},
title = {{Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.}},
url = {http://www.cs.toronto.edu/{~}tl/papers/ssl.pdf},
year = {2008}
}
@inproceedings{Dasgupta2002,
author = {Dasgupta, Sanjoy and Littman, Michael L. and McAlles},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta, Littman, McAlles - 2002 - PAC generalization bounds for co-training.pdf:pdf},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=PGrlRWV5-v0C{\&}oi=fnd{\&}pg=PA375{\&}dq=PAC+Generalization+Bounds+for+Co-training{\&}ots=auaN1CGPip{\&}sig=0dID1oXJYgeENxwSzfsntvwz{\_}oU},
year = {2002}
}
@inproceedings{Zhou2007a,
author = {Zhou, Zhi-hua and Xu, Jun-Ming},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Xu - 2007 - On the relation between multi-instance learning and semi-supervised learning.pdf:pdf},
number = {1997},
title = {{On the relation between multi-instance learning and semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1273643},
year = {2007}
}
@article{Halkidi2001,
author = {Halkidi, Maria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halkidi - 2001 - On Clustering Validation Techniques.pdf:pdf},
keywords = {cluster validity,clustering algorithms,unsupervised learning,validity indices},
pages = {107--145},
title = {{On Clustering Validation Techniques}},
year = {2001}
}
@inproceedings{Brefeld2006,
author = {Brefeld, Ulf and G{\"{a}}rtner, Thomas and Scheffer, Tobias and Wrobel, Stefan},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brefeld et al. - 2006 - Efficient co-regularised least squares regression.pdf:pdf},
pages = {137--144},
title = {{Efficient co-regularised least squares regression}},
url = {http://dl.acm.org/citation.cfm?id=1143862},
year = {2006}
}
@inproceedings{Scholkopf2012,
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf, Janzing, Peters - 2012 - On Causal and Anticausal Learning.pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@inproceedings{Cai2007,
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
booktitle = {IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408856},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, He, Han - 2007 - Semi-supervised Discriminant Analysis.pdf:pdf},
isbn = {978-1-4244-1630-1},
pages = {1--7},
title = {{Semi-supervised Discriminant Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408856},
year = {2007}
}
@inproceedings{Jordan2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation - which is borne out in repeated experiments - that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
author = {Jordan, Michael I. and Ng, Andrew Y},
booktitle = {Advances in neural information processing systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan, Ng - 2002 - On Discriminative vs. Generative classifiers comparison of logistic regression and naive Bayes.pdf:pdf},
number = {14},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: comparison of logistic regression and naive Bayes}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA841{\&}dq=On+Discriminative+vs.+Generative+classifiers:+comparison+of+logistic+regression+and+naive+Bayes{\&}ots=ZvO0F2{\_}vx9{\&}sig=0nMLd-CWMsb8-jyrI6YetIH6ZZU},
volume = {2},
year = {2002}
}
@article{Abney2004,
author = {Abney, Steven},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2004 - Understanding the yarowsky algorithm.pdf:pdf},
journal = {Computational Linguistics},
number = {3},
pages = {365--395},
title = {{Understanding the yarowsky algorithm}},
volume = {30},
year = {2004}
}
@article{Senn2011,
author = {Senn, Stephen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn - 2011 - You may believe you are a Bayesian but you are probably wrong.pdf:pdf},
journal = {Rationality, Markets and Morals},
pages = {48--66},
title = {{You may believe you are a Bayesian but you are probably wrong}},
url = {http://www.rmm-journal.com/downloads/Article{\_}Senn.pdf},
volume = {2},
year = {2011}
}
@article{Castelli1994,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1995 - On the exponential value of labeled samples.pdf:pdf},
journal = {Pattern Recognition Letters},
pages = {105--111},
title = {{On the exponential value of labeled samples}},
volume = {16},
year = {1995}
}
@inproceedings{Foulds2011,
author = {Foulds, James and Smyth, Padhraic},
booktitle = {SIAM International Conference on Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foulds, Smyth - 2011 - Multi-instance mixture models and semi-supervised learning.pdf:pdf},
number = {Mi},
title = {{Multi-instance mixture models and semi-supervised learning}},
url = {http://siam.omnibooksonline.com/2011datamining/data/papers/256.pdf},
year = {2011}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
url = {http://home.olemiss.edu/{~}xdang/676/Greedy{\_}function{\_}approximation{\_}a{\_}gradient{\_}bossting{\_}machine.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Greedy+Function+Approximation:+A+Gradient+Boosting+Machine{\#}3},
volume = {29},
year = {2001}
}
@article{Shevade2003,
author = {Shevade, S. K. and Keerthi, S. S.},
doi = {10.1093/bioinformatics/btg308},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shevade, Keerthi - 2003 - A simple and efficient algorithm for gene selection using sparse logistic regression.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {17},
pages = {2246--2253},
title = {{A simple and efficient algorithm for gene selection using sparse logistic regression}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btg308},
volume = {19},
year = {2003}
}
@article{Raudys1998,
author = {Raudys, Sarunas and Duin, Robert P.W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raudys, Duin - 1998 - Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {Dimensionality,fisher linear discriminant,generalization error,pseudo-inversion,sample size,scissors effect,statistical classification},
month = {apr},
number = {5-6},
pages = {385--392},
publisher = {Elsevier},
title = {{Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix}},
volume = {19},
year = {1998}
}
@article{Castelli1996,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1996 - The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {2102},
title = {{The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter}},
volume = {42},
year = {1996}
}
@article{Shi2011,
author = {Shi, Mingguang and Zhang, Bing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shi, Zhang - 2011 - Semi-supervised learning improves gene expression-based prediction of cancer recurrence.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3017--3023},
title = {{Semi-supervised learning improves gene expression-based prediction of cancer recurrence}},
volume = {27},
year = {2011}
}
@article{Ye2007a,
address = {New York, New York, USA},
author = {Ye, Jieping},
doi = {10.1145/1273496.1273633},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ye - 2007 - Least squares linear discriminant analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning},
keywords = {18,3,8,are linear combinations of,class separability,derived features in lda,dimension reduction,least squares,linear discriminant anal-,linear regression,the,the data achieves maximum,the orig-,ysis},
pages = {1087--1093},
publisher = {ACM Press},
title = {{Least squares linear discriminant analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273633},
year = {2007}
}
@article{Witten2011,
abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p â« n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00783.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2011 - Penalized classification using Fisher's linear discriminant.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {classification,discriminant analysis,feature selection,high dimensional problems,lasso,linear,supervised learning},
month = {nov},
number = {5},
pages = {753--772},
pmid = {22323898},
title = {{Penalized classification using Fisher's linear discriminant.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3272679{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {73},
year = {2011}
}
@article{Martella2011,
abstract = {In healthy aging research, typically multiple health outcomes are measured, representing health status. The aim of this paper was to develop a model-based clustering approach to identify homogeneous sibling pairs according to their health status. Model-based clustering approaches will be considered on the basis of linear mixed effect model for the mixture components. Class memberships of siblings within pairs are allowed to be correlated, and within a class the correlation between siblings is modeled using random sibling pair effects. We propose an expectation-maximization algorithm for maximum likelihood estimation. Model performance is evaluated via simulations in terms of estimating the correct parameters, degree of agreement, and the ability to detect the correct number of clusters. The performance of our model is compared with the performance of standard model-based clustering approaches. The methods are used to classify sibling pairs from the Leiden Longevity Study according to their health status. Our results suggest that homogeneous healthy sibling pairs are associated with a longer life span. Software is available for fitting the new models.},
author = {Martella, F. and Vermunt, J.K. and Beekman, M. and Westendorp, R.G.J. and Slagboom, P.E. and Houwing-Duistermaat, J.J.},
doi = {10.1002/sim.4365},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Martella et al. - 2011 - A mixture model with random-effects components for classifying sibling pairs.pdf:pdf},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {80 and over,Aged,Aging,Aging: physiology,Cluster Analysis,Computer Simulation,Female,Health,Humans,Longevity,Longevity: physiology,Male,Models,Siblings,Statistical},
month = {nov},
number = {27},
pages = {3252--64},
pmid = {21905068},
title = {{A mixture model with random-effects components for classifying sibling pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21905068},
volume = {30},
year = {2011}
}
@article{Hanczar2010a,
abstract = {The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = {mar},
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@inproceedings{Ratsaby1995,
author = {Ratsaby, Joel and Venkatesht, Santosh S.},
booktitle = {Proceedings of the 8th Annual conference on Computational learning theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ratsaby, Venkatesht - 1995 - Learning from a mixture of labeled and unlabeled examples with parametric side information.pdf:pdf},
pages = {412--417},
title = {{Learning from a mixture of labeled and unlabeled examples with parametric side information}},
url = {http://dl.acm.org/citation.cfm?id=225348},
year = {1995}
}
@article{Guo2010,
author = {Guo, Yuanyuan and Niu, Xiaoda and Zhang, Harry},
doi = {10.1109/ICDM.2010.66},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guo, Niu, Zhang - 2010 - An Extensive Empirical Study on Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {IEEE International Conference on Data Mining},
keywords = {-semi-supervised learning,bayesian classifiers},
month = {dec},
pages = {186--195},
publisher = {Ieee},
title = {{An Extensive Empirical Study on Semi-supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693972},
year = {2010}
}
@article{Wainwright2008,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@article{Dy2004a,
author = {Dy, Jennifer G. and Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dy, Brodley - 2004 - Feature selection for unsupervised learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation-maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature selection for unsupervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1016787},
volume = {5},
year = {2004}
}
@article{Wang2007,
author = {Wang, Junhui and Shen, Xiaotong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen - 2007 - Large margin Semi-supervised Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalization,grouping,sequential quadratic programming,support vectors},
pages = {1867--1891},
title = {{Large margin Semi-supervised Learning}},
volume = {8},
year = {2007}
}
@article{Mann2007,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2007 - Efficient computation of entropy gradient for semi-supervised conditional random fields.pdf:pdf},
journal = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Efficient computation of entropy gradient for semi-supervised conditional random fields}},
url = {http://dl.acm.org/citation.cfm?id=1614136},
year = {2007}
}
@inproceedings{Ben-David2007,
author = {Rakhlin, Alexander and Caponnetto, Andrea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rakhlin, Caponnetto - Unknown - Stability of k-means clustering.pdf:pdf},
title = {{Stability of k-means clustering}}
}
@article{Goldenberg2009,
author = {Luxburg, Ulrike Von},
doi = {10.1561/2200000008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Luxburg - 2009 - Clustering Stability An overview.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {3},
pages = {235--274},
title = {{Clustering Stability: An overview}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000008},
volume = {2},
year = {2009}
}
@article{Belkin2006,
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belkin, Niyogi, Sindhwani - 2006 - Manifold regularization A geometric framework for learning from labeled and unlabeled examples.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fold learning,graph transduction,kernel methods,mani-,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
pages = {2399--2434},
title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
url = {http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@phdthesis{Mika2002,
author = {Mika, Sebastian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mika - 2002 - Kernel fisher discriminants.pdf:pdf},
title = {{Kernel fisher discriminants}},
url = {http://opus.kobv.de/tuberlin/volltexte/2003/477/},
year = {2002}
}
@article{Wang2012a,
abstract = {This paper proposes a method to select a set of genes from a large number of genes with the ability of classifying types of diseases. The proposed gene selection method is designed according to correlation analysis and the concept of 95{\%} reference range. The method is very simple and uses the information of all genes. We have used the method in leukemia patients and achieved good classification results.},
author = {Wang, Xiaodong and Tian, Jun},
doi = {10.1155/2012/586246},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Tian - 2012 - A gene selection method for cancer classification.pdf:pdf},
issn = {1748-6718},
journal = {Computational and mathematical methods in medicine},
keywords = {array,cancer classification,diagnosis,diagnostic tests,dna micro-,drug discovery,feature selection,gene selection,genomics,proteomics,recursive feature elimination,rna expression,support vector machines},
month = {jan},
pages = {586246},
pmid = {23251228},
title = {{A gene selection method for cancer classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23289441},
volume = {2012},
year = {2012}
}
@misc{Tibshirani,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - Unknown - Machine Learning vs. Statistics.pdf:pdf},
title = {{Machine Learning vs. Statistics}}
}
@article{Witten2010,
abstract = {We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1198/jasa.2010.tm09415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2010 - A framework for feature selection in clustering.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hierarchical clustering,high-dimensional,k-means clustering,lasso,model selection,sparsity,unsupervised learning},
month = {jun},
number = {490},
pages = {713--726},
pmid = {20811510},
title = {{A framework for feature selection in clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930825{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {105},
year = {2010}
}
@article{Tibshirani2001,
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani, Walther, Hastie - 2001 - Estimating the number of clusters in a data set via the gap statistic.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
pages = {411--423},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract},
volume = {63},
year = {2001}
}
@article{Fraley2002,
author = {Fraley, Chris and Raftery, Adrian. E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fraley, Raftery - 2002 - Model-based clustering, discriminant analysis, and density estimation.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {458},
title = {{Model-based clustering, discriminant analysis, and density estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214502760047131},
volume = {97},
year = {2002}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arlot, Celisse - 2010 - A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrd et al. - 1995 - A limited memory algorithm for bound constrained optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
number = {5},
pages = {1190--1208},
title = {{A limited memory algorithm for bound constrained optimization}},
volume = {16},
year = {1995}
}
@article{Yarowsky1995,
address = {Morristown, NJ, USA},
author = {Yarowsky, David},
doi = {10.3115/981658.981684},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
journal = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
pages = {189--196},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
year = {1995}
}
@article{McLachlan1975,
author = {McLachlan, Geoffrey J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1975 - Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant An.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {365--369},
title = {{Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis}},
volume = {70},
year = {1975}
}
@article{Wilson,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4245v2},
author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
eprint = {arXiv:1302.4245v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Adams - Unknown - Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation.pdf:pdf},
title = {{Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation}}
}
@inproceedings{Joachims2003,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 2003 - Transductive learning via spectral graph partitioning.pdf:pdf},
pages = {290--297},
title = {{Transductive learning via spectral graph partitioning}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-040.pdf},
year = {2003}
}
@article{Zhang2004a,
author = {Zhang, Tong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2004 - Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {56--134},
title = {{Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization}},
url = {http://www.jstor.org/stable/10.2307/3448494},
volume = {32},
year = {2004}
}
@inproceedings{Ji2012,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ji et al. - 2012 - A simple algorithm for semi-supervised learning with improved generalization error bound.pdf:pdf},
number = {2},
title = {{A simple algorithm for semi-supervised learning with improved generalization error bound}},
url = {http://arxiv.org/abs/1206.6412},
year = {2012}
}
@inproceedings{Plessis2012,
author = {du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Plessis, Sugiyama - 2012 - Semi-supervised learning of class balance under class-prior change by distribution matching.pdf:pdf},
title = {{Semi-supervised learning of class balance under class-prior change by distribution matching}},
url = {http://arxiv.org/abs/1206.4677},
year = {2012}
}
@book{Gelman2003,
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2003 - Bayesian Data Analysis.pdf:pdf},
title = {{Bayesian Data Analysis}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Bayesian+Data+Analysis{\#}0},
year = {2003}
}
@article{Bartlett2006,
author = {Bartlett, Peter L and Jordan, Michael I. and McAuliffe, Jon D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Jordan, McAuliffe - 2006 - Convexity, Classification, and Risk Bounds.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {boosting,convex optimization},
month = {mar},
number = {473},
pages = {138--156},
title = {{Convexity, Classification, and Risk Bounds}},
volume = {101},
year = {2006}
}
@article{Nowak2008,
abstract = {When applying hierarchical clustering algorithms to cluster patient samples from microarray data, the clustering patterns generated by most algorithms tend to be dominated by groups of highly differentially expressed genes that have closely related expression patterns. Sometimes, these genes may not be relevant to the biological process under study or their functions may already be known. The problem is that these genes can potentially drown out the effects of other genes that are relevant or have novel functions. We propose a procedure called complementary hierarchical clustering that is designed to uncover the structures arising from these novel genes that are not as highly expressed. Simulation studies show that the procedure is effective when applied to a variety of examples. We also define a concept called relative gene importance that can be used to identify the influential genes in a given clustering. Finally, we analyze a microarray data set from 295 breast cancer patients, using clustering with the correlation-based distance measure. The complementary clustering reveals a grouping of the patients which is uncorrelated with a number of known prognostic signatures and significantly differing distant metastasis-free probabilities.},
author = {Nowak, Gen and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm046},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nowak, Tibshirani - 2008 - Complementary hierarchical clustering.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Computer Simulation,Female,Fuzzy Logic,Gene Expression,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Profiling: statistics {\&} numerical,Genetic Markers,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neoplasm Metastasis,Neoplasm Metastasis: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Principal Component Analysis,Reference Values},
month = {jul},
number = {3},
pages = {467--83},
pmid = {18093965},
title = {{Complementary hierarchical clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3294318{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@inproceedings{Zhu2003,
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Ghahramani, Lafferty - 2003 - Semi-supervised learning using gaussian fields and harmonic functions.pdf:pdf},
pages = {912--919},
title = {{Semi-supervised learning using gaussian fields and harmonic functions}},
year = {2003}
}
@article{Gelman2013,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
doi = {10.1111/j.2044-8317.2011.02037.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:pdf},
issn = {2044-8317},
journal = {The British journal of mathematical and statistical psychology},
month = {feb},
number = {1},
pages = {8--38},
pmid = {22364575},
title = {{Philosophy and the practice of Bayesian statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22364575},
volume = {66},
year = {2013}
}
@misc{Klein2004,
author = {Klein, Dan},
booktitle = {University of California at Berkeley, Computer Science {\ldots}},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klein - 2004 - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {http://www.ee.columbia.edu/{~}vittorio/LagrangeMultipliers-Klein.pdf},
year = {2004}
}
@inproceedings{Goldberg2007,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Wright, Stephen},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu, Wright - 2007 - Dissimilarity in graph-based semi-supervised classification.pdf:pdf},
number = {1},
pages = {55--162},
title = {{Dissimilarity in graph-based semi-supervised classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS07{\_}GoldbergZW.pdf},
year = {2007}
}
@inproceedings{Lawrence2004,
author = {Lawrence, Neil D. and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lawrence, Jordan - 2004 - Semi-supervised learning via Gaussian processes.pdf:pdf},
pages = {753--760},
title = {{Semi-supervised learning via Gaussian processes}},
year = {2004}
}
@inproceedings{Elworthy1994,
archivePrefix = {arXiv},
arxivId = {arXiv:cmp-lg/9410012v2},
author = {Elworthy, David},
booktitle = {Proceedings of the fourth conference on Applied natural language processing},
eprint = {9410012v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Elworthy - 1994 - Does Baum-Welch re-estimation help taggers.pdf:pdf},
pages = {53--58},
primaryClass = {arXiv:cmp-lg},
title = {{Does Baum-Welch re-estimation help taggers?}},
year = {1994}
}
@inproceedings{Cozman2003,
author = {Cozman, Fabio Gagliardi and Cohen, Ira and Cirelo, Marcelo Cesar},
booktitle = {Proceedings of the Twentieth International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen, Cirelo - 2003 - Semi-Supervised Learning of Mixture Models.pdf:pdf},
title = {{Semi-Supervised Learning of Mixture Models}},
year = {2003}
}
@article{Goldberg2009,
author = {Goldberg, Andrew B. and Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu - 2009 - Keepin'it real semi-supervised learning with realistic tuning.pdf:pdf},
journal = {NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing},
title = {{Keepin'it real: semi-supervised learning with realistic tuning}},
year = {2009}
}
@article{Sugiyama2009,
author = {Sugiyama, Masashi and Id{\'{e}}, Tsuyoshi and Nakajima, Shinichi and Sese, Jun},
doi = {10.1007/s10994-009-5125-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama et al. - 2009 - Semi-supervised local Fisher discriminant analysis for dimensionality reduction.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1-2},
pages = {35--61},
title = {{Semi-supervised local Fisher discriminant analysis forÂ dimensionality reduction}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5125-7},
volume = {78},
year = {2009}
}
@inproceedings{Ben-David2006,
author = {Ben-David, Shai and Luxburg, Ulrike Von and P{\'{a}}l, David},
booktitle = {Proceedings of the 19th Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Luxburg, P{\'{a}}l - 2006 - A sober look at clustering stability.pdf:pdf},
number = {2002},
pages = {5--19},
title = {{A sober look at clustering stability}},
url = {http://link.springer.com/chapter/10.1007/11776420{\_}4},
year = {2006}
}
@article{Gelman2013a,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - P values and statistical practice.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology},
month = {jan},
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Hanselmann2013,
abstract = {Digital staining for the automated annotation of mass spectrometry imaging (MSI) data has previously been achieved using state-of-the-art classifiers such as random forests or support vector machines (SVMs). However, the training of such classifiers requires an expert to label exemplary data in advance. This process is time-consuming and hence costly, especially if the tissue is heterogeneous. In theory, it may be sufficient to only label a few highly representative pixels of an MS image, but it is not known a priori which pixels to select. This motivates active learning strategies in which the algorithm itself queries the expert by automatically suggesting promising candidate pixels of an MS image for labeling. Given a suitable querying strategy, the number of required training labels can be significantly reduced while maintaining classification accuracy. In this work, we propose active learning for convenient annotation of MSI data. We generalize a recently proposed active learning method to the multiclass case and combine it with the random forest classifier. Its superior performance over random sampling is demonstrated on secondary ion mass spectrometry data, making it an interesting approach for the classification of MS images.},
author = {Hanselmann, Michael and R{\"{o}}der, Jens and K{\"{o}}the, Ullrich and Renard, Bernhard Y and Heeren, Ron M.A. and Hamprecht, Fred A.},
doi = {10.1021/ac3023313},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2013 - Active learning for convenient annotation and classification of secondary ion mass spectrometry images.pdf:pdf},
issn = {1520-6882},
journal = {Analytical Chemistry},
month = {jan},
number = {1},
pages = {147--55},
pmid = {23157438},
title = {{Active learning for convenient annotation and classification of secondary ion mass spectrometry images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23157438},
volume = {85},
year = {2013}
}
@inproceedings{Cortes2004,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2004 - AUC optimization vs. error rate minimization.pdf:pdf},
pages = {313--320},
title = {{AUC optimization vs. error rate minimization}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA313{\&}dq=AUC+Optimization+vs+.+Error+Rate+Minimization{\&}ots=TGKup{\_}Ra93{\&}sig=VTdv-C5TW9itNMlz43YJjmxRKAc},
year = {2004}
}
@article{Rifkin2003,
author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin, Yeo, Poggio - 2003 - Regularized least-squares classification.pdf:pdf},
journal = {Nato Science Series Sub Series III Computer and Systems Sciences 190},
title = {{Regularized least-squares classification}},
year = {2003}
}
@techreport{Welling,
author = {Welling, Max},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welling - Unknown - Kernel ridge Regression.pdf:pdf},
number = {3},
pages = {3--5},
title = {{Kernel ridge Regression}}
}
@book{Chapelle2006,
author = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, Alexander},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Sch{\"{o}}lkopf, Zien - 2006 - Semi-supervised learning.pdf:pdf},
isbn = {9780262033589},
publisher = {MIT press},
title = {{Semi-supervised learning}},
year = {2006}
}
@misc{Lichman2013,
author = {Lichman, M.},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, Leon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Large scale transductive SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large scale transductive SVMs}},
volume = {7},
year = {2006}
}
@article{Nigam2000,
author = {Nigam, Kamal and McCallum, Andrew and Kachites and Thrun, Sebastian and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam et al. - 2000 - Text classification from labeled and unlabeled documents using EM.pdf:pdf},
journal = {Machine learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
pages = {1--34},
title = {{Text classification from labeled and unlabeled documents using EM}},
volume = {34},
year = {2000}
}
@inproceedings{Sindhwani2006,
address = {New York, New York, USA},
author = {Sindhwani, Vikas and Keerthi, S. S.},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Keerthi - 2006 - Large scale semi-supervised linear SVMs.pdf:pdf},
isbn = {1595933697},
keywords = {global optimiza-,support vector machines,text categorization,tion,unlabeled data},
pages = {477},
publisher = {ACM Press},
title = {{Large scale semi-supervised linear SVMs}},
year = {2006}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression shrinkage and selection via the lasso}},
volume = {58},
year = {1996}
}
@incollection{Cozman2006,
author = {Cozman, F and Cohen, Ira},
booktitle = {Semi-Supervised Learning},
chapter = {4},
editor = {Chapelle, Olivier and Sch{\"{o}}lkopf, Bernhard and Zien, A},
pages = {56--72},
publisher = {MIT press},
title = {{Risks of Semi-Supervised Learning}},
year = {2006}
}
@incollection{Opper1996,
address = {New York},
author = {Opper, Manfred and Kinzel, Wolfgang},
booktitle = {Models of Neural Networks III},
editor = {Domany, Eytan and Hemmen, J. Leo and Schulten, Klaus},
pages = {151--209},
publisher = {Springer},
title = {{Statistical Mechanics of Generalization}},
year = {1996}
}
@inproceedings{Balcan2006,
author = {Balcan, Maria-Florina and Beygelzimer, Alina and Langford, John},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Beygelzimer, Langford - 2006 - Agnostic active learning.pdf:pdf},
keywords = {active learning,agnostic setting,linear,sample complexity},
pages = {65--72},
title = {{Agnostic active learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0022000008000652},
year = {2006}
}
@article{Stahlecker1996,
author = {Stahlecker, Peter and Knautz, Henning and Trenkler, Gotz},
doi = {10.1007/BF00046994},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stahlecker, Knautz, Trenkler - 1996 - Minimax adjustment technique in a parameter restricted linear model.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {linear regression,minimax adjustment,projection estimator},
month = {apr},
number = {1},
pages = {139--144},
title = {{Minimax adjustment technique in a parameter restricted linear model}},
url = {http://link.springer.com/10.1007/BF00046994},
volume = {43},
year = {1996}
}
@article{Schmidt1996,
author = {Schmidt, Karsten},
doi = {10.1007/BF00046993},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt - 1996 - A comparison of minimax and least squares estimators in linear regression with polyhedral prior information.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {1,3,average performance,estimator,inequality restricted least squares,minimax estima-,n vector of,parameter restrictions,polyhedral prior information in,projection estimators,regression model y,the linear regression model,tion,u,we consider the linear,where y is an,x},
month = {apr},
number = {1},
pages = {127--138},
title = {{A comparison of minimax and least squares estimators in linear regression with polyhedral prior information}},
url = {http://link.springer.com/10.1007/BF00046993},
volume = {43},
year = {1996}
}
@article{Hartley1968b,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and Estimation in Analysis of Variance Problems.pdf:pdf},
journal = {Revue de l'Institut International de Statistique},
number = {2},
pages = {141--147},
title = {{Classification and Estimation in Analysis of Variance Problems}},
url = {http://www.jstor.org/stable/10.2307/1401602},
volume = {36},
year = {1968}
}
@inproceedings{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-hua},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2011 - Towards making unlabeled data never hurt.pdf:pdf},
pages = {1081--1088},
title = {{Towards making unlabeled data never hurt}},
year = {2011}
}
@phdthesis{Lu2009,
author = {Lu, Tyler},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu - 2009 - Fundamental Limitations of Semi-Supervised Learning.pdf:pdf},
title = {{Fundamental Limitations of Semi-Supervised Learning}},
year = {2009}
}
@article{Wang2013,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Jebara, Chang - 2013 - Semi-Supervised Learning Using Greedy Max-Cut.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {771--800},
title = {{Semi-Supervised Learning Using Greedy Max-Cut}},
url = {http://www.ee.columbia.edu/ln/dvmm/publications/13/ggmc{\_}13.pdf},
volume = {14},
year = {2013}
}
@inproceedings{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz, Singer - 2007 - Pegasos Primal estimated sub-gradient solver for svm.pdf:pdf},
pages = {807--814},
title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
url = {http://link.springer.com/article/10.1007/s10107-010-0420-4},
year = {2007}
}
@article{Crammer2006,
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Crammer et al. - 2006 - Online passive-aggressive algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {551--585},
title = {{Online passive-aggressive algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1248566},
volume = {7},
year = {2006}
}
@article{Azizyan2013,
author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
doi = {10.1214/13-AOS1092},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Azizyan, Singh, Wasserman - 2013 - Density-sensitive semisupervised inference.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {751--771},
title = {{Density-sensitive semisupervised inference}},
url = {http://projecteuclid.org/euclid.aos/1368018172},
volume = {41},
year = {2013}
}
@article{Poggio2003,
author = {Poggio, Tomaso and Smale, Steve},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio, Smale - 2003 - The Mathematics of Learning Dealing with Data.pdf:pdf},
journal = {Notices of the AMS},
pages = {537--544},
title = {{The Mathematics of Learning: Dealing with Data}},
year = {2003}
}
@article{Zhao2013,
author = {Zhao, Ming-Jie and Edakunni, Narayanan and Pocock, Adam and Brown, Gavin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - Beyond Fano's Inequality Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {balanced error rate,conditional entropy,cost-sensitive risk,f $\beta$ -measure,f-score},
pages = {1033--1090},
title = {{Beyond Fano's Inequality : Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications}},
url = {http://jmlr.csail.mit.edu/papers/volume14/zhao13a/zhao13a.pdf},
volume = {14},
year = {2013}
}
@article{Chapelle2007,
abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
author = {Chapelle, Olivier},
doi = {10.1162/neco.2007.19.5.1155},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle - 2007 - Training a support vector machine in the primal.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated},
month = {may},
number = {5},
pages = {1155--78},
pmid = {17381263},
title = {{Training a support vector machine in the primal.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381263},
volume = {19},
year = {2007}
}
@inproceedings{Cortes1993,
author = {Cortes, Corinna and Jackel, L.D.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Jackel - 1993 - Learning Cuves Asymptotic Values and Rate of Convergence.pdf:pdf},
pages = {327--334},
title = {{Learning Cuves: Asymptotic Values and Rate of Convergence}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Cuves:+Asymptotic+Values+and+Rate+of+Convergence{\#}0},
year = {1993}
}
@article{Seaman2013,
author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
doi = {10.1214/13-STS415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seaman et al. - 2013 - What Is Meant by âMissing at Randomâ.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Ignorability, direct-likelihood inference, frequen,and phrases,direct-likelihood inference,frequen-,ignorability,missing completely at random,repeated sampling,tist inference},
month = {may},
number = {2},
pages = {257--268},
title = {{What Is Meant by âMissing at Randomâ?}},
url = {http://projecteuclid.org/euclid.ss/1369147915},
volume = {28},
year = {2013}
}
@article{Marchand2004,
author = {Marchand, E and Strawderman, WE},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Strawderman - 2004 - Estimation in Restricted Parameter Spaces A Review.pdf:pdf},
journal = {Institute of Mathematical Statistics Lecture Notes-Monograph Series},
number = {2004},
pages = {21--44},
title = {{Estimation in Restricted Parameter Spaces: A Review}},
url = {http://www.jstor.org/stable/10.2307/4356296},
volume = {45},
year = {2004}
}
@article{Chiou2007,
author = {Chiou, Jeng-Min and Li, Pai-Ling},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chiou, Li - 2007 - Functional clustering and identifying substructures of longitudinal data.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {analysis,classification,clustering,functional data,functional principal component,modes of variation,stochastic processes},
pages = {679--699},
title = {{Functional clustering and identifying substructures of longitudinal data}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00605.x/full},
volume = {69},
year = {2007}
}
@article{Gaffke1989,
author = {Gaffke, Norbert and Heiligers, Berthold},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gaffke, Heiligers - 1989 - Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space.pdf:pdf},
journal = {Statistics: A Journal of Theoretical and Applied Statistics},
pages = {487--508},
title = {{Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space}},
volume = {4},
year = {1989}
}
@article{Wang2009a,
author = {Wang, Fei and Wang, Xin and Li, Tao},
doi = {10.1109/CVPR.2009.5206675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Wang, Li - 2009 - Beyond the graphs Semi-parametric semi-supervised discriminant analysis.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {2113--2120},
publisher = {Ieee},
title = {{Beyond the graphs: Semi-parametric semi-supervised discriminant analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206675},
year = {2009}
}
@article{Gelman2012a,
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill, Yajima - 2012 - Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf:pdf},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
month = {apr},
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213},
volume = {5},
year = {2012}
}
@unpublished{Loog2013,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2013 - Conservative Transductive and Semi-Supervised Empirical Risk Minimization.pdf:pdf},
pages = {1--9},
title = {{Conservative Transductive and Semi-Supervised Empirical Risk Minimization}},
year = {2013}
}
@article{Schmidt1995,
author = {Schmidt, Karsten and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt, Stahlecker - 1995 - Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {1--15},
title = {{Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949659508811648},
volume = {52},
year = {1995}
}
@article{Arnold2000,
author = {Arnold, Bernard F. and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arnold, Stahlecker - 2000 - The minimax adjustment principle.pdf:pdf},
journal = {Mathematical methods of operations research},
keywords = {ellipsoidal information,minimax,minimax adjustment principle,principle,projection estimator,supply policy},
pages = {103--113},
title = {{The minimax adjustment principle}},
url = {http://link.springer.com/article/10.1007/s001860050005},
volume = {51},
year = {2000}
}
@article{Niyogi2013,
author = {Niyogi, Partha},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niyogi - 2013 - Manifold Regularization and Semi-supervised Learning Some Theoretical Analyses.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph laplacian,manifold regularization,minimax rates,semi-supervised learning},
pages = {1229--1250},
title = {{Manifold Regularization and Semi-supervised Learning : Some Theoretical Analyses}},
volume = {14},
year = {2013}
}
@article{Efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron, Morris - 1977 - Stein's paradox in statistics.pdf:pdf},
journal = {Scientific American},
pages = {119--127},
title = {{Stein's paradox in statistics}},
url = {https://www.cs.nyu.edu/{~}roweis/csc2515-2006/readings/stein{\_}sciam.pdf},
year = {1977}
}
@article{Keogh2005,
author = {Keogh, Eamonn and Lin, Jessica},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keogh, Lin - 2005 - Clustering of time-series subsequences is meaningless implications for previous and future research.pdf:pdf},
journal = {Knowledge and information systems},
number = {2},
pages = {154--177},
title = {{Clustering of time-series subsequences is meaningless: implications for previous and future research}},
url = {http://link.springer.com/article/10.1007/s10115-004-0172-7},
volume = {8},
year = {2005}
}
@article{Kanamori2012,
author = {Kanamori, Takafumi and Takeda, A and Suzuki, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kanamori, Takeda, Suzuki - 2012 - A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {consistency,convex conjugate,loss function,uncertainty set},
pages = {1461--1504},
title = {{A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems}},
url = {http://arxiv.org/abs/1204.6583},
volume = {14},
year = {2012}
}
@article{Dhillon2013,
author = {Dhillon, Paramveer S. and Foster, Dean P. and Kakade, Sham M. and Ungar, Lyle H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dhillon et al. - 2013 - A Risk Comparison of Ordinary Least Squares vs Ridge Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {pca,ridge regression,risk inflation},
pages = {1505--1511},
title = {{A Risk Comparison of Ordinary Least Squares vs Ridge Regression}},
url = {http://adsabs.harvard.edu/abs/2011arXiv1105.0875D},
volume = {14},
year = {2013}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = {may},
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@article{Vandewalle2013,
author = {Vandewalle, Vincent and Biernacki, Christophe},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle, Biernacki - 2013 - A predictive deviance criterion for selecting a generative model in semi-supervised classification.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
pages = {220--236},
title = {{A predictive deviance criterion for selecting a generative model in semi-supervised classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000546},
volume = {64},
year = {2013}
}
@book{Rothenberg1973,
author = {Rothenberg, Thomas J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothenberg - 1973 - Efficient Estimation with A Priori Information.pdf:pdf},
publisher = {Yale University Press},
title = {{Efficient Estimation with A Priori Information}},
url = {http://www.getcited.org/pub/101421013},
year = {1973}
}
@article{Blanchard2010,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blanchard, Lee, Scott - 2010 - Semi-supervised novelty detection.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2973--3009},
title = {{Semi-supervised novelty detection}},
url = {http://dl.acm.org/citation.cfm?id=1953028},
volume = {11},
year = {2010}
}
@inproceedings{Krijthe2012b,
author = {Krijthe, Jesse Hendrik and Ho, Tin Kam and Loog, Marco},
booktitle = {Proceedings of the 21st International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Ho, Loog - 2012 - Improving cross-validation based classifier selection using meta-learning.pdf:pdf},
number = {1},
pages = {2873--2876},
title = {{Improving cross-validation based classifier selection using meta-learning}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6460765},
year = {2012}
}
@article{Jones2012,
author = {Jones, Emrys A and Deininger, S{\"{o}}ren-oliver and Hogendoorn, Pancras C W and Deelder, Andr{\'{e}} M and Mcdonnell, Liam A},
doi = {10.1016/j.jprot.2012.06.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2012 - Imaging mass spectrometry statistical analysis.pdf:pdf},
issn = {1874-3919},
journal = {Journal of Proteomics},
keywords = {Biomarker discovery,Data analysis,Molecular histology,imaging mass spectrometry},
number = {16},
pages = {4962--4989},
publisher = {Elsevier B.V.},
title = {{Imaging mass spectrometry statistical analysis}},
url = {http://dx.doi.org/10.1016/j.jprot.2012.06.014},
volume = {75},
year = {2012}
}
@inproceedings{Nigam2000a,
author = {Nigam, Kamal and Ghani, R},
booktitle = {Proceedings of the 9th International Conference on Information and Knowledge Management},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training.pdf:pdf},
isbn = {1581133200},
keywords = {a related set of,blum and mitchell 1,for example,in problem domains where,into,present,research uses labeled and,the features naturally divide,two disjoint sets,unlabeled data},
pages = {86--93},
title = {{Analyzing the effectiveness and applicability of co-training}},
url = {http://dl.acm.org/citation.cfm?id=354805},
year = {2000}
}
@inproceedings{Collins1999,
author = {Collins, Michael and Singer, Yoram},
booktitle = {Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collins, Singer - 1999 - Unsupervised models for named entity classification.pdf:pdf},
pages = {189--196},
title = {{Unsupervised models for named entity classification}},
url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR{\%}7B.{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}C7{\%}9D{\%}EF{\%}BF{\%}BD{\%}E2{\%}80{\%}A1{\%}5E{\%}EF{\%}BF{\%}BD{\%}EF{\%}BF{\%}BD{\%}C3{\%}A87},
year = {1999}
}
@inproceedings{Blum1998,
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279962},
year = {1998}
}
@article{Zhou2007b,
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {1159--1166},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273642},
year = {2007}
}
@article{Sun2013,
author = {Sun, Quan and Pfahringer, Bernhard},
doi = {10.1007/s10994-013-5387-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Pfahringer - 2013 - Pairwise meta-rules for better meta-learning-based algorithm ranking.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {algorithm ranking,ensemble learning,meta-learning,ranking trees},
month = {jul},
number = {1},
pages = {141--161},
title = {{Pairwise meta-rules for better meta-learning-based algorithm ranking}},
url = {http://link.springer.com/10.1007/s10994-013-5387-y},
volume = {93},
year = {2013}
}
@article{Sun2010,
author = {Sun, Shiliang and Shawe-taylor, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Shawe-taylor - 2010 - Sparse Semi-supervised Learning Using Conjugate Functions.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fenchel-legendre conjugate,multi-,representer theorem,semi-supervised learning,statistical learning theory,support vector machine,view regularization},
pages = {2423--2455},
title = {{Sparse Semi-supervised Learning Using Conjugate Functions}},
volume = {11},
year = {2010}
}
@article{Pan2013,
author = {Pan, Wei and Shen, Xiaotong and Liu, Binghui},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pan, Shen, Liu - 2013 - Cluster Analysis Unsupervised Learning via Supervised Learning with a Non-convex Penalty.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalized degrees of freedom,gression,grouping,k-means clustering,lasso,penalized re-,tlp,truncated lasso penalty},
pages = {1865--1889},
title = {{Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty}},
volume = {14},
year = {2013}
}
@inproceedings{VanderMaaten2013,
author = {{Van der Maaten}, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten et al. - 2013 - Learning with Marginalized Corrupted Features.pdf:pdf},
pages = {410--418},
title = {{Learning with Marginalized Corrupted Features}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}vandermaaten13},
year = {2013}
}
@inproceedings{Ogawa2013,
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ogawa et al. - 2013 - Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines.pdf:pdf},
pages = {897--905},
title = {{Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}sugi/2013/ICML2013b.pdf},
year = {2013}
}
@inproceedings{Niu2013,
author = {Niu, G and Jitkrittum, W and Dai, Bo and Hachiya, H and Sugiyama, M},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2013 - Squared-loss Mutual Information Regularization A Novel Information-theoretic Approach to Semi-supervised Learning.pdf:pdf},
pages = {10--18},
title = {{Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning}},
url = {http://sugiyama-www.cs.titech.ac.jp/{~}gang/paper/niu{\_}icml13.pdf},
year = {2013}
}
@inproceedings{Balcan2013,
author = {Balcan, Maria-Florina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2013 - Efficient Semi-supervised and Active Learning of Disjunctions.pdf:pdf},
pages = {633--641},
title = {{Efficient Semi-supervised and Active Learning of Disjunctions}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}balcan13},
year = {2013}
}
@inproceedings{Joulin2012,
author = {Joulin, Armand and Bach, Francis},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joulin, Bach - 2012 - A convex relaxation for weakly supervised classifiers.pdf:pdf},
keywords = {MIL,convex relaxation,weak supervision},
pages = {1279--1286},
title = {{A convex relaxation for weakly supervised classifiers}},
url = {http://arxiv.org/abs/1206.6413},
year = {2012}
}
@inproceedings{McDowell2012,
author = {McDowell, Luke and Aha, David W.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McDowell, Aha - 2012 - Semi-supervised collective classification via hybrid label regularization.pdf:pdf},
pages = {975--982},
title = {{Semi-supervised collective classification via hybrid label regularization}},
url = {http://arxiv.org/abs/1206.6467},
year = {2012}
}
@article{Li2013,
author = {Li, YF and Tsang, IW and Kwok, JT and Zhou, ZH},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Convex and Scalable Weakly Labeled SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2151--2188},
title = {{Convex and Scalable Weakly Labeled SVMs}},
url = {http://arxiv.org/abs/1303.1271},
volume = {14},
year = {2013}
}
@article{Doksum2007,
author = {Doksum, Kjell and Ozeki, Akichika and Kim, Jihoon and {Chaibub Neto}, Elias},
doi = {10.1016/j.spl.2007.03.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doksum et al. - 2007 - Thinking outside the box Statistical inference based on KullbackâLeibler empirical projections.pdf:pdf},
issn = {01677152},
journal = {Statistics {\&} Probability Letters},
keywords = {bootstrap,box-cox transformation,classification,covariate,k-l divergence,klep,outside the box,sandwich formula},
month = {jul},
number = {12},
pages = {1201--1213},
title = {{Thinking outside the box: Statistical inference based on KullbackâLeibler empirical projections}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715207000843},
volume = {77},
year = {2007}
}
@article{Bruand2011,
abstract = {Mass Spectrometric Imaging (MSI) is a molecular imaging technique that allows the generation of 2D ion density maps for a large complement of the active molecules present in cells and sectioned tissues. Automatic segmentation of such maps according to patterns of co-expression of individual molecules can be used for discovery of novel molecular signatures (molecules that are specifically expressed in particular spatial regions). However, current segmentation techniques are biased toward the discovery of higher abundance molecules and large segments; they allow limited opportunity for user interaction, and validation is usually performed by similarity to known anatomical features. We describe here a novel method, AMASS (Algorithm for MSI Analysis by Semi-supervised Segmentation). AMASS relies on the discriminating power of a molecular signal instead of its intensity as a key feature, uses an internal consistency measure for validation, and allows significant user interaction and supervision as options. An automated segmentation of entire leech embryo data images resulted in segmentation domains congruent with many known organs, including heart, CNS ganglia, nephridia, nephridiopores, and lateral and ventral regions, each with a distinct molecular signature. Likewise, segmentation of a rat brain MSI slice data set yielded known brain features and provided interesting examples of co-expression between distinct brain regions. AMASS represents a new approach for the discovery of peptide masses with distinct spatial features of expression. Software source code and installation and usage guide are available at http://bix.ucsd.edu/AMASS/ .},
author = {Bruand, Jocelyne and Alexandrov, Theodore and Sistla, Srinivas and Wisztorski, Maxence and Meriaux, C{\'{e}}line and Becker, Michael and Salzet, Michel and Fournier, Isabelle and Macagno, Eduardo and Bafna, Vineet},
doi = {10.1021/pr2005378},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruand et al. - 2011 - AMASS algorithm for MSI analysis by semi-supervised segmentation.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: metabolism,Cluster Analysis,Computational Biology,Computational Biology: methods,Gene Expression Regulation,Gene Expression Regulation, Developmental,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Leeches,Mass Spectrometry,Mass Spectrometry: methods,Peptides,Peptides: chemistry,Rats,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = {oct},
number = {10},
pages = {4734--43},
pmid = {21800894},
title = {{AMASS: algorithm for MSI analysis by semi-supervised segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3190602{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Meding2012,
abstract = {In clinical diagnostics, it is of outmost importance to correctly identify the source of a metastatic tumor, especially if no apparent primary tumor is present. Tissue-based proteomics might allow correct tumor classification. As a result, we performed MALDI imaging to generate proteomic signatures for different tumors. These signatures were used to classify common cancer types. At first, a cohort comprised of tissue samples from six adenocarcinoma entities located at different organ sites (esophagus, breast, colon, liver, stomach, thyroid gland, n = 171) was classified using two algorithms for a training and test set. For the test set, Support Vector Machine and Random Forest yielded overall accuracies of 82.74 and 81.18{\%}, respectively. Then, colon cancer liver metastasis samples (n = 19) were introduced into the classification. The liver metastasis samples could be discriminated with high accuracy from primary tumors of colon cancer and hepatocellular carcinoma. Additionally, colon cancer liver metastasis samples could be successfully classified by using colon cancer primary tumor samples for the training of the classifier. These findings demonstrate that MALDI imaging-derived proteomic classifiers can discriminate between different tumor types at different organ sites and in the same site.},
author = {Meding, Stephan and Nitsche, Ulrich and Balluff, Benjamin and Elsner, Mareike and Rauser, Sandra and Sch{\"{o}}ne, C{\'{e}}drik and Nipp, Martin and Maak, Matthias and Feith, Marcus and Ebert, Matthias P and Friess, Helmut and Langer, Rupert and H{\"{o}}fler, Heinz and Zitzelsberger, Horst and Rosenberg, Robert and Walch, Axel},
doi = {10.1021/pr200784p},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meding et al. - 2012 - Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Adenocarcinoma,Adenocarcinoma: metabolism,Adenocarcinoma: secondary,Algorithms,Humans,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Neoplasms: pathology,Proteome,Proteome: metabolism,Proteomics,Sensitivity and Specificity,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Support Vector Machines},
month = {mar},
number = {3},
pages = {1996--2003},
pmid = {22224404},
title = {{Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22224404},
volume = {11},
year = {2012}
}
@article{Hanselmann2008,
abstract = {Imaging mass spectrometry (IMS) is a promising technology which allows for detailed analysis of spatial distributions of (bio)molecules in organic samples. In many current applications, IMS relies heavily on (semi)automated exploratory data analysis procedures to decompose the data into characteristic component spectra and corresponding abundance maps, visualizing spectral and spatial structure. The most commonly used techniques are principal component analysis (PCA) and independent component analysis (ICA). Both methods operate in an unsupervised manner. However, their decomposition estimates usually feature negative counts and are not amenable to direct physical interpretation. We propose probabilistic latent semantic analysis (pLSA) for non-negative decomposition and the elucidation of interpretable component spectra and abundance maps. We compare this algorithm to PCA, ICA, and non-negative PARAFAC (parallel factors analysis) and show on simulated and real-world data that pLSA and non-negative PARAFAC are superior to PCA or ICA in terms of complementarity of the resulting components and reconstruction accuracy. We further combine pLSA decomposition with a statistical complexity estimation scheme based on the Akaike information criterion (AIC) to automatically estimate the number of components present in a tissue sample data set and show that this results in sensible complexity estimates.},
author = {Hanselmann, Michael and Kirchner, Marc and Renard, Bernhard Y and Amstalden, Erika R and Glunde, Kristine and Heeren, Ron M a and Hamprecht, Fred a},
doi = {10.1021/ac801303x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2008 - Concise representation of mass spectrometry images by probabilistic latent semantic analysis.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: pathology,Computer Simulation,Female,Humans,Image Processing, Computer-Assisted,Mass Spectrometry,Principal Component Analysis,Signal Processing, Computer-Assisted},
month = {dec},
number = {24},
pages = {9649--58},
pmid = {18989936},
title = {{Concise representation of mass spectrometry images by probabilistic latent semantic analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18989936},
volume = {80},
year = {2008}
}
@article{Hilario2006,
abstract = {Among the many applications of mass spectrometry, biomarker pattern discovery from protein mass spectra has aroused considerable interest in the past few years. While research efforts have raised hopes of early and less invasive diagnosis, they have also brought to light the many issues to be tackled before mass-spectra-based proteomic patterns become routine clinical tools. Known issues cover the entire pipeline leading from sample collection through mass spectrometry analytics to biomarker pattern extraction, validation, and interpretation. This study focuses on the data-analytical phase, which takes as input mass spectra of biological specimens and discovers patterns of peak masses and intensities that discriminate between different pathological states. We survey current work and investigate computational issues concerning the different stages of the knowledge discovery process: exploratory analysis, quality control, and diverse transforms of mass spectra, followed by further dimensionality reduction, classification, and model evaluation. We conclude after a brief discussion of the critical biomedical task of analyzing discovered discriminatory patterns to identify their component proteins as well as interpret and validate their biological implications.},
author = {Hilario, Melanie and Kalousis, Alexandros and Pellegrini, Christian and M{\"{u}}ller, Markus},
doi = {10.1002/mas.20072},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hilario et al. - 2006 - Processing and classification of protein mass spectra.pdf:pdf},
issn = {0277-7037},
journal = {Mass spectrometry reviews},
keywords = {Algorithms,Animals,Biological Markers,Computational Biology,Humans,Mass Spectrometry,Mass Spectrometry: classification,Mass Spectrometry: methods,Models, Chemical,Peptide Mapping,Proteins,Proteins: analysis,Proteomics},
number = {3},
pages = {409--49},
pmid = {16463283},
title = {{Processing and classification of protein mass spectra.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16463283},
volume = {25},
year = {2006}
}
@inproceedings{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, K},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muandet, Fukumizu - 2012 - Learning from distributions via support measure machines.pdf:pdf},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://arxiv.org/abs/1202.6504},
year = {2012}
}
@inproceedings{Suzuki2008,
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {ACL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Isozaki - 2008 - Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.pdf:pdf},
pages = {665--673},
title = {{Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.5597{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@inproceedings{Sindhwani2005,
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Niyogi, Belkin - 2005 - Beyond the point cloud from transductive to semi-supervised learning.pdf:pdf},
number = {0},
pages = {824--831},
title = {{Beyond the point cloud: from transductive to semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102455},
year = {2005}
}
@inproceedings{Chapelle2002,
author = {Chapelle, Olivier and Weston, Jason and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Weston, Sch{\"{o}}lkopf - 2002 - Cluster kernels for semi-supervised learning.pdf:pdf},
pages = {585--592},
title = {{Cluster kernels for semi-supervised learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA13.pdf},
year = {2002}
}
@article{Astorino2007,
author = {Astorino, A and Fuduli, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Astorino, Fuduli - 2007 - Nonsmooth optimization techniques for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {nonsmooth optimization,semi,supervised learning},
number = {12},
pages = {2135--2142},
title = {{Nonsmooth optimization techniques for Semi-Supervised Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4359288},
volume = {29},
year = {2007}
}
@inproceedings{Cozman2002,
author = {Cozman, FG and Cohen, Ira},
booktitle = {FLAIRS Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen - 2002 - Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.pdf:pdf},
pages = {327--331},
title = {{Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.}},
url = {http://www.aaai.org/Papers/FLAIRS/2002/FLAIRS02-065.pdf},
year = {2002}
}
@inproceedings{Sa1994,
author = {Sa, Virginia R De},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sa - 1994 - Learning Classification with Unlabeled Data.pdf:pdf},
pages = {112--112},
title = {{Learning Classification with Unlabeled Data}},
year = {1994}
}
@inproceedings{Klinkenberg2001,
author = {Klinkenberg, Ralf},
booktitle = {Workshop notes of the IJCAI-01 Workshop on Learning from Temporal and Spatial Data},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klinkenberg - 2001 - Using labeled and unlabeled data to learn drifting concepts.pdf:pdf},
pages = {16--24},
title = {{Using labeled and unlabeled data to learn drifting concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.1798{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@inproceedings{Corduneanu2002,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
booktitle = {Proceedings of the 19th conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Corduneanu, Jaakkola - 2002 - On information regularization.pdf:pdf},
pages = {151--158},
title = {{On information regularization}},
url = {http://dl.acm.org/citation.cfm?id=2100602},
year = {2002}
}
@inproceedings{Szummer2000,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems 13},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2000 - Kernel expansions with unlabeled examples.pdf:pdf},
pages = {626--632},
title = {{Kernel expansions with unlabeled examples}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.222{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Szummer2002,
author = {Szummer, Martin and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2002 - Information regularization with partially labeled data.pdf:pdf},
pages = {1025--1032},
title = {{Information regularization with partially labeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA69.pdf},
year = {2002}
}
@inproceedings{Jaakkola2002,
author = {Jaakkola, MST and Szummer, Martin},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Szummer - 2002 - Partially labeled classification with Markov random walks.pdf:pdf},
pages = {945--952},
title = {{Partially labeled classification with Markov random walks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=GbC8cqxGR7YC{\&}oi=fnd{\&}pg=PA945{\&}dq=Partially+labeled+classification+with+Markov+random+walks{\&}ots=ZvP5J{\_}YBx6{\&}sig=dk27TWzUdp9G-e9OyvfYcGR14ro},
year = {2002}
}
@inproceedings{Szummer2001,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2001 - Clustering and efficient use of unlabeled examples.pdf:pdf},
title = {{Clustering and efficient use of unlabeled examples}},
url = {http://www.ai.mit.edu/projects/ntt/projects/MIT2000-08/documents/SzummerJaakkola.pdf},
year = {2001}
}
@inproceedings{Abney2002,
author = {Abney, Steven},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2002 - Bootstrapping.pdf:pdf},
number = {July},
pages = {360--367},
title = {{Bootstrapping}},
year = {2002}
}
@unpublished{Blum2001,
author = {Blum, Avrim and Chawla, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Chawla - 2001 - Learning from labeled and unlabeled data using graph mincuts.pdf:pdf},
institution = {Carnegie Mellon University, Computer Science Department},
title = {{Learning from labeled and unlabeled data using graph mincuts}},
url = {http://repository.cmu.edu/compsci/163/?utm{\_}source=repository.cmu.edu{\%}2Fcompsci{\%}2F163{\&}utm{\_}medium=PDF{\&}utm{\_}campaign=PDFCoverPages},
year = {2001}
}
@article{Chapelle2006a,
address = {New York, New York, USA},
author = {Chapelle, Olivier and Chi, Mingmin and Zien, Alexander},
doi = {10.1145/1143844.1143868},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Chi, Zien - 2006 - A continuation method for semi-supervised SVMs.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {185--192},
publisher = {ACM Press},
title = {{A continuation method for semi-supervised SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143868},
year = {2006}
}
@article{O'Neill1978,
author = {O'Neill, Terence J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/O'Neill - 1978 - Normal discrimination with unclassified observations.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {unclassified},
number = {364},
pages = {821--826},
title = {{Normal discrimination with unclassified observations}},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480106},
volume = {73},
year = {1978}
}
@article{Poggio2004,
abstract = {Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.},
author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
doi = {10.1038/nature02341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2004 - General conditions for predictivity in learning theory.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Algorithms,Intelligence,Language,Learning,Learning: physiology,Models, Theoretical,Probability,Research Design},
month = {mar},
number = {6981},
pages = {419--22},
pmid = {15042089},
title = {{General conditions for predictivity in learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15042089},
volume = {428},
year = {2004}
}
@article{Loogc,
author = {Loog, M. and van Ginneken, B.},
doi = {10.1109/ICPR.2002.1048456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, van Ginneken - Unknown - Supervised segmentation by iterated contextual pixel classification.pdf:pdf},
isbn = {0-7695-1695-X},
journal = {Object recognition supported by user interaction for service robots},
pages = {925--928},
publisher = {IEEE Comput. Soc},
title = {{Supervised segmentation by iterated contextual pixel classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1048456},
volume = {2}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1111/j.1467-9868.2010.00740.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meinshausen, B{\"{u}}hlmann - 2010 - Stability selection.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
month = {jul},
number = {4},
pages = {417--473},
title = {{Stability selection}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2010.00740.x},
volume = {72},
year = {2010}
}
@article{Hoogerbrugge1983,
author = {Hoogerbrugge, Ronald and Willig, Simon J. and Kistemaker, Piet G.},
doi = {10.1021/ac00261a016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoogerbrugge, Willig, Kistemaker - 1983 - Discriminant analysis by double stage principal component analysis.pdf:pdf},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = {sep},
number = {11},
pages = {1710--1712},
title = {{Discriminant analysis by double stage principal component analysis}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00261a016},
volume = {55},
year = {1983}
}
@article{Stigler2013,
author = {Stigler, Stephen M.},
doi = {10.1214/13-STS438},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stigler - 2013 - The True Title of Bayes's Essay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Thomas Bayes, Richard Price, Bayes's theorem, hist,and phrases},
month = {aug},
number = {3},
pages = {283--288},
title = {{The True Title of Bayes's Essay}},
url = {http://projecteuclid.org/euclid.ss/1377696937},
volume = {28},
year = {2013}
}
@inproceedings{Zhang2000a,
author = {Zhang, Tong and Oles, Frank J.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Oles - 2000 - A Probability Analysis on the Value of Unlabeled Data for Classification Problems.pdf:pdf},
pages = {1191--1198},
title = {{A Probability Analysis on the Value of Unlabeled Data for Classification Problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@article{Wang2009b,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2009 - On efficient large margin semisupervised learning Method and theory.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {classification,difference convex programming,nonconvex minimization,regulariza-,support vectors,tion},
pages = {719--742},
title = {{On efficient large margin semisupervised learning: Method and theory}},
url = {http://dl.acm.org/citation.cfm?id=1577094},
volume = {10},
year = {2009}
}
@article{Harville2013,
author = {Approach, Nondenominational Model-based and Harville, David A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Approach, Harville - 2013 - The Need for More Emphasis on Prediction a âNondenominationalâ Model-Based Approach.pdf:pdf},
journal = {The American Statistician},
month = {sep},
number = {September},
title = {{The Need for More Emphasis on Prediction: a âNondenominationalâ Model-Based Approach}},
year = {2013}
}
@inproceedings{Bottou2010,
author = {Bottou, Leon},
booktitle = {Proceedings of COMPSTAT'2010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2010 - Large-scale machine learning with stochastic gradient descent.pdf:pdf},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Springer},
title = {{Large-scale machine learning with stochastic gradient descent}},
year = {2010}
}
@article{Geyer1994,
author = {Geyer, Charles J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geyer - 1994 - On the asymptotics of constrained M-estimation.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1993--2010},
title = {{On the asymptotics of constrained M-estimation}},
url = {http://www.jstor.org/stable/2242495},
volume = {22},
year = {1994}
}
@article{Culp2008b,
abstract = {Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers, and therefore a semi-supervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced, and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.},
author = {Culp, Mark and Michailidis, George},
doi = {10.1109/TPAMI.2007.70765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - Graph-based semisupervised learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer Simulation,Data Interpretation,Models,Pattern Recognition,Statistical},
month = {jan},
number = {1},
pages = {174--9},
pmid = {18000333},
title = {{Graph-based semisupervised learning.}},
volume = {30},
year = {2008}
}
@article{Kent1996,
author = {Kent, John T. and Tyler, David E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kent, Tyler - 1996 - Constrained M-estimation for Multivariate Location and Scatter.pdf:pdf},
journal = {Annals of statistics},
number = {January 1994},
pages = {1346--1370},
title = {{Constrained M-estimation for Multivariate Location and Scatter}},
year = {1996}
}
@inproceedings{Fan2008,
author = {Fan, Bin and Lei, Zhen and Li, Stan Z.},
booktitle = {8th IEEE International Conference on Automatic Face {\&} Gesture Recognition},
doi = {10.1109/AFGR.2008.4813329},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Lei, Li - 2008 - Normalized LDA for Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-2153-4},
month = {sep},
pages = {1--6},
title = {{Normalized LDA for Semi-supervised Learning}},
year = {2008}
}
@article{Huang1998,
author = {Huang, Jianhua Z.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,anova,curse of dimensionality,finite elements,interaction,least},
number = {1},
pages = {242--272},
title = {{Projection estimation in multiple regression with application to functional ANOVA models}},
url = {http://projecteuclid.org/euclid.aos/1030563984},
volume = {26},
year = {1998}
}
@book{Aubin2000,
author = {Aubin, Jean-Pierre},
edition = {Second},
isbn = {9780471179764},
publisher = {John Wiley {\&} Sons},
title = {{Applied functional analysis}},
year = {2000}
}
@article{Jordan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1309.7804v1},
author = {Jordan, Michael I.},
doi = {10.3150/12-BEJSP17},
eprint = {arXiv:1309.7804v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan - 2013 - On statistics, computation and scalability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {()},
month = {sep},
number = {4},
pages = {1378--1390},
title = {{On statistics, computation and scalability}},
url = {http://projecteuclid.org/euclid.bj/1377612856},
volume = {19},
year = {2013}
}
@article{Jager2013a,
abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported {\$}P{\$}-values as the data. We then collect {\$}P{\$}-values from the abstracts of all 77â430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported {\$}P{\$}-values. We estimate that the overall rate of false discoveries among reported results is 14{\%} (s.d. 1{\%}), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5{\%} more false positives (FP) per year, {\$}P = 0.18{\$}) or with respect to journal submissions (0.5{\%} more FP per 100 submissions, {\$}P = 0.12{\$}). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
author = {Jager, Leah R and Leek, Jeffrey T},
doi = {10.1093/biostatistics/kxt007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jager, Leek - 2013 - An estimate of the science-wise false discovery rate and application to the top medical literature.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {false discovery rate,genomics,meta-analysis,multiple testing,science-wise false discovery rate},
month = {sep},
pages = {1--12},
pmid = {24068246},
title = {{An estimate of the science-wise false discovery rate and application to the top medical literature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24068246},
year = {2013}
}
@article{Macia2013,
author = {Maci{\`{a}}, N{\'{u}}ria and Bernad{\'{o}}-Mansilla, Ester},
doi = {10.1016/j.ins.2013.08.059},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maci{\`{a}}, Bernad{\'{o}}-Mansilla - 2013 - Towards UCI A mindful repository design.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
month = {sep},
title = {{Towards UCI+: A mindful repository design}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
year = {2013}
}
@article{Wasserman2009,
author = {Wasserman, Larry and Roeder, Kathryn},
doi = {10.1214/08-AOS646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman, Roeder - 2009 - High dimensional variable selection.pdf:pdf},
journal = {Annals of statistics},
number = {5},
pages = {2178--2201},
title = {{High dimensional variable selection}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2752029/},
volume = {37},
year = {2009}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2004 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Entropy,Linear Models,Models, Statistical,Normal Distribution,Regression Analysis,Statistics, Nonparametric},
month = {apr},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@book{Berger1985,
author = {Berger, James O},
publisher = {Springer},
title = {{Statistical decision theory and Bayesian analysis}},
year = {1985}
}
@inproceedings{Widrow1960,
author = {Widrow, Bernard and Hoff, Marcian E.},
booktitle = {IRE WESCON Convention Record 4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Widrow, Hoff - 1960 - Adaptive switching circuits.pdf:pdf},
pages = {96--104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@inproceedings{Ben-David2012,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David et al. - 2012 - Minimizing the misclassification error rate using a surrogate convex loss.pdf:pdf},
pages = {1863--1870},
title = {{Minimizing the misclassification error rate using a surrogate convex loss}},
year = {2012}
}
@article{JuliaFlores2013,
author = {{Julia Flores}, M. and G{\'{a}}mez, Jos{\'{e}} a. and Mart{\'{i}}nez, Ana M.},
doi = {10.1016/j.ins.2013.10.007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Julia Flores, G{\'{a}}mez, Mart{\'{i}}nez - 2013 - Domains of competence of the semi-naive Bayesian network classifiers.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {domains of competence,semi-naive bayesian network classifiers},
month = {oct},
number = {October},
publisher = {Elsevier Inc.},
title = {{Domains of competence of the semi-naive Bayesian network classifiers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513007226},
year = {2013}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baraniuk - 2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4286571 http://omni.isr.ist.utl.pt/{~}aguiar/CS{\_}notes.pdf},
year = {2007}
}
@article{Loog2014a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2014 - Semi-supervised linear discriminant analysis through moment-constraint parameter estimation.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {linear discriminant analysis,semi-supervised learning},
month = {mar},
pages = {24--31},
publisher = {Elsevier B.V.},
title = {{Semi-supervised linear discriminant analysis through moment-constraint parameter estimation}},
volume = {37},
year = {2014}
}
@article{Sejdinovic2013,
author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
doi = {10.1214/13-AOS1140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based statistics in hypothesis testing.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2263--2291},
title = {{Equivalence of distance-based and RKHS-based statistics in hypothesis testing}},
url = {http://projecteuclid.org/euclid.aos/1383661264},
volume = {41},
year = {2013}
}
@article{Pitt1988,
author = {Pitt, Leonard and Valiant, Leslie G.},
doi = {10.1145/48014.63140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitt, Valiant - 1988 - Computational limitations on learning from examples.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = {oct},
number = {4},
pages = {965--984},
title = {{Computational limitations on learning from examples}},
url = {http://portal.acm.org/citation.cfm?doid=48014.63140},
volume = {35},
year = {1988}
}
@article{Janzing2013,
author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1214/13-AOS1145},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing et al. - 2013 - Quantifying causal influences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {oct},
number = {5},
pages = {2324--2358},
title = {{Quantifying causal influences}},
url = {http://projecteuclid.org/euclid.aos/1383661266},
volume = {41},
year = {2013}
}
@article{Devroye1982,
abstract = {Consider the basic discrimination problem based on a sample of size n drawn from the distribution of (X, Y) on the Borel sets of Rdx {\{}O, 1{\}}. If 0 {\textless} R*{\textless} is a given number, and 'n - 0 is an arbitrary positive sequence, then for any discrimination rule one can find a distribution for (X, Y), not depending upon n, with Bayes probability of error R* such that the probability of error (Rn) of the discrimination rule is larger than R* + 'On for infinitely many n. We give a formal proof of this result, which is a generalization of a result by Cover [1].},
author = {Devroye, L},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Devroye - 1982 - Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {feb},
number = {2},
pages = {154--7},
pmid = {21869021},
title = {{Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869021},
volume = {4},
year = {1982}
}
@article{Weston2006,
address = {New York, New York, USA},
author = {Weston, Jason and Collobert, Ronan and Sinz, Fabian and Bottou, L{\'{e}}on and Vapnik, Vladimir},
doi = {10.1145/1143844.1143971},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2006 - Inference with the Universum.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {1009--1016},
publisher = {ACM Press},
title = {{Inference with the Universum}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143971},
year = {2006}
}
@article{Browner1987,
abstract = {Just as diagnostic tests are most helpful in light of the clinical presentation, statistical tests are most useful in the context of scientific knowledge. Knowing the specificity and sensitivity of a diagnostic test is necessary, but insufficient: the clinician must also estimate the prior probability of the disease. In the same way, knowing the P value and power, or the confidence interval, for the results of a research study is necessary but insufficient: the reader must estimate the prior probability that the research hypothesis is true. Just as a positive diagnostic test does not mean that a patient has the disease, especially if the clinical picture suggests otherwise, a significant P value does not mean that a research hypothesis is correct, especially if it is inconsistent with current knowledge. Powerful studies are like sensitive tests in that they can be especially useful when the results are negative. Very low P values are like very specific tests; both result in few false-positive results due to chance. This Bayesian approach can clarify much of the confusion surrounding the use and interpretation of statistical tests.},
author = {Browner, W S and Newman, T B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Browner, Newman - 1987 - Are all significant P values created equal The analogy between diagnostic tests and clinical research.pdf:pdf},
issn = {0098-7484},
journal = {JAMA : the journal of the American Medical Association},
keywords = {Bayes Theorem,Predictive Value of Tests,Research,Statistics as Topic},
month = {may},
number = {18},
pages = {2459--63},
pmid = {3573245},
title = {{Are all significant P values created equal? The analogy between diagnostic tests and clinical research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3573245},
volume = {257},
year = {1987}
}
@article{Jones2011,
abstract = {MALDI mass spectrometry can generate profiles that contain hundreds of biomolecular ions directly from tissue. Spatially-correlated analysis, MALDI imaging MS, can simultaneously reveal how each of these biomolecular ions varies in clinical tissue samples. The use of statistical data analysis tools to identify regions containing correlated mass spectrometry profiles is referred to as imaging MS-based molecular histology because of its ability to annotate tissues solely on the basis of the imaging MS data. Several reports have indicated that imaging MS-based molecular histology may be able to complement established histological and histochemical techniques by distinguishing between pathologies with overlapping/identical morphologies and revealing biomolecular intratumor heterogeneity. A data analysis pipeline that identifies regions of imaging MS datasets with correlated mass spectrometry profiles could lead to the development of novel methods for improved diagnosis (differentiating subgroups within distinct histological groups) and annotating the spatio-chemical makeup of tumors. Here it is demonstrated that highlighting the regions within imaging MS datasets whose mass spectrometry profiles were found to be correlated by five independent multivariate methods provides a consistently accurate summary of the spatio-chemical heterogeneity. The corroboration provided by using multiple multivariate methods, efficiently applied in an automated routine, provides assurance that the identified regions are indeed characterized by distinct mass spectrometry profiles, a crucial requirement for its development as a complementary histological tool. When simultaneously applied to imaging MS datasets from multiple patient samples of intermediate-grade myxofibrosarcoma, a heterogeneous soft tissue sarcoma, nodules with mass spectrometry profiles found to be distinct by five different multivariate methods were detected within morphologically identical regions of all patient tissue samples. To aid the further development of imaging MS based molecular histology as a complementary histological tool the Matlab code of the agreement analysis, instructions and a reduced dataset are included as supporting information.},
author = {Jones, Emrys a and van Remoortere, Alexandra and van Zeijl, Ren{\'{e}} J M and Hogendoorn, Pancras C W and Bov{\'{e}}e, Judith V M G and Deelder, Andr{\'{e}} M and McDonnell, Liam a},
doi = {10.1371/journal.pone.0024913},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2011 - Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry dataset.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Databases, Factual,Diagnostic Imaging,Diagnostic Imaging: methods,Fibroma,Fibroma: metabolism,Fibrosarcoma,Fibrosarcoma: metabolism,Gene Expression Regulation, Neoplastic,Humans,Ions,Ions: chemistry,Models, Statistical,Molecular Imaging,Molecular Imaging: methods,Multivariate Analysis,Peptides,Peptides: chemistry,Proteins,Proteins: chemistry,Sarcoma,Sarcoma: metabolism,Software,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = {jan},
number = {9},
pages = {e24913},
pmid = {21980364},
title = {{Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry datasets of myxofibrosarcoma.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3183001{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {6},
year = {2011}
}
@article{Zhao2013a,
author = {Zhao, Junlong and Leng, Chenlei and Li, Lexin and Wang, Hansheng},
doi = {10.1214/13-AOS1165},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - High-dimensional influence measure.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2639--2667},
title = {{High-dimensional influence measure}},
url = {http://projecteuclid.org/euclid.aos/1384871348},
volume = {41},
year = {2013}
}
@article{Horton2007,
abstract = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Development of statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be utilized in practice.},
author = {Horton, Nicholas J and Kleinman, Ken P},
doi = {10.1198/000313007X172556},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Horton, Kleinman - 2007 - Much ado about nothing A comparison of missing data methods and software to fit incomplete data regression mod.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American statistician},
keywords = {conditional gaussian,health services research,maximum likelihood,multiple imputation,psychiatric epidemi-},
month = {feb},
number = {1},
pages = {79--90},
pmid = {17401454},
title = {{Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839993{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {61},
year = {2007}
}
@article{Mclachlan1977,
author = {McLachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1977 - Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observatio.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {358},
pages = {403--406},
title = {{Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations}},
volume = {72},
year = {1977}
}
@inproceedings{Rohrbach2013,
author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rohrbach, Ebert, Schiele - 2013 - Transfer Learning in a Transductive Setting.pdf:pdf},
pages = {46--54},
title = {{Transfer Learning in a Transductive Setting}},
url = {http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting},
year = {2013}
}
@article{Zhang2013a,
author = {Zhang, Q. and Qian, P. Z. G.},
doi = {10.1093/biomet/ast034},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Qian - 2013 - Designs for crossvalidating approximation models.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {sep},
number = {4},
pages = {997--1004},
title = {{Designs for crossvalidating approximation models}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast034},
volume = {100},
year = {2013}
}
@article{Zach2014,
abstract = {In this work, we present a unified view on Markov random fields (MRFs) and recently proposed continuous tight convex relaxations for multilabel assignment in the image plane. These relaxations are far less biased toward the grid geometry than Markov random fields on grids. It turns out that the continuous methods are nonlinear extensions of the well-established local polytope MRF relaxation. In view of this result, a better understanding of these tight convex relaxations in the discrete setting is obtained. Further, a wider range of optimization methods is now applicable to find a minimizer of the tight formulation. We propose two methods to improve the efficiency of minimization. One uses a weaker, but more efficient continuously inspired approach as initialization and gradually refines the energy where it is necessary. The other one reformulates the dual energy enabling smooth approximations to be used for efficient optimization. We demonstrate the utility of our proposed minimization schemes in numerical experiments. Finally, we generalize the underlying energy formulation from isotropic metric smoothness costs to arbitrary nonmetric and orientation dependent smoothness terms.},
author = {Zach, Christopher and H{\"{a}}ne, Christian and Pollefeys, Marc},
doi = {10.1109/TPAMI.2013.105},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zach, H{\"{a}}ne, Pollefeys - 2014 - What Is Optimized in Convex Relaxations for Multilabel Problems Connecting Discrete and Continuously Ins.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {jan},
number = {1},
pages = {157--70},
pmid = {24231873},
title = {{What Is Optimized in Convex Relaxations for Multilabel Problems: Connecting Discrete and Continuously Inspired MAP Inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24231873},
volume = {36},
year = {2014}
}
@article{Johnson2013,
author = {Johnson, Valen E.},
doi = {10.1214/13-AOS1123},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - 2013 - Uniformly most powerful Bayesian tests.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {bayes factor,higgs,ily model,jeffreys-lindley paradox,neyman-pearson lemma,non-local prior density,objective bayes,one-parameter exponential fam-,uniformly most powerful test},
month = {aug},
number = {4},
pages = {1716--1741},
title = {{Uniformly most powerful Bayesian tests}},
url = {http://projecteuclid.org/euclid.aos/1378386237},
volume = {41},
year = {2013}
}
@book{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bertsekas - 1982 - Constrained optimization and Lagrange multiplier methods.pdf:pdf},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
url = {http://adsabs.harvard.edu/abs/1982colm.book.....b},
year = {1982}
}
@article{Kappen2013,
author = {Kappen, Hilbert J. and G{\'{o}}mez, Vicen{\c{c}}},
doi = {10.1007/s10994-013-5427-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kappen, G{\'{o}}mez - 2013 - The Variational Garrote.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {mean-field,sparse regression,spike-and-slab,variational approximation},
month = {dec},
number = {January 2012},
title = {{The Variational Garrote}},
url = {http://link.springer.com/10.1007/s10994-013-5427-7},
year = {2013}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {incomplete,likelihood,maximum},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
volume = {39},
year = {1977}
}
@article{Weston2005,
abstract = {MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
author = {Weston, Jason and Leslie, Christina and Ie, Eugene and Zhou, Dengyong and Elisseeff, Andre and Noble, William Stafford},
doi = {10.1093/bioinformatics/bti497},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2005 - Semi-supervised protein classification using cluster kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Pattern Recognition,Protein,Protein: methods,Proteins,Proteins: analysis,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = {aug},
number = {15},
pages = {3241--7},
pmid = {15905279},
title = {{Semi-supervised protein classification using cluster kernels.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905279},
volume = {21},
year = {2005}
}
@unpublished{Savov,
author = {Savov, Ivan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Savov - Unknown - Linear algebra explained in four pages.pdf:pdf},
pages = {1--4},
title = {{Linear algebra explained in four pages}}
}
@inproceedings{Ho1996,
author = {Ho, Tin Kam and Kleinberg, Eeugene M.},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Kleinberg - 1996 - Building projectable classifiers of arbitrary complexity.pdf:pdf},
pages = {880--885},
title = {{Building projectable classifiers of arbitrary complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=547202},
volume = {2},
year = {1996}
}
@article{Bottou2013,
author = {Bottou, L{\'{e}}on},
doi = {10.1007/s10994-013-5335-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2013 - From machine learning to machine reasoning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {machine learning,reasoning,recursive networks},
month = {apr},
pages = {133--149},
title = {{From machine learning to machine reasoning}},
url = {http://link.springer.com/10.1007/s10994-013-5335-x},
year = {2013}
}
@article{Rasmussen2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2005}
}
@article{Jaffe2013,
abstract = {BACKGROUND: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features associated with illness. We propose a new approach, called gene set bagging, for measuring the probability that a gene set replicates in future studies. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate in the bagged samples.

RESULTS: Using both simulated and publicly-available genomics data, we demonstrate that significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. We show our method estimates the replication probability (R), the probability that a gene set will replicate as a significant result in future studies, and show in simulations that this method reflects replication better than each set's p-value.

CONCLUSIONS: Our results suggest that gene lists based on p-values are not necessarily stable, and therefore additional steps like gene set bagging may improve biological inference on gene sets.},
author = {Jaffe, Andrew E and Storey, John D and Ji, Hongkai and Leek, Jeffrey T},
doi = {10.1186/1471-2105-14-360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaffe et al. - 2013 - Gene set bagging for estimating the probability a statistically significant result will replicate.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {dna methylation,gene expression,gene ontology,gene set enrichment analysis},
month = {jan},
pages = {360},
pmid = {24330332},
title = {{Gene set bagging for estimating the probability a statistically significant result will replicate.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3890500{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {14},
year = {2013}
}
@article{Leistner2009,
author = {Leistner, Christian and Saffari, Amir and Santner, Jakob and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459198},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leistner et al. - 2009 - Semi-Supervised Random Forests.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = {sep},
pages = {506--513},
publisher = {Ieee},
title = {{Semi-Supervised Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459198},
year = {2009}
}
@article{Geer2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0910.0722v1},
author = {Geer, Sara Van De and B{\"{u}}hlmann, Peter},
eprint = {arXiv:0910.0722v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geer, B{\"{u}}hlmann - 2009 - On the conditions used to prove oracle results for the Lasso.pdf:pdf},
journal = {Electronic Journal of Statistics},
keywords = {and phrases,coherence,compatibility,irrepresentable condition,lasso,re-,restricted isometry,sparsity,stricted eigenvalue},
pages = {1--33},
title = {{On the conditions used to prove oracle results for the Lasso}},
url = {http://projecteuclid.org/euclid.ejs/1260801227},
year = {2009}
}
@article{He2010,
abstract = {Feature selection techniques have been used as the workhorse in biomarker discovery applications for a long time. Surprisingly, the stability of feature selection with respect to sampling variations has long been under-considered. It is only until recently that this issue has received more and more attention. In this article, we review existing stable feature selection methods for biomarker discovery using a generic hierarchical framework. We have two objectives: (1) providing an overview on this new yet fast growing topic for a convenient reference; (2) categorizing existing methods under an expandable framework for future research and development.},
author = {He, Zengyou and Yu, Weichuan},
doi = {10.1016/j.compbiolchem.2010.07.002},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/He, Yu - 2010 - Stable feature selection for biomarker discovery.pdf:pdf},
issn = {1476-928X},
journal = {Computational biology and chemistry},
keywords = {Animals,Artificial Intelligence,Biological Markers,Biological Markers: analysis,Biological Markers: metabolism,Computational Biology,Computational Biology: methods,Humans},
month = {aug},
number = {4},
pages = {215--25},
pmid = {20702140},
publisher = {Elsevier Ltd},
title = {{Stable feature selection for biomarker discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20702140},
volume = {34},
year = {2010}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halevy, Norvig, Pereira - 2009 - The Unreasonable Effectiveness of Data.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
month = {mar},
number = {2},
pages = {8--12},
title = {{The Unreasonable Effectiveness of Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804817},
volume = {24},
year = {2009}
}
@unpublished{Liu2014,
author = {Liu, Mingxia and Zhang, Daoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Zhang - 2014 - CGS A Novel Pairwise Constraint-Guided Sparse Feature Selection Method.pdf:pdf},
title = {{CGS: A Novel Pairwise Constraint-Guided Sparse Feature Selection Method}},
year = {2014}
}
@inproceedings{White2012,
author = {White, Martha and Schuurmans, Dale},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White, Schuurmans - 2012 - Generalized optimal reverse prediction.pdf:pdf},
pages = {1305--1313},
title = {{Generalized optimal reverse prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}WhiteS12.pdf},
year = {2012}
}
@article{Nock2009,
abstract = {Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same "master" algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.},
author = {Nock, Richard and Nielsen, Frank},
doi = {10.1109/TPAMI.2008.225},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nock, Nielsen - 2009 - Bregman divergences and surrogates for learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Decision Support Techniques,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {nov},
number = {11},
pages = {2048--59},
pmid = {19762930},
title = {{Bregman divergences and surrogates for learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19762930},
volume = {31},
year = {2009}
}
@unpublished{Tan2013,
author = {Tan, Yimin and Zhu, X},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tan, Zhu - 2013 - Dragging Density-Ratio Bagging.pdf:pdf},
pages = {1--10},
title = {{Dragging: Density-Ratio Bagging}},
url = {https://minds.wisconsin.edu/bitstream/handle/1793/65831/TR1795.pdf?sequence=1},
year = {2013}
}
@article{Bottou2012,
author = {Bottou, L{\'{e}}on},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@inproceedings{Collobert2006a,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://dl.acm.org/citation.cfm?id=1143870},
year = {2006}
}
@inproceedings{Herbrich2006a,
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
title = {{TrueSkill: A Bayesian Skill Rating System}},
year = {2006}
}
@article{Cesa-Bianchi2007,
author = {Cesa-Bianchi, Nicol{\`{o}}},
doi = {10.1016/j.tcs.2007.03.053},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cesa-Bianchi - 2007 - Applications of regularized least squares to pattern classification.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {on-line learning,perceptron,ridge regression,selective sampling},
month = {sep},
number = {3},
pages = {221--231},
title = {{Applications of regularized least squares to pattern classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030439750700237X},
volume = {382},
year = {2007}
}
@article{Xu2009,
address = {New York, New York, USA},
author = {Xu, Linli and White, Martha and Schuurmans, Dale},
doi = {10.1145/1553374.1553519},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, White, Schuurmans - 2009 - Optimal reverse prediction.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Optimal reverse prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553519},
year = {2009}
}
@inproceedings{Yu2013,
author = {Yu, Felix X. and Liu, Dong and Kumar, Sanjiv and Jebara, Tony and Chang, Shih-Fu},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu et al. - 2013 - {\$}{\$}backslash{\$} propto {\$} SVM for Learning with Label Proportions.pdf:pdf},
pages = {504--512},
title = {{{\$}{\$}$\backslash$backslash{\$} propto {\$} SVM for Learning with Label Proportions}},
year = {2013}
}
@article{Chen,
archivePrefix = {arXiv},
arxivId = {arXiv:1104.2930v3},
author = {Chen, Aiyou and Jordan, Michael I},
eprint = {arXiv:1104.2930v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Jordan - Unknown - Cluster Forests.pdf:pdf},
pages = {1--23},
title = {{Cluster Forests}}
}
@inproceedings{Wager2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.1493v2},
author = {Wager, Stefan and Wang, Sida and Liang, Percy},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1307.1493v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wager, Wang, Liang - 2013 - Dropout training as adaptive regularization.pdf:pdf},
pages = {351--359},
title = {{Dropout training as adaptive regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
year = {2013}
}
@article{Eddelbuettel2014,
author = {Eddelbuettel, Dirk and Sanderson, Conrad},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Eddelbuettel, Sanderson - 2014 - Rcpparmadillo Accelerating R with high-performance C linear algebra.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {c,linear algebra,r,software},
pages = {1054--1063},
title = {{Rcpparmadillo: Accelerating R with high-performance C++ linear algebra}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000492},
volume = {71},
year = {2014}
}
@article{Mirowski2008,
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
doi = {10.1109/MLSP.2008.4685487},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mirowski et al. - 2008 - Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG.pdf:pdf},
isbn = {978-1-4244-2375-0},
journal = {2008 IEEE Workshop on Machine Learning for Signal Processing},
month = {oct},
pages = {244--249},
publisher = {Ieee},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4685487},
year = {2008}
}
@article{Meier2008,
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1111/j.1467-9868.2007.00627.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meier, Van De Geer, B{\"{u}}hlmann - 2008 - The group lasso for logistic regression.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {categorical data,co-ordinate descent algorithm,dna splice site,group variable,high dimensional generalized linear,model,penalized likelihood,selection},
month = {jan},
number = {1},
pages = {53--71},
title = {{The group lasso for logistic regression}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
volume = {70},
year = {2008}
}
@inproceedings{Graepel2013,
author = {Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
booktitle = {Information Security and Cryptology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graepel, Lauter, Naehrig - 2013 - ML confidential Machine learning on encrypted data.pdf:pdf},
title = {{ML confidential: Machine learning on encrypted data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37682-5{\_}1},
year = {2013}
}
@article{Liu2013,
author = {Liu, J. and Gelman, Andrew and Hill, J. and Su, Y.-S. and Kropko, J.},
doi = {10.1093/biomet/ast044},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2013 - On the stationary distribution of iterative imputations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {nov},
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast044},
volume = {101},
year = {2013}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Courville, Vincent - 2013 - Representation learning a review and new perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
month = {aug},
number = {8},
pages = {1798--828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Ranzato, Marc Aurelio and Aviv, Tel and Park, Menlo},
booktitle = {Computer Vision and Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
title = {{DeepFace : Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@article{Homrighausen2014,
author = {Homrighausen, Darren and McDonald, Daniel J.},
doi = {10.1007/s10994-014-5438-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Homrighausen, McDonald - 2014 - Leave-one-out cross-validation is risk consistent for lasso.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {persistence,stochastic equicontinuity,uniform convergence},
month = {mar},
number = {March 2013},
title = {{Leave-one-out cross-validation is risk consistent for lasso}},
url = {http://link.springer.com/10.1007/s10994-014-5438-z},
year = {2014}
}
@article{Dimitroff2014,
author = {Dimitroff, Georgi and Georgiev, Georgi and ToloÅi, Laura and Popov, Borislav},
doi = {10.1007/s10994-014-5439-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dimitroff et al. - 2014 - Efficient {\$}{\$}F{\$}{\$} F measure maximization via weighted maximum likelihood.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {apr},
title = {{Efficient {\$}{\$}F{\$}{\$} F measure maximization via weighted maximum likelihood}},
url = {http://link.springer.com/10.1007/s10994-014-5439-y},
year = {2014}
}
@article{Gneiting2007,
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
month = {mar},
number = {477},
pages = {359--378},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
volume = {102},
year = {2007}
}
@article{Gratiet2014,
author = {Gratiet, Loic and Garnier, Josselin},
doi = {10.1007/s10994-014-5437-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gratiet, Garnier - 2014 - Asymptotic analysis of the learning curve for Gaussian process regression.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {asymptotic mean squared error,convergence rate,gaussian process regression,generalization error,learning curves},
month = {mar},
title = {{Asymptotic analysis of the learning curve for Gaussian process regression}},
url = {http://link.springer.com/10.1007/s10994-014-5437-0},
year = {2014}
}
@inproceedings{Krijthe2014,
address = {Stockholm},
author = {Krijthe, Jesse H. and Loog, Marco},
booktitle = {International Conference on Pattern Recognition},
pages = {3762--3767},
title = {{Implicitly Constrained Semi-Supervised Linear Discriminant Analysis}},
year = {2014}
}
@incollection{Wiering,
author = {Wiering, Marco A. and Schomaker, Lambert R.B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wiering, Schomaker - Unknown - Multi-Layer Support Vector Machines.pdf:pdf},
title = {{Multi-Layer Support Vector Machines}},
url = {http://www.ai.rug.nl/{~}mwiering/GROUP/ARTICLES/Multi-Layer-SVM.pdf}
}
@article{Buhlmann2014,
author = {B{\"{u}}hlmann, Peter},
doi = {10.1214/13-STS460},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/B{\"{u}}hlmann - 2014 - Discussion of Big Bayes Stories and BayesBag.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
month = {feb},
number = {1},
pages = {91--94},
title = {{Discussion of Big Bayes Stories and BayesBag}},
url = {http://projecteuclid.org/euclid.ss/1399645732},
volume = {29},
year = {2014}
}
@article{Efron2004,
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron et al. - 2004 - Least Angle Regression.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
number = {2},
pages = {407--499},
title = {{Least Angle Regression}},
url = {http://projecteuclid.org/euclid.aos/1083178935},
volume = {32},
year = {2004}
}
@article{Hand2014,
author = {Hand, David J.},
doi = {10.1214/13-STS446},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2014 - Wonderful Examples, but Let's not Close Our Eyes.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Frequentist, likelihood inference, Neyman-Pearson,and phrases,frequentist,hypothesis testing,informative and thought-provoking,likelihood inference,making specific comments,neyman,on each of these,pearson,schools of inference,space prohibits me from},
month = {feb},
number = {1},
pages = {98--100},
title = {{Wonderful Examples, but Let's not Close Our Eyes}},
url = {http://projecteuclid.org/euclid.ss/1399645735},
volume = {29},
year = {2014}
}
@article{Huang2013,
abstract = {Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.},
author = {Huang, Xiaolin and Shi, Lei and Suykens, Johan a K},
doi = {D7CF84C8-DF7E-492B-A669-0B4CFDEE5D3D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Shi, Suykens - 2013 - Support Vector Machine Classifier with Pinball Loss.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {sep},
number = {5},
pages = {984--997},
pmid = {24062537},
title = {{Support Vector Machine Classifier with Pinball Loss.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24062537},
volume = {36},
year = {2013}
}
@article{Raftery2014a,
author = {Raftery, Adrian E. and Alkema, Leontine and Gerland, Patrick},
doi = {10.1214/13-STS419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raftery, Alkema, Gerland - 2014 - Bayesian Population Projections for the United Nations.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian hierarchical model, cohort component proj,and phrases,bayesian hierarchical model,cohort component,double logistic function,leslie matrix,life expectancy,projection method,total fertility rate},
month = {feb},
number = {1},
pages = {58--68},
title = {{Bayesian Population Projections for the United Nations}},
url = {http://projecteuclid.org/euclid.ss/1399645729},
volume = {29},
year = {2014}
}
@article{Palazzo2014,
author = {Palazzo, Alexander F. and Gregory, T. Ryan},
doi = {10.1371/journal.pgen.1004351},
editor = {Akey, Joshua M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Palazzo, Gregory - 2014 - The Case for Junk DNA.pdf:pdf},
issn = {1553-7404},
journal = {PLoS Genetics},
month = {may},
number = {5},
pages = {e1004351},
title = {{The Case for Junk DNA}},
url = {http://dx.plos.org/10.1371/journal.pgen.1004351},
volume = {10},
year = {2014}
}
@inproceedings{Miguel2014,
author = {Carreira-Perpinan, Miguel A. and Wang, Weiran},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carreira-Perpinan, Wang - 2014 - Distributed Optimization of Deeply Nested Systems.pdf:pdf},
pages = {10--19},
title = {{Distributed Optimization of Deeply Nested Systems}},
year = {2014}
}
@book{Lehmann1998,
author = {Lehmann, E. L. and Casella, G.},
publisher = {Springer-Verlag},
title = {{Theory of Point Estimation}},
year = {1998}
}
@article{Loog2014b,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2014 - Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {5},
pages = {995 -- 1006},
title = {{Semi-Supervised Nearest Mean Classification through a constrained Log-Likelihood}},
volume = {26},
year = {2014}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zeiler, Fergus - 2013 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2013}
}
@article{Buhlmann2014a,
author = {Buhlmann, Peter and Meier, Lukas and van de Geer, Sara},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Meier, van de Geer - 2014 - Discussion âa significance test for the lassoâ.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {469--477},
title = {{Discussion: âa significance test for the lassoâ}},
volume = {42},
year = {2014}
}
@article{Henmi2004,
author = {Henmi, Masayuki and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Eguchi - 2004 - A paradox concerning nuisance parameters and projected estimating functions.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {929--941},
title = {{A paradox concerning nuisance parameters and projected estimating functions}},
volume = {91},
year = {2004}
}
@article{Buja2014,
author = {Buja, A and Brown, L},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buja, Brown - 2014 - Discussion of A significance test for the lasso''.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {509--517},
title = {{Discussion of "A significance test for the lasso''}},
volume = {42},
year = {2014}
}
@article{Lockhart2014c,
author = {Wasserman, Larry},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - 2014 - Discussion âa significance test for the lassoâ.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {501--508},
title = {{Discussion: âa significance test for the lassoâ}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Lockhart2014d,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - A significance test for the lasso(3).pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {413--468},
title = {{A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Henmi2007,
author = {Henmi, Masayuki and Yoshida, Ryo and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Yoshida, Eguchi - 2007 - Importance sampling via the estimated sampler.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {985--991},
title = {{Importance sampling via the estimated sampler}},
volume = {94},
year = {2007}
}
@article{Lv2014,
author = {Lv, Jinchi and Zheng, Zemin},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv, Zheng - 2014 - Discussion âa significance test for the lassoâ.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {493--500},
title = {{Discussion: âa significance test for the lassoâ}},
volume = {42},
year = {2014}
}
@article{Fan2014,
author = {Fan, Jianqing and Ke, Zheng Tracy},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Ke - 2014 - Discussion âa significance test for the lassoâ.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {483--492},
title = {{Discussion: âa significance test for the lassoâ}},
volume = {42},
year = {2014}
}
@article{Cai2014,
author = {Cai, T. Tony and Yuan, Ming},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, Yuan - 2014 - Discussion âa significance test for the lassoâ.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {478--482},
title = {{Discussion: âa significance test for the lassoâ}},
volume = {42},
year = {2014}
}
@article{Chatterjee2007,
author = {Chatterjee, Sangit and Firat, Aykut},
doi = {10.1198/000313007X220057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Firat - 2007 - Generating Data with Identical Statistics but Dissimilar Graphics.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American Statistician},
month = {aug},
number = {3},
pages = {248--254},
title = {{Generating Data with Identical Statistics but Dissimilar Graphics}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X220057},
volume = {61},
year = {2007}
}
@article{Lockhart2014f,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - Rejoinder A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {518--531},
title = {{Rejoinder: A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@book{Lehmann1986,
author = {Lehmann, E.L.},
edition = {Second},
publisher = {Wiley},
title = {{Testing Statistical Hyptoheses}},
year = {1986}
}
@book{Casella2002,
author = {Casella, George and Berger, Roger L.},
edition = {Second},
title = {{Statistical Inference}},
year = {2002}
}
@article{Li2013a,
abstract = {In this paper, we study the heterogeneous domain adaptation (HDA) problem, in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. By introducing two different projection matrices, we first transform the data from two domains into a common subspace such that the similarity between samples across different domains can be measured. We then develop two new feature mapping functions for two domains, which respectively augments the transformed source and target samples with their original features and padding zeros. Existing supervised learning methods (e.g., SVM and SVR) can be readily employed by incorporating our newly proposed augmented feature representations for supervised HDA. As a showcase, we propose a novel method called Heterogeneous Feature Augmentation (HFA) based on SVM. We show that the proposed formulation can be equivalently derived as a standard Multiple Kernel Learning (MKL) problem, which is convex and thus the global solution can be guaranteed. To additionally utilize the unlabeled data in the target domain, we further propose the semi-supervised HFA (SHFA) which can simultaneously learn the target classifier as well as infer the labels of unlabeled target samples. Comprehensive experiments on three different applications clearly demonstrate that our SHFA and HFA outperform the existing HDA methods.},
author = {Li, Wen and Duan, Lixin and Xu, Dong and Tsang, Ivor W},
doi = {3E685107-2977-47E4-B935-16F0A8864540},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {aug},
number = {6},
pages = {1134--1148},
pmid = {23999386},
title = {{Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23999386},
volume = {36},
year = {2013}
}
@article{Anand2013,
abstract = {Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function.We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.},
author = {Anand, Saket and Mittal, Sushil and Tuzel, Oncel and Meer, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Anand et al. - 2013 - Semi-Supervised Kernel Mean Shift Clustering.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {sep},
number = {6},
pages = {1201--1215},
pmid = {24101327},
title = {{Semi-Supervised Kernel Mean Shift Clustering.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24101327},
volume = {36},
year = {2013}
}
@article{Zhang2014,
author = {Zhang, Kai and Lan, Liang and Kwok, James T. and Vucetic, Slobodan and Parvin, Bahram},
doi = {10.1109/TNNLS.2014.2315526},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2014 - Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--1},
title = {{Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6803073},
year = {2014}
}
@article{Schafer2002,
author = {Schafer, Joseph L. and Graham, John W.},
doi = {10.1037//1082-989X.7.2.147},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schafer, Graham - 2002 - Missing data Our view of the state of the art.pdf:pdf},
issn = {1082-989X},
journal = {Psychological Methods},
number = {2},
pages = {147--177},
title = {{Missing data: Our view of the state of the art.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.2.147},
volume = {7},
year = {2002}
}
@article{VanderKooi2013,
abstract = {OBJECTIVES: We investigated how much the Human Development Index (HDI), a global measure of development, modifies the effect of education on self-reported health.

METHODS: We analyzed cross-sectional World Health Survey data on 217,642 individuals from 49 countries, collected in 2002 to 2005, with random-intercept multilevel linear regression models.

RESULTS: We observed greater positive associations between educational levels and self-reported good health with increasing HDI. The magnitude of this effect modification of the education-health relation tended to increase with educational attainment. For example, before adjustment for effect modification, at comparable HDI, on average, finishing primary school was associated with better general health (b = 1.49; 95{\%} confidence interval [CI] = 1.18, 1.80). With adjustment for effect modification by HDI, the impact became 4.63 (95{\%} CI = 3.63, 5.62) for every 0.1 increase in HDI. Among those who completed high school, these associations were, respectively, 5.59 (95{\%} CI = 5.20, 5.98) and 9.95 (95{\%} CI = 8.89, 11.00).

CONCLUSIONS: The health benefits of educational attainment are greater in countries with greater human development. Health inequalities attributable to education are, therefore, larger in more developed countries.},
author = {van der Kooi, Anne L F and Stronks, Karien and Thompson, Caroline a and DerSarkissian, Maral and Arah, Onyebuchi a},
doi = {10.2105/AJPH.2013.301593},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Kooi et al. - 2013 - The modifying influence of country development on the effect of individual educational attainment on self-r.pdf:pdf},
issn = {1541-0048},
journal = {American journal of public health},
keywords = {Adult,Aged,Cross-Sectional Studies,Developed Countries,Developed Countries: statistics {\&} numerical data,Developing Countries,Developing Countries: statistics {\&} numerical data,Educational Status,Female,Health Status,Health Status Disparities,Health Surveys,Humans,Male,Middle Aged,Self Report,Young Adult},
month = {nov},
number = {11},
pages = {e49--54},
pmid = {24028233},
title = {{The modifying influence of country development on the effect of individual educational attainment on self-rated health.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24028233},
volume = {103},
year = {2013}
}
@article{Suykens1999,
author = {Suykens, Johan A. K. and Vandewalle, J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suykens, Vandewalle - 1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
journal = {Neural Processing Letters},
keywords = {abbreviations,classification,linear least squares,radial basis,radial basis function kernel,rbf,support vector machines,svm,vapnik-chervonenkis,vc},
pages = {293--300},
title = {{Least Squares Support Vector Machine Classifiers}},
volume = {9},
year = {1999}
}
@article{Efron2014,
author = {Efron, Bradley},
doi = {10.1214/13-STS455},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron - 2014 - Two Modeling Strategies for Empirical Bayes Estimation.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayes rule in terms,f -modeling,f-modeling, g-modeling, Bayes rule in terms of f,,g -modeling,of f,prior exponential families},
month = {may},
number = {2},
pages = {285--301},
title = {{Two Modeling Strategies for Empirical Bayes Estimation}},
url = {http://projecteuclid.org/euclid.ss/1408368582},
volume = {29},
year = {2014}
}
@article{Janson2013,
abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
archivePrefix = {arXiv},
arxivId = {1312.7851},
author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
eprint = {1312.7851},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janson, Fithian, Hastie - 2013 - Effective Degrees of Freedom A Flawed Metaphor.pdf:pdf},
month = {dec},
pages = {1--15},
title = {{Effective Degrees of Freedom: A Flawed Metaphor}},
url = {http://arxiv.org/abs/1312.7851},
year = {2013}
}
@article{Yuille2003,
abstract = {The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.},
author = {Yuille, a L and Rangarajan, Anand},
doi = {10.1162/08997660360581958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yuille, Rangarajan - 2003 - The concave-convex procedure.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Energy Metabolism,Neural Networks (Computer)},
month = {apr},
number = {4},
pages = {915--36},
pmid = {12689392},
title = {{The concave-convex procedure.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12689392},
volume = {15},
year = {2003}
}
@article{Peltonen2014,
author = {Peltonen, Jaakko and Lin, Ziyuan},
doi = {10.1007/s10994-014-5464-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peltonen, Lin - 2014 - Information retrieval approach to meta-visualization.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {and masashi sugiyama,b,bob williamson,cheng soon ong,editors,j,lin,meta-visualization,neighbor embedding,nonlinear dimensionality reduction,peltonen,tu bao ho,wray buntine,z},
month = {oct},
number = {January},
title = {{Information retrieval approach to meta-visualization}},
url = {http://link.springer.com/10.1007/s10994-014-5464-x},
year = {2014}
}
@article{Ridgway2014,
archivePrefix = {arXiv},
arxivId = {1410.1771},
author = {Ridgway, James and Alquier, Pierre and Chopin, Nicolas and Liang, Feng},
eprint = {1410.1771},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ridgway et al. - 2014 - PAC-Bayesian AUC classification and scoring.pdf:pdf},
month = {oct},
pages = {1--18},
title = {{PAC-Bayesian AUC classification and scoring}},
url = {http://arxiv.org/abs/1410.1771v1},
year = {2014}
}
@article{Williams1998,
author = {Williams, Christopher K. I.},
doi = {10.1162/089976698300017412},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williams - 1998 - Computation with Infinite Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {5},
pages = {1203--1216},
title = {{Computation with Infinite Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017412},
volume = {10},
year = {1998}
}
@article{Chatterjee2014a,
author = {Chatterjee, Sourav},
doi = {10.1214/14-AOS1254},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee - 2014 - A new perspective on least squares under convex constraint.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,convex constraint,empirical pro-,least squares,maximum likelihood},
month = {dec},
number = {6},
pages = {2340--2381},
title = {{A new perspective on least squares under convex constraint}},
url = {http://projecteuclid.org/euclid.aos/1413810730},
volume = {42},
year = {2014}
}
@misc{Shalev-Shwartz2014,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
year = {2014}
}
@article{Hollenbach2014,
abstract = {Gold-standard approaches to missing data imputation are complicated and computationally expensive. We present a principled solution to this situation, using copula distributions from which missing data may be quickly drawn. We compare this approach to other imputation techniques and show that it performs at least as well as less computationally efficient approaches. Our results demonstrate that most applied researchers can achieve great speed improvements implementing a copula-based imputation approach, while still maintaining the performance of other approaches to multiple imputation. Moreover, this approach can be easily implemented at the point of need in Bayesian analyses.},
archivePrefix = {arXiv},
arxivId = {1411.0647},
author = {Hollenbach, Florian M. and Metternich, Nils W. and Minhas, Shahryar and Ward, Michael D.},
eprint = {1411.0647},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hollenbach et al. - 2014 - Fast {\&} Easy Imputation of Missing Social Science Data.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Fast {\&} Easy Imputation of Missing Social Science Data}},
url = {http://arxiv.org/abs/1411.0647},
year = {2014}
}
@article{Kucukelbir2014,
abstract = {Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference. We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.},
archivePrefix = {arXiv},
arxivId = {1411.0292},
author = {Kucukelbir, Alp and Blei, David M.},
eprint = {1411.0292},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kucukelbir, Blei - 2014 - Profile Predictive Inference.pdf:pdf},
month = {nov},
pages = {8},
title = {{Profile Predictive Inference}},
url = {http://arxiv.org/abs/1411.0292},
year = {2014}
}
@article{Pilanci2014,
abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including {\$}\backslashell{\_}1{\$}-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
archivePrefix = {arXiv},
arxivId = {1411.0347},
author = {Pilanci, Mert and Wainwright, Martin J.},
eprint = {1411.0347},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pilanci, Wainwright - 2014 - Iterative Hessian sketch Fast and accurate solution approximation for constrained least-squares.pdf:pdf},
month = {nov},
number = {1},
pages = {1--33},
title = {{Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares}},
url = {http://arxiv.org/abs/1411.0347},
year = {2014}
}
@article{Hand2009,
author = {Hand, David J.},
doi = {10.1007/s10994-009-5119-5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
month = {jun},
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: aÂ coherent alternative to the area under the ROC curve}},
url = {http://link.springer.com/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@unpublished{Graves2014,
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
month = {oct},
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401v1},
year = {2014}
}
@article{Welinder2013,
author = {Welinder, Peter and Welling, Max and Perona, Pietro},
doi = {10.1109/CVPR.2013.419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welinder, Welling, Perona - 2013 - A Lazy Man's Approach to Benchmarking Semisupervised Classifier Evaluation and Recalibration.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3262--3269},
publisher = {Ieee},
title = {{A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619263},
year = {2013}
}
@article{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
month = {jun},
pages = {1--9},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@phdthesis{Rifkin2002,
author = {Rifkin, Ryan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin - 2002 - Everything Old Is New Again A Fresh Look at Historical Approaches in Machine Learning.pdf:pdf},
title = {{Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning}},
year = {2002}
}
@article{Dai2006,
author = {Dai, Yu-Hong},
doi = {10.1137/040613305},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dai - 2006 - Fast Algorithms for Projection on an Ellipsoid.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {040613305,1,10,1137,65k05,90c06,ams subject classifications,doi,ellipsoid,hybrid algorithm,introduction,large-scale,linear convergence,on a general convex,projection,set,the problem of projection},
month = {jan},
number = {4},
pages = {986--1006},
title = {{Fast Algorithms for Projection on an Ellipsoid}},
url = {http://epubs.siam.org/doi/abs/10.1137/040613305},
volume = {16},
year = {2006}
}
@unpublished{Grunwald2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3730v1},
author = {Gr{\"{u}}nwald, Peter and van Ommen, Thijs},
eprint = {arXiv:1412.3730v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Ommen - 2014 - Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It.pdf:pdf},
pages = {1--70},
title = {{Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It}},
year = {2014}
}
@unpublished{Chena,
archivePrefix = {arXiv},
arxivId = {arXiv:0000.0000},
author = {Chen, Aiyou and Owen, Art B. and Shi, Minghui},
eprint = {arXiv:0000.0000},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Owen, Shi - Unknown - Data Enriched Linear Regression.pdf:pdf},
pages = {1--37},
title = {{Data Enriched Linear Regression}}
}
@article{Xiao2015,
author = {Xiao, Min and Guo, Yuhong},
doi = {10.1109/TPAMI.2014.2343216},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiao, Guo - 2015 - Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {54--66},
title = {{Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6866177},
volume = {37},
year = {2015}
}
@article{Li2015,
author = {Li, Yu-Feng and Zhou, Zhi-Hua},
doi = {10.1109/TPAMI.2014.2299812},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2015 - Towards Making Unlabeled Data Never Hurt.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {175--188},
title = {{Towards Making Unlabeled Data Never Hurt}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6710159},
volume = {37},
year = {2015}
}
@article{Schuurmans2002,
author = {Schuurmans, Dale and Southey, Finnegan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schuurmans, Southey - 2002 - Metric-Based Methods for Adaptive Model Selection and Regularization.pdf:pdf},
journal = {Machine Learning},
keywords = {model selection,regularization,unlabeled examples},
pages = {51--84},
title = {{Metric-Based Methods for Adaptive Model Selection and Regularization}},
volume = {48},
year = {2002}
}
@article{Zhang2004,
author = {Zhang, P. and Peng, J.},
doi = {10.1109/ICPR.2004.1334050},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Peng - 2004 - SVM vs regularized least squares classification.pdf:pdf},
isbn = {0-7695-2128-2},
journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
number = {3},
pages = {176--179 Vol.1},
publisher = {Ieee},
title = {{SVM vs regularized least squares classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334050},
year = {2004}
}
@article{Tolstikhin2014,
archivePrefix = {arXiv},
arxivId = {1411.7200},
author = {Tolstikhin, Ilya and Blanchard, Gilles and Kloft, Marius},
eprint = {1411.7200},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Blanchard, Kloft - 2014 - Localized Complexities for Transductive Learning.pdf:pdf},
keywords = {centration inequalities,con-,empirical processes,fast rates,kernel classes,localized complexities,statistical learning,transductive learning},
month = {nov},
pages = {1--28},
title = {{Localized Complexities for Transductive Learning}},
url = {http://arxiv.org/abs/1411.7200v1},
volume = {35},
year = {2014}
}
@article{Shiao2014,
author = {Shiao, Han-Tai and Cherkassky, Vladimir},
doi = {10.1109/IJCNN.2014.6889517},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shiao, Cherkassky - 2014 - Learning using privileged information (LUPI) for modeling survival data.pdf:pdf},
isbn = {978-1-4799-1484-5},
journal = {2014 International Joint Conference on Neural Networks (IJCNN)},
month = {jul},
pages = {1042--1049},
publisher = {Ieee},
title = {{Learning using privileged information (LUPI) for modeling survival data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6889517},
year = {2014}
}
@article{Hand2013,
author = {Hand, D.J. and Anagnostopoulos, C.},
doi = {10.1016/j.patrec.2012.12.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand, Anagnostopoulos - 2013 - When is the area under the receiver operating characteristic curve an appropriate measure of classifier p.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {area under the curve},
month = {apr},
number = {5},
pages = {492--495},
title = {{When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512003923},
volume = {34},
year = {2013}
}
@article{Barber2014,
abstract = {We explore and compare a variety of definitions for privacy and disclosure limitation in statistical estimation and data analysis, including (approximate) differential privacy, testing-based definitions of privacy, and posterior guarantees on disclosure risk. We give equivalence results between the definitions, shedding light on the relationships between different formalisms for privacy. We also take an inferential perspective, where---building off of these definitions---we provide minimax risk bounds for several estimation problems, including mean estimation, estimation of the support of a distribution, and nonparametric density estimation. These bounds highlight the statistical consequences of different definitions of privacy and provide a second lens for evaluating the advantages and disadvantages of different techniques for disclosure limitation.},
archivePrefix = {arXiv},
arxivId = {1412.4451},
author = {Barber, Rina Foygel and Duchi, John C.},
eprint = {1412.4451},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber, Duchi - 2014 - Privacy and Statistical Risk Formalisms and Minimax Bounds.pdf:pdf},
month = {dec},
pages = {29},
title = {{Privacy and Statistical Risk: Formalisms and Minimax Bounds}},
url = {http://arxiv.org/abs/1412.4451},
year = {2014}
}
@unpublished{DeDeo2014,
abstract = {A recurring problem with statistical prediction for policy-making is that many useful variables are associated with others on which it would be ethically problematic to base decisions. This problem becomes particularly acute in the Big Data era, when predictions are often made in the absence of strong theories for the underlying causal mechanisms. Given this, we show how to use information theory to construct the distribution closest in predictive power to the full distribution, but in which predictions---and thus policy outcomes, provision of services, and so forth---are not correlated with protected variables.},
archivePrefix = {arXiv},
arxivId = {1412.4643},
author = {DeDeo, Simon},
eprint = {1412.4643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/DeDeo - 2014 - Wrong side of the tracks Big Data and Protected Categories.pdf:pdf},
month = {dec},
pages = {3},
title = {{"Wrong side of the tracks": Big Data and Protected Categories}},
url = {http://arxiv.org/abs/1412.4643},
year = {2014}
}
@article{Hand2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0606441v1},
author = {Hand, David J.},
doi = {10.1214/088342306000000060},
eprint = {0606441v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2006 - Classifier Technology and the Illusion of Progress.pdf:pdf},
isbn = {0883423060000},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,empirical com-,error rate,flat maximum effect,lectivity bias,misclas-,population drift,principle of parsimony,problem uncertainty,se-,sification rate,simplicity,supervised classification},
month = {feb},
number = {1},
pages = {1--14},
primaryClass = {arXiv:math},
title = {{Classifier Technology and the Illusion of Progress}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1149600839/},
volume = {21},
year = {2006}
}
@article{Mooij2014,
archivePrefix = {arXiv},
arxivId = {1412.3773},
author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"{o}}lkopf, Bernhard},
eprint = {1412.3773},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2014 - Distinguishing cause from effect using observational data methods and benchmarks.pdf:pdf},
month = {dec},
title = {{Distinguishing cause from effect using observational data: methods and benchmarks}},
url = {http://arxiv.org/abs/1412.3773v1},
year = {2014}
}
@article{Neyshabur2014,
abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
archivePrefix = {arXiv},
arxivId = {1412.6614},
author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
eprint = {1412.6614},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neyshabur, Tomioka, Srebro - 2014 - In Search of the Real Inductive Bias On the Role of Implicit Regularization in Deep Learning.pdf:pdf},
month = {dec},
pages = {7},
title = {{In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning}},
url = {http://arxiv.org/abs/1412.6614},
year = {2014}
}
@article{Koyamada2014,
abstract = {We present a novel algorithm(Principal Sensitivity Analysis; PSA) to analyze the knowledge of the classifier obtained from supervised machine learning technique. In particular, we define principal sensitivity map (PSM) as the direction on the input space to which the trained classifier is most sensitive, and use analogously defined k-th PSM to define a basis for the input space. We train neural networks with artificial data and real data, and apply the algorithm to the obtained supervised classifiers. We then visualize the PSMs to demonstrate the PSA's ability to decompose the knowledge acquired by the trained classifiers.},
archivePrefix = {arXiv},
arxivId = {1412.6785},
author = {Koyamada, Sotetsu and Koyama, Masanori and Nakae, Ken and Ishii, Shin},
eprint = {1412.6785},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Koyamada et al. - 2014 - Principal Sensitivity Analysis.pdf:pdf},
keywords = {dark knowledge,knowl-,pca,sensitibity analysis,sensitivity map},
month = {dec},
pages = {1--13},
title = {{Principal Sensitivity Analysis}},
url = {http://arxiv.org/abs/1412.6785},
year = {2014}
}
@unpublished{Maeda2014,
abstract = {Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.},
archivePrefix = {arXiv},
arxivId = {1412.7003},
author = {Maeda, Shin-ichi},
eprint = {1412.7003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maeda - 2014 - A Bayesian encourages dropout.pdf:pdf},
month = {dec},
pages = {1--9},
title = {{A Bayesian encourages dropout}},
url = {http://arxiv.org/abs/1412.7003},
year = {2014}
}
@article{Kleijn2006a,
abstract = {We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution {\$}P{\_}0{\$}, which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback--Leibler divergence with respect to {\$}P{\_}0{\$}. An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models.},
archivePrefix = {arXiv},
arxivId = {math/0607023},
author = {Kleijn, B. J K and {Van Der Vaart}, a. W.},
doi = {10.1214/009053606000000029},
eprint = {0607023},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleijn, Van Der Vaart - 2006 - Misspecification in infinite-dimensional Bayesian statistics.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Infinite-dimensional model,Misspecification,Posterior distribution,Rate of convergence},
number = {2},
pages = {837--877},
primaryClass = {math},
title = {{Misspecification in infinite-dimensional Bayesian statistics}},
volume = {34},
year = {2006}
}
@inproceedings{Maddison2014,
author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maddison, Tarlow, Minka - 2014 - A Sampling.pdf:pdf},
title = {{A* Sampling}},
year = {2014}
}
@article{Roweis1999,
abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
author = {Roweis, Sam and Ghahramani, Zoubin},
doi = {10.1162/089976699300016674},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roweis, Ghahramani - 1999 - A unifying review of linear gaussian models.pdf:pdf},
isbn = {0899766993000},
issn = {0899-7667},
journal = {Neural computation},
number = {1995},
pages = {305--345},
pmid = {9950734},
title = {{A unifying review of linear gaussian models.}},
volume = {11},
year = {1999}
}
@article{Poon2011,
abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs) and will present in this abstract.},
archivePrefix = {arXiv},
arxivId = {1202.3732},
author = {Poon, Hoifung and Domingos, Pedro},
doi = {10.1109/ICCVW.2011.6130310},
eprint = {1202.3732},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poon, Domingos - 2011 - Sum-product networks A new deep architecture.pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {689--690},
title = {{Sum-product networks: A new deep architecture}},
year = {2011}
}
@inproceedings{Kim2014,
author = {Kim, Do-kyum and Der, Matthew and Saul, Lawrence K.},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kim, Der, Saul - 2014 - A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data.pdf:pdf},
title = {{A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data}},
url = {http://jmlr.org/proceedings/papers/v33/kim14a.pdf},
volume = {33},
year = {2014}
}
@article{Nguyen2010,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization(2).pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {nov},
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@unpublished{Scholkopf2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.06794v1},
author = {Sch{\"{o}}lkopf, Bernhard and Muandet, Krikamol and Fukumizu, Kenji and Peters, Jonas},
eprint = {arXiv:1501.06794v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch{\"{o}}lkopf et al. - 2015 - Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations.pdf:pdf},
pages = {1--20},
title = {{Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations}},
year = {2015}
}
@article{Smola2003,
abstract = {We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.},
author = {Smola, Aj Alexander J and Kondor, Risi},
doi = {10.1007/978-3-540-45167-9_12},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Kondor - 2003 - Kernels and Regularization on Graphs.pdf:pdf},
isbn = {3540407200},
issn = {03029743},
journal = {Machine Learning},
pages = {1--15},
title = {{Kernels and Regularization on Graphs}},
url = {http://www.springerlink.com/index/H96KDX90DCMM6FX0.pdf$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}12},
volume = {2777},
year = {2003}
}
@article{Balsubramani,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01811v1},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {arXiv:1503.01811v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - Unknown - Optimally Combining Classifiers Using Unlabeled Data.pdf:pdf},
title = {{Optimally Combining Classifiers Using Unlabeled Data}}
}
@article{Carlo2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07645v1},
author = {Carlo, Monte and Feb, M L},
eprint = {arXiv:1502.07645v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carlo, Feb - 2015 - Privacy for Free Posterior Sampling and Stochastic Gradient.pdf:pdf},
pages = {1--27},
title = {{Privacy for Free : Posterior Sampling and Stochastic Gradient}},
year = {2015}
}
@article{Vovk2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06254v1},
author = {Vovk, Vladimir},
eprint = {arXiv:1502.06254v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vovk - 2015 - The fundamental nature of the log loss function arXiv 1502 . 06254v1 cs . LG 22 Feb 2015.pdf:pdf},
pages = {1--6},
title = {{The fundamental nature of the log loss function arXiv : 1502 . 06254v1 [ cs . LG ] 22 Feb 2015}},
year = {2015}
}
@article{Wang2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06309v1},
author = {Wang, Yu-xiang and Lei, Jing and Fienberg, Stephen E and Feb, M L},
eprint = {arXiv:1502.06309v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Learning with Differential Privacy Stability , Learnability and the Sufficiency and Necessity of ERM Principle.pdf:pdf},
pages = {1--33},
title = {{Learning with Differential Privacy : Stability , Learnability and the Sufficiency and Necessity of ERM Principle}},
year = {2015}
}
@article{Balcan2014,
abstract = {One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise $\backslash$cite{\{}qcluster2005{\}}. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm consistently achieves better performance than other hierarchical algorithms in the presence of noise.},
archivePrefix = {arXiv},
arxivId = {1401.0247},
author = {Balcan, Mf Maria-florina and Liang, Yingyu and Gupta, Pramod},
eprint = {1401.0247},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Liang, Gupta - 2014 - Robust hierarchical clustering.pdf:pdf},
isbn = {9780982252925},
journal = {arXiv preprint arXiv:1401.0247},
keywords = {agglomerative algorithms,clustering,robustness,unsupervised learning},
pages = {35},
title = {{Robust hierarchical clustering}},
url = {http://arxiv.org/abs/1401.0247},
volume = {15},
year = {2014}
}
@article{Gelman1999,
abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be difficult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artefacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artefacts. We present examples from Bayesian modelling, but, as we explain, the artefacts occur generally.},
author = {Gelman, Andrew and Price, Phlllip N.},
doi = {10.1002/(SICI)1097-0258(19991215)18:23<3221::AID-SIM312>3.0.CO;2-M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Price - 1999 - All maps of parameter estimates are misleading.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
pages = {3221--3234},
pmid = {10602147},
title = {{All maps of parameter estimates are misleading}},
volume = {18},
year = {1999}
}
@article{Baldassarre2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1303.3207v3},
author = {Baldassarre, Luca and Bhan, Nirav},
eprint = {arXiv:1303.3207v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baldassarre, Bhan - 2013 - Group-Sparse Model Selection Hardness and Relaxations.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--18},
title = {{Group-Sparse Model Selection: Hardness and Relaxations}},
url = {http://arxiv.org/abs/1303.3207},
year = {2013}
}
@article{Qiu2013,
abstract = {A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a high-rank structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separation between the subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, called Robust Sparse Subspace Clustering, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.},
archivePrefix = {arXiv},
arxivId = {1309.2074},
author = {Qiu, Qiang and Sapiro, Guillermo},
eprint = {1309.2074},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Qiu, Sapiro - 2013 - Learning Transformations for Clustering and Classification.pdf:pdf},
pages = {187--225},
title = {{Learning Transformations for Clustering and Classification}},
url = {http://arxiv.org/abs/1309.2074},
volume = {16},
year = {2013}
}
@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M.},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neal - 2011 - Handbook of Markov Chain Monte Carlo.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{Williamson,
author = {Williamson, Robert C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williamson - Unknown - Loss Functions.pdf:pdf},
number = {3},
pages = {1--10},
title = {{Loss Functions}}
}
@article{Reid2009,
abstract = {We present tight surrogate regret bounds for the
class of proper (i.e., Fisher consistent) losses.
The bounds generalise the margin-based bounds
due to Bartlett et al. (2006). The proof uses Taylor's
theorem and leads to new representations
for loss and regret and a simple proof of the integral
representation of proper losses. We also
present a different formulation of a duality result
of Bregman divergences which leads to a simple
demonstration of the convexity of composite
losses using canonical link functions},
author = {Reid, Mark and Williamson, Bob},
doi = {10.1145/1553374.1553489},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2009 - Surrogate Regret Bounds for Proper Losses.pdf:pdf},
isbn = {9781605585161},
keywords = {Learning/Statistics {\&} Optimisation},
number = {Theorem 3},
pages = {897--904},
title = {{Surrogate Regret Bounds for Proper Losses}},
url = {http://eprints.pascal-network.org/archive/00008977/},
year = {2009}
}
@article{Davies1995,
abstract = {This article attempts to provide a formal framework for a data based inference which explicitly and consistently recognizes the approximate nature of probability models. It is based on the idea that a stochastic model is adequate if samples generated under the model are very much like the sample actually obtained. The formalization is based on the concept of data feature. Examples are given of applying the ideas to different areas of statistics including location-scale models, densities, non-parametric regression, interlaboratory test, auto-regressive processes and the analysis of variance. The four cornerstones of the approach are direct comparison, approximation, weak topologies and parsimony. The approach is contrasted to that of much of conventional statistics many of whose concepts are pathologically discontinuous with respect to the topology of data analysis and common sense.},
author = {Davies, Pl},
doi = {10.1111/j.1467-9574.1995.tb01464.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Davies - 1995 - Data features '.pdf:pdf},
issn = {0039-0402},
journal = {Statistica Neerlandica},
keywords = {adequacy,continuity,data analysis,data features,inference,phrases,smooth functionals,topologies,weak and strong},
number = {2},
pages = {185--245},
title = {{Data features '}},
volume = {49},
year = {1995}
}
@article{Ferkingstad2014,
abstract = {As increasingly complex hypothesis-testing scenarios are considered in many scientific fields, analytic derivation of null distributions is often out of reach. To the rescue comes Monte Carlo testing, which may appear deceptively simple: as long as you can sample test statistics under the null hypothesis, the p-value is just the proportion of sampled test statistics that exceed the observed test statistic. Sampling test statistics is often simple once you have a Monte Carlo null model for your data, and defining some form of randomization procedure is also, in many cases, relatively straightforward. However, there may be several possible choices of randomization null model for the data, and no clear-cut criteria for choosing among them. Obviously, different null models may lead to very different p-values, and a very low p-value may thus occur due to the inadequacy of the chosen null model. It is preferable to use assumptions about the underlying random data generation process to guide selection of a null model. In many cases, we may order the null models by increasing preservation of the data characteristics, and we argue in this paper that this ordering in most cases gives increasing p-values, i.e. lower significance. We denote this as the null complexity principle. The principle gives a better understanding of the different null models and may guide in the choice between the different models.},
archivePrefix = {arXiv},
arxivId = {1404.5970},
author = {Ferkingstad, Egil and Holden, Lars and Sandve, Geir Kjetil},
eprint = {1404.5970},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ferkingstad, Holden, Sandve - 2014 - Monte Carlo null models for genomic data.pdf:pdf},
keywords = {Genomics,Hypothesis testing,Monte Carlo methods,and phrases,ge-,hypothesis testing,monte carlo methods},
pages = {1--20},
title = {{Monte Carlo null models for genomic data}},
url = {http://arxiv.org/abs/1404.5970},
year = {2014}
}
@article{Cule2011,
abstract = {BACKGROUND: Technological developments have increased the feasibility of large scale genetic association studies. Densely typed genetic markers are obtained using SNP arrays, next-generation sequencing technologies and imputation. However, SNPs typed using these methods can be highly correlated due to linkage disequilibrium among them, and standard multiple regression techniques fail with these data sets due to their high dimensionality and correlation structure. There has been increasing interest in using penalised regression in the analysis of high dimensional data. Ridge regression is one such penalised regression technique which does not perform variable selection, instead estimating a regression coefficient for each predictor variable. It is therefore desirable to obtain an estimate of the significance of each ridge regression coefficient.$\backslash$n$\backslash$nRESULTS: We develop and evaluate a test of significance for ridge regression coefficients. Using simulation studies, we demonstrate that the performance of the test is comparable to that of a permutation test, with the advantage of a much-reduced computational cost. We introduce the p-value trace, a plot of the negative logarithm of the p-values of ridge regression coefficients with increasing shrinkage parameter, which enables the visualisation of the change in p-value of the regression coefficients with increasing penalisation. We apply the proposed method to a lung cancer case-control data set from EPIC, the European Prospective Investigation into Cancer and Nutrition.$\backslash$n$\backslash$nCONCLUSIONS: The proposed test is a useful alternative to a permutation test for the estimation of the significance of ridge regression coefficients, at a much-reduced computational cost. The p-value trace is an informative graphical tool for evaluating the results of a test of significance of ridge regression coefficients as the shrinkage parameter increases, and the proposed test makes its production computationally feasible.},
author = {Cule, Erika and Vineis, Paolo and {De Iorio}, Maria},
doi = {10.1186/1471-2105-12-372},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cule, Vineis, De Iorio - 2011 - Significance testing in ridge regression for genetic data.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {372},
pmid = {21929786},
publisher = {BioMed Central Ltd},
title = {{Significance testing in ridge regression for genetic data}},
url = {http://www.biomedcentral.com/1471-2105/12/372},
volume = {12},
year = {2011}
}
@article{Keiding2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02853v1},
author = {Keiding, Niels and Clayton, David},
doi = {10.1214/13-STS453},
eprint = {arXiv:1503.02853v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keiding, Clayton - 2014 - Standardization and Control for Confounding in Observational Studies A Historical Perspective.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {2,22K table, causality, decomposition of rates, epid,and phrases,causality,decomposition,epidemiology,expected number of deaths,g,h,k table,log-linear model,marginal structural model,national halothane study,odds ratio,of rates,rate,ratio,transportability,u,westergaard,yule},
number = {4},
pages = {529--558},
title = {{Standardization and Control for Confounding in Observational Studies: A Historical Perspective}},
url = {http://projecteuclid.org/euclid.ss/1421330546},
volume = {29},
year = {2014}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02531v1},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {arXiv:1503.02531v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hinton, Vinyals, Dean - Unknown - Distilling the Knowledge in a Neural Network.pdf:pdf},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}}
}
@inproceedings{Kulesza2010,
abstract = {We present a novel probabilistic model for distributions over sets of structuresâ for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of},
author = {Kulesza, Alex and Taskar, Ben},
booktitle = {Advances in Neural Information Processing Systems 23},
doi = {10.1080/00036840500405656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2010 - Structured Determinantal Point Processes.pdf:pdf},
isbn = {0003684050040},
issn = {{\textless}null{\textgreater}},
pages = {1--9},
title = {{Structured Determinantal Point Processes}},
year = {2010}
}
@article{Kulesza2012,
abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
archivePrefix = {arXiv},
arxivId = {1207.6083},
author = {Kulesza, Alex and Taskar, Ben},
doi = {10.1561/2200000044},
eprint = {1207.6083},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:pdf},
isbn = {9781601986283},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {2-3},
pages = {123--286},
title = {{Determinantal Point Processes for Machine Learning}},
url = {http://arxiv.org/abs/1207.6083$\backslash$nhttp://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000044},
volume = {5},
year = {2012}
}
@article{Betancourt2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01510v1},
author = {Betancourt, Michael},
eprint = {arXiv:1502.01510v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling.pdf:pdf},
journal = {Arxiv preprint arXiv:1502.01510v1},
title = {{The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling}},
year = {2015}
}
@article{Adams2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01664v1},
author = {Adams, Niall M},
eprint = {arXiv:1502.01664v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - 2014 - Estimating Optimal Active Learning via Model Retraining.pdf:pdf},
keywords = {active learning,classification,estimation framework,ex-,model retraining improvement,pected loss reduction},
pages = {1--36},
title = {{Estimating Optimal Active Learning via Model Retraining}},
volume = {15},
year = {2014}
}
@article{Taddy2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02312v1},
author = {Taddy, Matt and Chen, Chun-sheng and Yun, Jun},
eprint = {arXiv:1502.02312v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taddy, Chen, Yun - 2001 - Bayesian and Empirical Bayesian Forests.pdf:pdf},
number = {1},
title = {{Bayesian and Empirical Bayesian Forests}},
year = {2001}
}
@article{Lopez-paz2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02398v1},
author = {Lopez-paz, David and Sch, Bernhard},
eprint = {arXiv:1502.02398v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-paz, Sch - 2012 - Towards a Learning Theory of Causation.pdf:pdf},
title = {{Towards a Learning Theory of Causation}},
year = {2012}
}
@article{Chakraborty2015,
archivePrefix = {arXiv},
arxivId = {1502.03491v1},
author = {Chakraborty, Mithun and Das, Sanmay and Lavoie, Allen},
eprint = {1502.03491v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chakraborty, Das, Lavoie - 2015 - How to show a probablistic model is better.pdf:pdf},
pages = {1--5},
title = {{How to show a probablistic model is better}},
year = {2015}
}
@unpublished{Hennig2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02555v1},
author = {Hennig, Christian},
eprint = {arXiv:1502.02555v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig - 2015 - What are the true clusters.pdf:pdf},
keywords = {active scientific realism,cat-,comparison of clustering methods,constructivism,egorization,mixture models,natural kinds,variable},
title = {{What are the true clusters?}},
year = {2015}
}
@article{Hussami2013,
abstract = {We propose a new sparse regression method called the component lasso, based on a simple idea. The method uses the connected-components structure of the sample covariance matrix to split the problem into smaller ones. It then applies the lasso to each subproblem separately, obtaining a coefficient vector for each one. Then, it uses non-negative least squares to recombine the different vectors into a single so- lution. This step is useful in selecting and reweighting components that are correlated with the response. Simulated and real data examples show that the component lasso can outperform standard regression methods such as the lasso and elastic net, achieving a lower mean squared error as well as better support recovery. Keywords.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.4472v2},
author = {Hussami, Nadine and Tibshirani, Robert},
eprint = {arXiv:1311.4472v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hussami, Tibshirani - 2013 - A Component Lasso.pdf:pdf},
journal = {arXiv preprint arXiv:1311.4472},
keywords = {connected components,elastic net,graphical lasso,grouping effect,lasso,negative least squares,sparsity},
number = {2},
pages = {1--19},
title = {{A Component Lasso}},
url = {http://arxiv.org/abs/1311.4472},
year = {2013}
}
@article{Li2011a,
abstract = {Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification.},
author = {Li, Limin and Rakitsch, Barbara and Borgwardt, Karsten},
doi = {10.1093/bioinformatics/btr204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Rakitsch, Borgwardt - 2011 - ccSVM Correcting Support Vector Machines for confounding factors in biological data classification.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {13},
pmid = {21685091},
title = {{ccSVM: Correcting Support Vector Machines for confounding factors in biological data classification}},
volume = {27},
year = {2011}
}
@article{Leek2007,
abstract = {It has unambiguously been shown that genetic, environmental, demographic, and technical factors may have substantial effects on gene expression levels. In addition to the measured variable(s) of interest, there will tend to be sources of signal due to factors that are unknown, unmeasured, or too complicated to capture through simple models. We show that failing to incorporate these sources of heterogeneity into an analysis can have widespread and detrimental effects on the study. Not only can this reduce power or induce unwanted dependence across genes, but it can also introduce sources of spurious signal to many genes. This phenomenon is true even for well-designed, randomized studies. We introduce "surrogate variable analysis" (SVA) to overcome the problems caused by heterogeneity in expression studies. SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest. We apply SVA to disease class, time course, and genetics of gene expression studies. We show that SVA increases the biological accuracy and reproducibility of analyses in genome-wide expression studies.},
author = {Leek, Jeffrey T. and Storey, John D.},
doi = {10.1371/journal.pgen.0030161},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leek, Storey - 2007 - Capturing heterogeneity in gene expression studies by surrogate variable analysis.pdf:pdf},
isbn = {1553-7404 (Electronic)$\backslash$n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {9},
pages = {1724--1735},
pmid = {17907809},
title = {{Capturing heterogeneity in gene expression studies by surrogate variable analysis}},
volume = {3},
year = {2007}
}
@article{Saeys2007,
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.},
author = {Saeys, Yvan and Inza, I{\~{n}}aki and Larra{\~{n}}aga, Pedro},
doi = {10.1093/bioinformatics/btm344},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Saeys, Inza, Larra{\~{n}}aga - 2007 - A review of feature selection techniques in bioinformatics.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {19},
pages = {2507--2517},
pmid = {17720704},
title = {{A review of feature selection techniques in bioinformatics}},
volume = {23},
year = {2007}
}
@article{Greenland1985,
abstract = {The authors examine some recently proposed criteria for determining when to adjust for covariates related to misclassification, and show these criteria to be incorrect. In particular, they show that when misclassification is present, covariate control can sometimes increase net bias, even when the covariate would have been a confounder under perfect classification, and even if the covariate is a determinant of classification. Thus, bias due to misclassification cannot be adequately dealt with by the methods used for control of confounding. The examples presented also show that the "change-in-estimate" criterion for deciding whether to control a covariate can be systematically misleading when misclassification is present. These results demonstrate that it is necessary to consider the degree of misclassification when deciding whether to control a covariate.},
author = {Greenland, S and Robins, J M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Greenland, Robins - 1985 - Confounding and misclassification.pdf:pdf},
issn = {0002-9262},
journal = {American journal of epidemiology},
number = {3},
pages = {495--506},
pmid = {4025298},
title = {{Confounding and misclassification.}},
volume = {122},
year = {1985}
}
@article{Gonzalez-Covarrubias2013,
abstract = {Middle-aged offspring of nonagenarians, as compared to their spouses (controls), show a favorable lipid metabolism marked by larger LDL particle size in men and lower total triglyceride levels in women. To investigate which specific lipids associate with familial longevity, we explore the plasma lipidome by measuring 128 lipid species using liquid chromatography coupled to mass spectrometry in 1526 offspring of nonagenarians (59 years Â± 6.6) and 675 (59 years Â± 7.4) controls from the Leiden Longevity Study. In men, no significant differences were observed between offspring and controls. In women, however, 19 lipid species associated with familial longevity. Female offspring showed higher levels of ether phosphocholine (PC) and sphingomyelin (SM) species (3.5-8.7{\%}) and lower levels of phosphoethanolamine PE (38:6) and long-chain triglycerides (TG) (9.4-12.4{\%}). The association with familial longevity of two ether PC and four SM species was independent of total triglyceride levels. In addition, the longevity-associated lipid profile was characterized by a higher ratio of monounsaturated (MUFA) over polyunsaturated (PUFA) lipid species, suggesting that female offspring have a plasma lipidome less prone to oxidative stress. Ether PC and SM species were identified as novel longevity markers in females, independent of total triglycerides levels. Several longevity-associated lipids correlated with a lower risk of hypertension and diabetes in the Leiden Longevity Study cohort. This sex-specific lipid signature marks familial longevity and may suggest a plasma lipidome with a better antioxidant capacity, lower lipid peroxidation and inflammatory precursors, and an efficient beta-oxidation function.},
author = {Gonzalez-Covarrubias, Vanessa and Beekman, Marian and Uh, Hae Won and Dane, Adrie and Troost, Jorne and Paliukhovich, Iryna and van der Kloet, Frans M. and Houwing-Duistermaat, Jeanine and Vreeken, Rob J. and Hankemeier, Thomas and Slagboom, Eline P.},
doi = {10.1111/acel.12064},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gonzalez-Covarrubias et al. - 2013 - Lipidomics of familial longevity.pdf:pdf},
isbn = {1474-9726 (Electronic)$\backslash$r1474-9718 (Linking)},
issn = {14749718},
journal = {Aging Cell},
keywords = {Aging,Gender differences,Human,Longevity,Mass spectrometry,Oxidative stress},
number = {3},
pages = {426--434},
pmid = {23451766},
title = {{Lipidomics of familial longevity}},
volume = {12},
year = {2013}
}
@article{Tu2015,
author = {Tu, Enmei and Yang, Jie and Kasabov, Nicola and Zhang, Yaqian},
doi = {10.1016/j.neucom.2015.01.020},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tu et al. - 2015 - Posterior Distribution Learning (PDL) A novel supervised learning framework using unlabeled samples to improve classi.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Posterior distribution learning,Supervised learning,Supervised manifold classification},
pages = {173--186},
publisher = {Elsevier},
title = {{Posterior Distribution Learning (PDL): A novel supervised learning framework using unlabeled samples to improve classification performance}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215000417},
volume = {157},
year = {2015}
}
@article{Gomez-Chova2008,
abstract = {This letter presents a semisupervised method based on kernel machines and graph theory for remote sensing image classification. The support vector machine (SVM) is regularized with the unnormalized graph Laplacian, thus leading to the Laplacian SVM (LapSVM). The method is tested in the challenging problems of urban monitoring and cloud screening, in which an adequate exploitation of the wealth of unlabeled samples is critical. Results obtained using different sensors, and with low number of training samples, demonstrate the potential of the proposed LapSVM for remote sensing image classification.},
author = {G{\'{o}}mez-Chova, Luis and Camps-Valls, Gustavo and Mu{\~{n}}oz-Mari, Jordi and Calpe, Javier},
doi = {10.1109/LGRS.2008.916070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/G{\'{o}}mez-Chova et al. - 2008 - Semisupervised image classification with Laplacian support vector machines.pdf:pdf},
isbn = {1545-598X},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Kernel methods,Manifold learning,Regularization,Semisupervised learning (SSL),Support vector machines (SVMs)},
number = {3},
pages = {336--340},
title = {{Semisupervised image classification with Laplacian support vector machines}},
volume = {5},
year = {2008}
}
@article{Garcia-Laencina2010,
abstract = {Pattern classification has been successfully applied in many problem domains, such as biometric recognition, document classification or medical diagnosis. Missing or unknown data are a common drawback that pattern recognition techniques need to deal with when solving real-life classification tasks. Machine learning approaches and methods imported from statistical learning theory have been most intensively studied and used in this subject. The aim of this work is to analyze the missing data problem in pattern classification tasks, and to summarize and compare some of the well-known methods used for handling missing values.},
author = {Garc{\'{i}}a-Laencina, Pedro J. and Sancho-G{\'{o}}mez, Jos{\'{e}}-Luis and Figueiras-Vidal, An{\'{i}}bal R.},
doi = {10.1007/s00521-009-0295-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Garc{\'{i}}a-Laencina, Sancho-G{\'{o}}mez, Figueiras-Vidal - 2010 - Pattern classification with missing data a review.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {data {\'{a}},learning,neural networks {\'{a}} machine,pattern classification {\'{a}} missing},
number = {2},
pages = {263--282},
title = {{Pattern classification with missing data: a review}},
volume = {19},
year = {2010}
}
@article{Juszczak2004,
author = {Juszczak, P and Duin, R P W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Juszczak, Duin - 2004 - Combining one-class classifiers to classify missing data.pdf:pdf},
issn = {03029743},
journal = {Multiple Classifier Systems},
pages = {92--101},
title = {{Combining one-class classifiers to classify missing data}},
year = {2004}
}
@inproceedings{Ghahramani1994,
author = {Ghahramani, Zoubin and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani, Jordan - 1994 - Supervised Learning from incomplete data via an EM approach.pdf:pdf},
title = {{Supervised Learning from incomplete data via an EM approach}},
year = {1994}
}
@article{Smola2005,
abstract = {We present methods for dealing with missing variables in the context
  of Gaussian Processes and Support Vector Machines. This solves an
  important problem which has largely been ignored by kernel methods:
  How to systematically deal with incomplete data? Our method can also
  be applied to problems with partially observed labels as well as to
  the transductive setting where we view the labels as missing data.
  
  Our approach relies on casting kernel methods as an estimation
  problem in exponential families. Hence, estimation with missing
  variables becomes a problem of computing marginal distributions, and
  finding efficient optimization methods. To that extent we propose an
  optimization scheme which extends the Concave Convex Procedure (CCP)
  of Yuille and Rangarajan, and present a simplified and intuitive
  proof of its convergence. We show how our algorithm can be
  specialized to various cases in order to efficiently solve the
  optimization problems that arise. Encouraging preliminary
  experimental results on the USPS dataset are also presented.},
author = {Smola, Alex and Vishwanathan, S V N and Hoffman, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Vishwanathan, Hoffman - 2005 - Kernel Methods for Missing Variables.pdf:pdf},
isbn = {097273581X},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
title = {{Kernel Methods for Missing Variables}},
url = {http://eprints.pascal-network.org/archive/00002053/},
year = {2005}
}
@article{Parker2012,
abstract = {Measurements from microarrays and other high-throughput technologies are susceptible to non-biological artifacts like batch effects. It is known that batch effects can alter or obscure the set of significant results and biological conclusions in high-throughput studies. Here we examine the impact of batch effects on predictors built from genomic technologies. To investigate batch effects, we collected publicly available gene expression measurements with known outcomes, and estimated batches using date. Using these data we show (1) the impact of batch effects on prediction depends on the correlation between outcome and batch in the training data, and (2) removing expression measurements most affected by batch before building predictors may improve the accuracy of those predictors. These results suggest that (1) training sets should be designed to minimize correlation between batches and outcome, and (2) methods for identifying batch-affected probes should be developed to improve prediction results for studies with high correlation between batches and outcome.},
author = {Parker, Hilary S. and Leek, Jeffrey T.},
doi = {10.1515/1544-6115.1766},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Parker, Leek - 2012 - The practical effect of batch on genomic prediction.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
number = {3},
pmid = {22611599},
title = {{The practical effect of batch on genomic prediction}},
volume = {11},
year = {2012}
}
@article{Kelejian1969,
author = {Kelejian, H.H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kelejian - 1969 - Missing Observations in Multivaraite Regression Efficiency of a First-Order Method.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1609--1616},
title = {{Missing Observations in Multivaraite Regression: Efficiency of a First-Order Method}},
volume = {64},
year = {1969}
}
@article{Dagenais1971,
abstract = {The purpose of this article is to suggest a method of estimating parameters of linear regressions containing two independent variables, when data is missing among these variables. The problem envisaged concerns the case where: (1) the independent variables are considered as fixed numbers; (2) each observation contains the values of the dependent variable and at least one of the independent variables; (3) some observations are complete. In contrast with other approaches dealing with similar problems, the technique developed in this article has the following advantages: (1) it is based on rather unrestrictive hypotheses; (2) the resulting estimators are consistent; (3) the asymptotic variances of these estimators are smaller than those of comparable estimators described in the literature. Although the question is not examined in the present article, it seems also that the proposed method offers good possibilities of generalization.},
author = {Dagenais, Marcel G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dagenais - 1971 - Further suggestions concerning the utilization of incomplete observations in regression analysis.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Missing data,Model specification},
number = {333},
pages = {93--98},
title = {{Further suggestions concerning the utilization of incomplete observations in regression analysis }},
volume = {66},
year = {1971}
}
@article{Rubin1976,
abstract = {SUMMARY When making sampling distribution inferences about the parameter of the data, $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are âmissing at random' and the observed data are âobserved at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is âdistinct' from $\theta$. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
author = {Rubin, Donald B.},
doi = {10.2307/2335739},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 1976 - Inference and Missing Data.pdf:pdf},
isbn = {0006344414643510},
issn = {00063444},
journal = {Biometrika},
number = {3},
pages = {581},
pmid = {86},
title = {{Inference and Missing Data}},
url = {http://biomet.oxfordjournals.org.libproxy1.nus.edu.sg/content/63/3/581$\backslash$nhttp://www.jstor.org/stable/2335739?origin=crossref},
volume = {63},
year = {1976}
}
@article{Pearson1926,
author = {Pearson, Karl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearson - 1926 - Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population.pdf:pdf},
journal = {Proceedings of the Royal Society of London. Series A},
number = {760},
pages = {1--14},
title = {{Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population}},
volume = {112},
year = {1926}
}
@article{Minka2005,
abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
author = {Minka, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 2005 - Divergence measures and message passing.pdf:pdf},
pages = {MSR--TR--2005--173},
title = {{Divergence measures and message passing}},
year = {2005}
}
@article{Hoffman2013,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
eprint = {1206.7051},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
number = {2},
pages = {1303--1347},
title = {{Stochastic Variational Inference}},
url = {http://jmlr.org/papers/v14/hoffman13a.html$\backslash$nhttp://arxiv.org/abs/1206.7051},
volume = {14},
year = {2013}
}
@article{Ranganath2013,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ranganath, Gerrish, Blei - 2013 - Black Box Variational Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1401.0118},
title = {{Black Box Variational Inference}},
url = {http://arxiv.org/abs/1401.0118},
year = {2013}
}
@article{Maaten2014,
author = {Maaten, Laurens Van Der},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maaten - 2014 - Accelerating t-SNE using Tree-Based Algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {barnes-hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-sne},
pages = {3221â3245},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
volume = {15},
year = {2014}
}
@article{Platanios2014,
author = {Platanios, Emmanouil Antonios and Blum, Avrim and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Platanios, Blum, Mitchell - 2014 - Estimating Accuracy from Unlabeled Data.pdf:pdf},
isbn = {9780974903910},
journal = {30th Conference on Uncertainty in Artificial Intelligence},
title = {{Estimating Accuracy from Unlabeled Data}},
year = {2014}
}
@article{Germain2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06944v1},
author = {Germain, Pascal},
eprint = {arXiv:1503.06944v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Germain - 2013 - PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers.pdf:pdf},
title = {{PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers}},
year = {2013}
}
@article{Xue2015,
author = {Xue, Jing-hao and Hall, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xue, Hall - 2015 - Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {5},
pages = {1109--1112},
title = {{Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis?}},
volume = {37},
year = {2015}
}
@article{Rooyen,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00083v1},
author = {Rooyen, Brendan Van and Williamson, Robert C},
eprint = {arXiv:1504.00083v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rooyen, Williamson - Unknown - A Theory of Feature Learning.pdf:pdf},
title = {{A Theory of Feature Learning}}
}
@article{Patel2015,
abstract = {A grand challenge in machine learning is the development of computational al-gorithms that match or outperform humans in perceptual inference tasks such as visual object and speech recognition. The key factor complicating such tasks is the presence of numerous nuisance variables, for instance, the unknown object position, orientation, and scale in object recognition or the unknown voice pronunciation, pitch, and speed in speech recognition. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks; they are constructed from many layers of alternating linear and nonlin-ear processing units and are trained using large-scale algorithms and massive amounts of training data. The recent success of deep learning systems is im-pressive â they now routinely yield pattern recognition systems with near-or super-human capabilities â but a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on a Bayesian generative probabilistic model that explicitly cap-tures variation due to nuisance variables. The graphical structure of the model enables it to be learned from data using classical expectation-maximization techniques. Furthermore, by relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks (DCNs) and random decision forests (RDFs), providing insights into their successes and shortcomings as well as a princi-pled route to their improvement.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00641v1},
author = {Patel, Ankit B and Nguyen, Tan and Baraniuk, Richard G},
eprint = {arXiv:1504.00641v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patel, Nguyen, Baraniuk - 2015 - A Probabilistic Theory of Deep Learning.pdf:pdf},
pages = {1--56},
title = {{A Probabilistic Theory of Deep Learning}},
year = {2015}
}
@article{Ma2008,
abstract = {In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.},
author = {Ma, Shuangge and Huang, Jian},
doi = {10.1093/bib/bbn027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Huang - 2008 - Penalized feature selection and classification in bioinformatics.pdf:pdf},
isbn = {1477-4054},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics application,Feature selection,Penalization},
number = {5},
pages = {392--403},
pmid = {18562478},
title = {{Penalized feature selection and classification in bioinformatics}},
volume = {9},
year = {2008}
}
@article{Marchand,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08329v1},
author = {Marchand, Mario and Roy, Jean-francis},
eprint = {arXiv:1503.08329v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Roy - Unknown - PAC-Bayesian Bounds for the Risk of the Majority Vote.pdf:pdf},
keywords = {ensemble methods,learning theory,majority vote,pac-bayesian theory},
number = {2006},
title = {{PAC-Bayesian Bounds for the Risk of the Majority Vote}}
}
@article{Piironen2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
eprint = {arXiv:1503.08650v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Piironen, Vehtari - 2012 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
keywords = {bayesian model selection,cross-validation,map,median,overfitting,probability model,projection,reference model,selection bias,waic},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2012}
}
@article{Johnson,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01255v1},
author = {Johnson, Rie},
eprint = {arXiv:1504.01255v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - Unknown - Semi-Supervised Learning with Multi-View Embedding Theory and Application with Convolutional Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Semi-Supervised Learning with Multi-View Embedding : Theory and Application with Convolutional Neural Networks}}
}
@article{Adams,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01344v1},
author = {Adams, Ryan P},
eprint = {arXiv:1504.01344v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - Unknown - Early Stopping is Nonparametric Variational Inference.pdf:pdf},
title = {{Early Stopping is Nonparametric Variational Inference}}
}
@article{Minka1998,
author = {Minka, Thomas P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 1998 - Escpectation-Maximization as lower bound maximization.pdf:pdf},
number = {1977},
pages = {1--8},
title = {{Escpectation-Maximization as lower bound maximization}},
year = {1998}
}
@article{Kall2007,
abstract = {Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17{\%} more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77{\%} more spectra from non-tryptic digests, relative to a fully supervised approach.},
author = {K{\"{a}}ll, Lukas and Canterbury, Jesse D and Weston, Jason and Noble, William Stafford and MacCoss, Michael J},
doi = {10.1038/nmeth1113},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K{\"{a}}ll et al. - 2007 - Semi-supervised learning for peptide identification from shotgun proteomics datasets.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature methods},
number = {11},
pages = {923--925},
pmid = {17952086},
title = {{Semi-supervised learning for peptide identification from shotgun proteomics datasets.}},
volume = {4},
year = {2007}
}
@article{Tuia,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02338v1},
author = {Tuia, Devis and Member, Senior and Camps-valls, Gustau and Member, Senior},
eprint = {arXiv:1504.02338v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tuia et al. - Unknown - Kernel Manifold Alignment.pdf:pdf},
pages = {1--6},
title = {{Kernel Manifold Alignment}}
}
@article{Sivaganesan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02689v1},
author = {Berger, James O. and Bernardo, Jose M. and Sun, Dongchu},
doi = {10.1214/14-BA935},
eprint = {arXiv:1504.02689v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berger, Bernardo, Sun - 2015 - Overall Objective Priors.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Joint Reference Prior,Logarithmic Divergence,Mul,joint reference prior,logarithmic divergence,multinomial model,objective priors,reference analysis},
number = {1},
pages = {189--221},
title = {{Overall Objective Priors}},
url = {http://projecteuclid.org/euclid.ba/1422556417},
year = {2015}
}
@article{Le2015,
author = {Le, Thanh-Binh and Kim, Sang-Woon},
doi = {10.1016/j.patrec.2015.04.011},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Le, Kim - 2015 - Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Semi-supervised learning,Semi-supervised boosting,,semi-supervised boosting,semi-supervised learning,support vector machines},
pages = {48--56},
publisher = {Elsevier Ltd.},
title = {{Modified criterion to select useful unlabeled data for improving semi-supervised support vector machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001282},
volume = {60-61},
year = {2015}
}
@article{Duin2015,
author = {Duin, Robert P.W.},
doi = {10.1016/j.patrec.2015.04.015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin - 2015 - The dissimilarity representation for finding universals from particulars by an anti-essentialist approach.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Anti-essentialism,Generalization,Nearest Neighbor Rule,Representation},
publisher = {Elsevier Ltd.},
title = {{The dissimilarity representation for finding universals from particulars by an anti-essentialist approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001324},
year = {2015}
}
@article{Bloodgood2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06329v1},
author = {Bloodgood, Michael and Grothendieck, John},
eprint = {arXiv:1504.06329v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bloodgood, Grothendieck - 2013 - Analysis of Stopping Active Learning based on Stabilizing Predictions.pdf:pdf},
journal = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
pages = {10--19},
title = {{Analysis of Stopping Active Learning based on Stabilizing Predictions}},
url = {http://www.aclweb.org/anthology/W13-3502},
year = {2013}
}
@article{Matthews2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.07027v1},
author = {Matthews, Alexander G De G and Hensman, James and Turner, Richard E and Ghahramani, Zoubin},
eprint = {arXiv:1504.07027v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Matthews et al. - 2015 - On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes.pdf:pdf},
number = {1},
pages = {1--8},
title = {{On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes}},
year = {2015}
}
@article{Rasmus2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08215v1},
author = {Rasmus, Antti and Valpola, Harri},
eprint = {arXiv:1504.08215v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmus, Valpola - 2015 - Lateral Connections in Denoising Autoencoders Support Supervised Learning.pdf:pdf},
pages = {1--5},
title = {{Lateral Connections in Denoising Autoencoders Support Supervised Learning}},
year = {2015}
}
@article{Ramdas2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04214v1},
author = {Ramdas, Aaditya and Singh, Aarti},
doi = {10.1109/GlobalSIP.2013.6737091},
eprint = {arXiv:1505.04214v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ramdas, Singh - 2013 - Exploring the intersection of active learning and stochastic convex optimization.pdf:pdf},
isbn = {9781479902484},
issn = {03029743},
journal = {2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2013 - Proceedings},
keywords = {Active learning,Adaptive algorithms,Stochastic convex optimization,Tsybakov noise condition,Uniform convexity},
pages = {1122},
title = {{Exploring the intersection of active learning and stochastic convex optimization}},
year = {2013}
}
@article{Huggins2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04984v1},
author = {Huggins, Jonathan H and Tenenbaum, Joshua B},
eprint = {arXiv:1505.04984v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huggins, Tenenbaum - 2015 - Risk and Regret of Hierarchical Bayesian Learners.pdf:pdf},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
title = {{Risk and Regret of Hierarchical Bayesian Learners}},
volume = {37},
year = {2015}
}
@article{Nguyen2015,
author = {Nguyen, Thanh and Khosravi, Abbas and Creighton, Douglas and Nahavandi, Saeid},
doi = {10.1016/j.patrec.2015.03.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen et al. - 2015 - A novel aggregate gene selection method for microarray data classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Analytic hierarchy process,Classification,Gene expression profiles,Gene selection,Microarray data},
pages = {16--23},
publisher = {Elsevier Ltd.},
title = {{A novel aggregate gene selection method for microarray data classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001105},
volume = {60-61},
year = {2015}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learn-ing a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a com-pression cost, known as the variational free en-ergy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforce-ment learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05424v2},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {arXiv:1505.05424v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
title = {{Weight Uncertainty in Neural Networks}},
volume = {37},
year = {2015}
}
@article{Ajakan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.07818v1},
author = {Ajakan, Hana and Larochelle, Hugo and Marchand, Mario and Lempitsky, Victor},
eprint = {arXiv:1505.07818v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ajakan et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
journal = {arXiv},
title = {{Domain-Adversarial Training of Neural Networks}},
year = {2015}
}
@article{Ho2004,
abstract = {Studies on ensemble methods for classification suffer from the difficulty of modeling the complementary strengths of the components. Kleinberg's theory of stochastic discrimination (SD) addresses this rigorously via mathematical notions of enrichment, uniformity, and projectability of an ensemble. We explain these concepts via a very simple numerical example that captures the basic principles of the SD theory and method. We focus on a fundamental symmetry in point set covering that is the key observation leading to the foundation of the theory. We believe a better understanding of the SD method will lead to developments of better tools for analyzing other ensemble methods.},
archivePrefix = {arXiv},
arxivId = {cs/0402021},
author = {Ho, Tin Kam},
eprint = {0402021},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2004 - A Numerical Example on the Principles of Stochastic Discrimination.pdf:pdf},
primaryClass = {cs},
title = {{A Numerical Example on the Principles of Stochastic Discrimination}},
url = {http://arxiv.org/abs/cs/0402021},
year = {2004}
}
@article{Janson2015,
author = {Janson, L. and Fithian, W. and Hastie, T. J.},
doi = {10.1093/biomet/asv019},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janson, Fithian, Hastie - 2015 - Effective degrees of freedom a flawed metaphor.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {May},
pages = {479--485},
title = {{Effective degrees of freedom: a flawed metaphor}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asv019},
year = {2015}
}
@article{Zou2014,
abstract = {In epigenome-wide association studies, cell-type composition often differs between cases and controls, yielding associations that simply tag cell type rather than reveal fundamental biology. Current solutions require actual or estimated cell-type composition--information not easily obtainable for many samples of interest. We propose a method, FaST-LMM-EWASher, that automatically corrects for cell-type composition without the need for explicit knowledge of it, and then validate our method by comparison with the state-of-the-art approach. Corresponding software is available from http://www.microsoft.com/science/.},
author = {Zou, James and Lippert, Christoph and Heckerman, David and Aryee, Martin and Listgarten, Jennifer},
doi = {10.1038/nmeth.2815},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zou et al. - 2014 - Epigenome-wide association studies without the need for cell-type composition.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7105},
journal = {Nature methods},
keywords = {Cells,Epigenomics,Genome-Wide Association Study,Humans,Linear Models},
number = {3},
pages = {309--11},
pmid = {24464286},
title = {{Epigenome-wide association studies without the need for cell-type composition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24464286},
volume = {11},
year = {2014}
}
@article{Ghahramani2002,
author = {Ghahramani, Z},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani - 2002 - Graphical models parameter learning.pdf:pdf},
keywords = {BRAIN, learning, MODEL, models, Theories},
pages = {1--305},
title = {{Graphical models: parameter learning}},
url = {http://discovery.ucl.ac.uk/185880/},
volume = {1},
year = {2002}
}
@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rahimi, Recht - 2007 - Random features for large-scale kernel machines.pdf:pdf},
isbn = {160560352X},
journal = {Advances in neural information {\ldots}},
number = {1},
pages = {1--8},
title = {{Random features for large-scale kernel machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}
@article{Yu2006,
abstract = {This paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y = f(x). We propose a novel and simple concept for active learning, transductive experimental design, that explores available unmeasured experiments (i.e., unlabeled data) and has a better scalability in comparison with classic experimental design methods. Our in-depth analysis shows that the new method tends to favor experiments that are on the one side hard-to-predict and on the other side representative for the rest of the experiments. Efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search. Extensive experimental results on synthetic problems and three real-world tasks, including questionnaire design for preference learning, active learning for text categorization, and spatial sensor placement, highlight the advantages of the proposed approaches.},
author = {Yu, Kai and Bi, Jinbo and Tresp, Volker},
doi = {10.1145/1143844.1143980},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu, Bi, Tresp - 2006 - Active learning via transductive experimental design.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning ICML 06},
number = {6},
pages = {1081--1088},
title = {{Active learning via transductive experimental design}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143980},
volume = {148},
year = {2006}
}
@article{Kosmidis2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01388v1},
author = {Kosmidis, Ioannis and Passfield, Louis},
eprint = {arXiv:1506.01388v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kosmidis, Passfield - 2015 - Linking the performance of endurance runners to training and physiological effects via multi-resolution ela.pdf:pdf},
keywords = {collinearity,grouping effect,law,power,regularization,training distribution profile,wearable gps devices},
number = {1975},
title = {{Linking the performance of endurance runners to training and physiological effects via multi-resolution elastic net}},
year = {2015}
}
@article{Rooyena,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01520v1},
author = {Rooyen, Brendan Van and Menon, Aditya Krishna and Williamson, Robert C},
eprint = {arXiv:1506.01520v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rooyen, Menon, Williamson - Unknown - An Average Classification Algorithm.pdf:pdf},
pages = {1--16},
title = {{An Average Classification Algorithm}}
}
@article{Wanga,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01782v1},
author = {Wang, Xiangyu and Leng, Chenlei},
eprint = {arXiv:1506.01782v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Leng - Unknown - High-dimensional Ordinary Least-squares Projection for Screening Variables.pdf:pdf},
keywords = {consistency,forward regression,generalized inverse,high dimensionality,lasso,marginal correlation,moore-penrose inverse,ordinary least squares,screening,sure independent,variable selection},
pages = {1--49},
title = {{High-dimensional Ordinary Least-squares Projection for Screening Variables}}
}
@article{Gal,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02142v1},
author = {Gal, Yarin},
eprint = {arXiv:1506.02142v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gal - Unknown - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
pages = {1--10},
title = {{Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning}}
}
@article{Betancourt2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02273v1},
author = {Betancourt, Michael},
eprint = {arXiv:1506.02273v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - A Unified Treatment of Predictive Model Comparison arXiv 1506 . 02273v1 stat . ME 7 Jun 2015.pdf:pdf},
pages = {1--20},
title = {{A Unified Treatment of Predictive Model Comparison arXiv : 1506 . 02273v1 [ stat . ME ] 7 Jun 2015}},
year = {2015}
}
@article{Tsakonas2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02348v1},
author = {Tsakonas, Efthymios and Jald{\'{e}}n, Joakim and Member, Senior and Sidiropoulos, Nicholas D and Ottersten, Bjorn},
eprint = {arXiv:1506.02348v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tsakonas et al. - 2013 - Likelihood Estimation.pdf:pdf},
number = {22},
pages = {5704--5715},
title = {{Likelihood Estimation}},
volume = {61},
year = {2013}
}
@article{Brown2015,
author = {Brown, Gavin},
doi = {10.1016/j.patrec.2015.04.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brown - 2015 - On Unifiers, Diversifiers, and the Nature of Pattern Recognition.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Nature of pattern recognition,Unifying,Diversifyin,nature of pattern recognition},
pages = {1--10},
publisher = {Elsevier Ltd.},
title = {{On Unifiers, Diversifiers, and the Nature of Pattern Recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001312},
volume = {000},
year = {2015}
}
@article{Seth2013,
abstract = {Archetypal analysis represents a set of observations as convex combinations of pure patterns, or archetypes. The original geometric formulation of finding archetypes by approximating the convex hull of the observations assumes them to be real valued. This, unfortunately, is not compatible with many practical situations. In this paper we revisit archetypal analysis from the basic principles, and propose a probabilistic framework that accommodates other observation types such as integers, binary, and probability vectors. We corroborate the proposed methodology with convincing real-world applications on finding archetypal winter tourists based on binary survey data, archetypal disaster-affected countries based on disaster count data, and document archetypes based on term-frequency data. We also present an appropriate visualization tool to summarize archetypal analysis solution better.},
archivePrefix = {arXiv},
arxivId = {1312.7604},
author = {Seth, Sohan and Eugster, Manuel J. a.},
doi = {10.1007/s10994-015-5498-8},
eprint = {1312.7604},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seth, Eugster - 2013 - Probabilistic Archetypal Analysis.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Archetypal analysis,Binary observation,Convex hull,Majorizationâminimization,Probabilistic modeling,Visualization,archetypal analysis,binary observation,convex hull,majorization,minimization,probabilistic modeling,visualization},
number = {April 2014},
pages = {24},
publisher = {Springer US},
title = {{Probabilistic Archetypal Analysis}},
url = {http://arxiv.org/abs/1312.7604},
year = {2013}
}
@article{Grunwald2007a,
abstract = {We show that forms of Bayesian and MDL inference that are often applied to classification problems can be {\{}$\backslash$em inconsistent{\}}.  This means that there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error. We extensively discuss the result from both a Bayesian and an MDL perspective.},
archivePrefix = {arXiv},
arxivId = {math/0406221},
author = {Gr{\"{u}}nwald, Peter and Langford, John},
doi = {10.1007/s10994-007-0716-7},
eprint = {0406221},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Langford - 2007 - Suboptimal behavior of Bayes and MDL in classification under misspecification.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Bayesian statistics,Classification,Consistency,Inconsistency,Minimum description length,Misspecification},
number = {2-3},
pages = {119--149},
primaryClass = {math},
title = {{Suboptimal behavior of Bayes and MDL in classification under misspecification}},
volume = {66},
year = {2007}
}
@article{Wang2015a,
author = {Wang, Fang and Li, Renfu and Lei, Zhikun and Ni, Xuelei Sherry and Huo, Xiaoming and Chen, Ming},
doi = {10.1016/j.patrec.2015.06.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dimension reduction,Semi-supervised learning,kernel fusion-refinement},
publisher = {Elsevier Ltd.},
title = {{Kernel Fusion-Refinement for Semi-supervised Nonlinear Dimension Reduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001671},
year = {2015}
}
@article{Ma2015,
author = {Ma, Jianping and Jiang, Jin},
doi = {10.1016/j.net.2014.12.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Jiang - 2015 - Semisupervised classification for fault diagnosis in nuclear power plants.pdf:pdf},
issn = {17385733},
journal = {Nuclear Engineering and Technology},
number = {2},
pages = {176--186},
publisher = {Elsevier B.V},
title = {{Semisupervised classification for fault diagnosis in nuclear power plants}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1738573315000054},
volume = {47},
year = {2015}
}
@article{Hayashi2015,
author = {Hayashi, K. and Takai, K.},
doi = {10.1080/03610918.2014.957847},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hayashi, Takai - 2015 - Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analy.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {June},
pages = {00--00},
title = {{Finite-sample analysis of impacts of unlabelled data and their labelling mechanisms in linear discriminant analysis}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918.2014.957847},
year = {2015}
}
@article{Esposito2015,
author = {Esposito, G. and Martin, M.},
doi = {10.1080/08839514.2015.1035951},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Esposito, Martin - 2015 - A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines.pdf:pdf},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
number = {5},
pages = {459--479},
title = {{A Randomized Algorithm for the Exact Solution of Transductive Support Vector Machines}},
url = {http://www.tandfonline.com/doi/full/10.1080/08839514.2015.1035951},
volume = {29},
year = {2015}
}
@article{Bodo2015,
abstract = {{\textless}p{\textgreater}Semi-supervised learning has become an important and thoroughly studied subdomain of machine learning in the past few years, because gathering large unlabeled data is almost costless, and the costly human labeling process can be minimized by semi-supervision. Label propagation is a transductive semi-supervised learning method that operates on theâmost of the time undirectedâdata graph. It was introduced in [8] and since many variants were proposed. However, the base algorithm has two variants: the first variant presented in [8] and its slightly modified version used afterwards, e.g. in [7]. This paper presents and compares the two algorithmsâboth theoretically and experimentallyâand also tries to make a recommendation which variant to use.{\textless}/p{\textgreater}},
author = {Bod{\'{o}}, Zal{\'{a}}n and Csat{\'{o}}, Lehel},
doi = {10.1515/ausi-2015-0010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bod{\'{o}}, Csat{\'{o}} - 2015 - A note on label propagation for semi-supervised learning.pdf:pdf},
issn = {2066-7760},
journal = {Acta Universitatis Sapientiae, Informatica},
keywords = {and phrases,label propagation,semi-supervised learning},
number = {1},
pages = {18--30},
title = {{A note on label propagation for semi-supervised learning}},
url = {http://www.degruyter.com/view/j/ausi.2015.7.issue-1/ausi-2015-0010/ausi-2015-0010.xml},
volume = {7},
year = {2015}
}
@article{Grunwald2004,
abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss.  Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other.  Although Topsoe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. 

We here generalize this theory to apply to arbitrary decision problems and loss functions.  We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context.  This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts.  We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions.  This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the "redundancy-capacity theorem'' of information theory).  For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions.},
archivePrefix = {arXiv},
arxivId = {math/0410076},
author = {Gr{\"{u}}nwald, Peter D. and {Philip Dawid}, a.},
doi = {10.1214/009053604000000553},
eprint = {0410076},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr{\"{u}}nwald, Philip Dawid - 2004 - Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Additive model,Bayes act,Bregman divergence,Brier score,Convexity,Duality,Equalizer rule,Exponential family,Gamma-minimax,Generalized exponential family,Kullback-Leibler divergence,Logarithmic score,Maximin,Mean-value constraints,Minimax},
number = {4},
pages = {1367--1433},
primaryClass = {math},
title = {{Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory}},
volume = {32},
year = {2004}
}
@article{Lei2015,
author = {Lei, Zhikun and Li, Renfu and {Sherry Ni}, Xuelei and Huo, Xiaoming},
doi = {10.1016/j.sigpro.2015.03.003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lei et al. - 2015 - High-dimensional semi-supervised learning via a fusion-refinement procedure.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Fusion-refinement (FR) procedure,Semi-supervised learning (SSL),Sufficient dimension reduction (SDR),fr,fusion-refinement,procedure,sdr,semi-supervised learning,ssl,sufficient dimension reduction},
pages = {171--182},
publisher = {Elsevier},
title = {{High-dimensional semi-supervised learning via a fusion-refinement procedure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168415000961},
volume = {114},
year = {2015}
}
@article{Markatou2005,
author = {Markatou, Marianthi and Tian, H},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Markatou, Tian - 2005 - Analysis of variance of cross-validation estimators of the generalization error.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine {\ldots}},
keywords = {cross-validation,generalization error,moment approximation,prediction,variance},
pages = {1127--1168},
title = {{Analysis of variance of cross-validation estimators of the generalization error}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/MarkatouTBH05.pdf},
volume = {6},
year = {2005}
}
@article{Tanha2015,
author = {Tanha, Jafar and van Someren, Maarten and Afsarmanesh, Hamideh},
doi = {10.1007/s13042-015-0328-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tanha, van Someren, Afsarmanesh - 2015 - Semi-supervised self-training for decision tree classifiers.pdf:pdf},
isbn = {1304201503},
issn = {1868-8071},
journal = {International Journal of Machine Learning and Cybernetics},
number = {JANUARY},
title = {{Semi-supervised self-training for decision tree classifiers}},
url = {http://link.springer.com/10.1007/s13042-015-0328-7},
year = {2015}
}
@article{Drummond2009,
abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount.  I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two.  Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
author = {Drummond, Dr. Chris},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Drummond - 2009 - Replicability is not Reproducibility Nor is it Good Science.pdf:pdf},
isbn = {978-1-60558-516-1},
keywords = {Artificial Intelligence},
number = {2005},
pages = {2005--2008},
pmid = {1000044434},
title = {{Replicability is not Reproducibility: Nor is it Good Science}},
url = {http://cogprints.org/7691/},
year = {2009}
}
@article{Hullermeier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0698v1},
author = {H{\"{u}}llermeier, Eyke},
doi = {10.1016/j.ijar.2013.09.003},
eprint = {arXiv:1305.0698v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/H{\"{u}}llermeier - 2013 - Learning from imprecise and fuzzy observations Data disambiguation through generalized loss minimization.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {data disambiguation,extension principle,fuzzy sets,imprecise data,inductive bias,logistic,loss function,machine learning,risk minimization},
pages = {1--16},
title = {{Learning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0888613X13001722},
volume = {1},
year = {2013}
}
@article{Hallac2015,
abstract = {Convex optimization is an essential tool for modern data analysis, as it provides a framework to formulate and solve many problems in machine learning and data mining. However, general convex optimization solvers do not scale well, and scalable solvers are often specialized to only work on a narrow class of problems. Therefore, there is a need for simple, scalable algorithms that can solve many common optimization problems. In this paper, we introduce the $\backslash$emph{\{}network lasso{\}}, a generalization of the group lasso to a network setting that allows for simultaneous clustering and optimization on graphs. We develop an algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve this problem in a distributed and scalable manner, which allows for guaranteed global convergence even on large graphs. We also examine a non-convex extension of this approach. We then demonstrate that many types of problems can be expressed in our framework. We focus on three in particular - binary classification, predicting housing prices, and event detection in time series data - comparing the network lasso to baseline approaches and showing that it is both a fast and accurate method of solving large optimization problems.},
archivePrefix = {arXiv},
arxivId = {1507.00280},
author = {Hallac, David and Leskovec, Jure and Boyd, Stephen},
doi = {10.1145/2783258.2783313},
eprint = {1507.00280},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hallac, Leskovec, Boyd - 2015 - Network Lasso Clustering and Optimization in Large Graphs.pdf:pdf},
isbn = {9781450336642},
keywords = {27,admm,and to,apply to a variety,convex optimization,formulate general classes of,it is necessary to,model,network lasso,of relevant problems,often an infeasible assumption,optimiza-,ploit structure in the,therefore,tion solvers that can},
title = {{Network Lasso: Clustering and Optimization in Large Graphs}},
url = {http://arxiv.org/abs/1507.00280 http://dx.doi.org/10.1145/2783258.2783313},
year = {2015}
}
@article{Vehtari2015,
abstract = {Importance weighting is a convenient general way to adjust for draws from the wrong distribution, but the resulting ratio estimate can be noisy when the importance weights have a heavy right tail, as routinely occurs when there are aspects of the target distribution not well captured by the approximating distribution. More stable estimates can be obtained by truncating the importance ratios. Here we present a new method for stabilizing importance weights using generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios.},
archivePrefix = {arXiv},
arxivId = {1507.02646},
author = {Vehtari, Aki and Gelman, Andrew},
eprint = {1507.02646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vehtari, Gelman - 2015 - Very Good Importance Sampling.pdf:pdf},
keywords = {bayesian computation,importance sampling,leave-one-,loo,monte carlo,out cross-validation},
number = {July},
title = {{Very Good Importance Sampling}},
url = {http://arxiv.org/abs/1507.02646},
year = {2015}
}
@inproceedings{Rasmus2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02672v1},
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1507.02672v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rasmus et al. - 2015 - Semi-Supervised Learning with Ladder Network.pdf:pdf},
pages = {3532--3540},
title = {{Semi-Supervised Learning with Ladder Network}},
year = {2015}
}
@article{Orhan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04155v1},
author = {Orhan, Cem},
eprint = {arXiv:1507.04155v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Orhan - 2015 - ALEVS Active Learning by Statistical Leverage Sampling.pdf:pdf},
keywords = {Active learning, Statistical leverage, Classificat},
title = {{ALEVS: Active Learning by Statistical Leverage Sampling}},
year = {2015}
}
@article{Vehtari2015a,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We compute LOO using Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
eprint = {1507.04544},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vehtari, Gelman, Gabry - 2015 - Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian model.pdf:pdf},
keywords = {bayesian computation,k -fold cross-validation,leave-one-out cross-validation,loo,pareto smoothed importance sampling,stan,waic,widely applicable information criterion},
number = {July},
title = {{Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian models}},
url = {http://arxiv.org/abs/1507.04544},
year = {2015}
}
@article{Zafar2015,
abstract = {Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead to user discrimination, even in the absence of intent. In this paper, we introduce fairness constraints, a mechanism to ensure fairness in a wide variety of classifiers in a principled manner. Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data. We then instantiate fairness constraints on three well-known classifiers -- logistic regression, hinge loss and support vector machines (SVM) -- and evaluate their performance in a real-world dataset with meaningful sensitive human attributes. Experiments show that fairness constraints allow for an optimal trade-off between accuracy and fairness.},
archivePrefix = {arXiv},
arxivId = {1507.05259},
author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
eprint = {1507.05259},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zafar et al. - 2015 - Fairness Constraints A Mechanism for Fair Classification.pdf:pdf},
title = {{Fairness Constraints: A Mechanism for Fair Classification}},
url = {http://arxiv.org/abs/1507.05259},
year = {2015}
}
@article{Rojas-Carulla2015,
abstract = {From training data from several related domains (or tasks), methods of domain adaptation try to combine knowledge to improve performance. This paper discusses an approach to domain adaptation which is inspired by a causal interpretation of the multi-task problem. We assume that a covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant with respect to shifts in those predictors (covariates). We propose to learn the corresponding conditional expectation in the training domains and use it for estimation in the target domain. We further introduce a method which allows for automatic inference of the above subset in regression and classification. We study the performance of this approach in an adversarial setting, in the case where no additional examples are available in the test domain. If a labeled sample is available, we provide a method for using both the transferred invariant conditional and task specific information. We present results on synthetic data sets and a sentiment analysis problem.},
archivePrefix = {arXiv},
arxivId = {1507.05333},
author = {Rojas-Carulla, Mateo and Sch{\"{o}}lkopf, Bernhard and Turner, Richard and Peters, Jonas},
eprint = {1507.05333},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rojas-Carulla et al. - 2015 - A Causal Perspective on Domain Adaptation.pdf:pdf},
pages = {1--14},
title = {{A Causal Perspective on Domain Adaptation}},
url = {http://arxiv.org/abs/1507.05333},
year = {2015}
}
@article{Kyrillidis2015,
abstract = {Compressive sensing (CS) exploits sparsity to recover sparse or compressible signals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity is also used to enhance interpretability in machine learning and statistics applications: While the ambient dimension is vast in modern data analysis problems, the relevant information therein typically resides in a much lower dimensional space. However, many solutions proposed nowadays do not leverage the true underlying structure. Recent results in CS extend the simple sparsity idea to more sophisticated {\{}$\backslash$em structured{\}} sparsity models, which describe the interdependency between the nonzero components of a signal, allowing to increase the interpretability of the results and lead to better recovery performance. In order to better understand the impact of structured sparsity, in this chapter we analyze the connections between the discrete models and their convex relaxations, highlighting their relative advantages. We start with the general group sparse model and then elaborate on two important special cases: the dispersive and the hierarchical models. For each, we present the models in their discrete nature, discuss how to solve the ensuing discrete problems and then describe convex relaxations. We also consider more general structures as defined by set functions and present their convex proxies. Further, we discuss efficient optimization solutions for structured sparsity problems and illustrate structured sparsity in action via three applications.},
archivePrefix = {arXiv},
arxivId = {1507.05367},
author = {Kyrillidis, Anastasios and Baldassarre, Luca and El-Halabi, Marwa and Tran-Dinh, Quoc and Cevher, Volkan},
eprint = {1507.05367},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kyrillidis et al. - 2015 - Structured Sparsity Discrete and Convex approaches.pdf:pdf},
pages = {1--30},
title = {{Structured Sparsity: Discrete and Convex approaches}},
url = {http://arxiv.org/abs/1507.05367},
year = {2015}
}
@article{Denis2015,
abstract = {Confident prediction is highly relevant in machine learning; for example, in applications such as medical diagnoses, wrong prediction can be fatal. For classification, there already exist procedures that allow to not classify data when the confidence in their prediction is weak. This approach is known as classification with reject option. In the present paper, we provide new methodology for this approach. Predicting a new instance via a confidence set, we ensure an exact control of the probability of classification. Moreover, we show that this methodology is easily implementable and entails attractive theoretical and numerical properties.},
archivePrefix = {arXiv},
arxivId = {1507.07235},
author = {Denis, Christophe and Hebiri, Mohamed},
eprint = {1507.07235},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Denis, Hebiri - 2015 - Consistency of plug-in confidence sets for classification in semi-supervised learning.pdf:pdf},
keywords = {classification,classification with reject option,confidence sets,conformal predictors,plug-in confidence sets},
pages = {1--26},
title = {{Consistency of plug-in confidence sets for classification in semi-supervised learning}},
url = {http://arxiv.org/abs/1507.07235},
year = {2015}
}
@article{Briggs2015,
abstract = {Probability models are only useful at explaining the uncertainty of what we do not know, and should never be used to say what we already know. Probability and statistical models are useless at discerning cause. Classical statistical procedures, in both their frequentist and Bayesian implementations are, falsely imply they can speak about cause. No hypothesis test, or Bayes factor, should ever be used again. Even assuming we know the cause or partial cause for some set of observations, reporting via relative risk exagerates the certainty we have in the future, often by a lot. This over-certainty is made much worse when parametetric and not predictive methods are used. Unfortunately, predictive methods are rarely used; and even when they are, cause must still be an assumption, meaning (again) certainty in our scientific pronouncements is too high.},
archivePrefix = {arXiv},
arxivId = {1507.07244},
author = {Briggs, William M.},
eprint = {1507.07244},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Briggs - 2015 - The Crisis Of Evidence Why Probability And Statistics Cannot Discover Cause.pdf:pdf},
pages = {1--22},
title = {{The Crisis Of Evidence: Why Probability And Statistics Cannot Discover Cause}},
url = {http://arxiv.org/abs/1507.07244},
year = {2015}
}
@article{Gigerenzer2004,
abstract = {For instance, consider HIV screening for people who are in no known  group ( , 2002). In this population, the a priori probability p(H },
author = {Gigerenzer, Gerd and Krauss, Sv},
doi = {10.4135/9781412986311.n21},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gigerenzer, Krauss - 2004 - The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask.pdf:pdf},
isbn = {9780761923596},
journal = {The Sage Handbook of Methodology for the Social {\ldots}},
pages = {391--408},
title = {{The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Null+Ritual+What+You+Always+Wanted+to+Know+About+Signifi+cance+Testing+but+Were+Afraid+to+Ask{\#}3$\backslash$nhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Null+Ritual+What+You+Alway},
year = {2004}
}
@article{Gu2012,
author = {Gu, Quanquan and Han, Jiawei},
doi = {10.1109/ICDM.2012.72},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gu, Han - 2012 - Towards active learning on graphs An error bound minimization approach.pdf:pdf},
isbn = {9780769549057},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Active learning,Generalization error bound,Graph,Sequential optimization},
pages = {882--887},
title = {{Towards active learning on graphs: An error bound minimization approach}},
year = {2012}
}
@article{Li2009a,
abstract = {Semi-Supervised Support Vector Machines (S3VMs) typically directly$\backslash$nestimate the label assignments for the unlabeled instances. This$\backslash$nis often inefficient even with recent advances in the efficient training$\backslash$nof the (supervised) SVM. In this paper, we show that S3VMs, with$\backslash$nknowledge of the means of the class labels of the unlabeled data,$\backslash$nis closely related to the supervised SVM with known labels on all$\backslash$nthe unlabeled data. This motivates us to first estimate the label$\backslash$nmeans of the unlabeled data. Two versions of the meanS3VM, which$\backslash$nwork by maximizing the margin between the label means, are proposed.$\backslash$nThe first one is based on multiple kernel learning, while the second$\backslash$none is based on alternating optimization. Experiments show that both$\backslash$nof the proposed algorithms achieve highly competitive and sometimes$\backslash$neven the best performance as compared to the state-of-the-art semi-supervised$\backslash$nlearners. Moreover, they are more efficient than existing S3VMs.},
author = {Li, Yu-Feng and Kwok, James T. and Zhou, Zhi-hua},
doi = {10.1145/1553374.1553456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Kwok, Zhou - 2009 - Semi-supervised learning using label mean(2).pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {633--640},
title = {{Semi-supervised learning using label mean}},
url = {http://dl.acm.org/citation.cfm?id=1553456},
year = {2009}
}
@article{Sugiyama2010,
author = {Sugiyama, Masashi},
doi = {10.1587/transinf.E93.D.2690},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama - 2010 - Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting.pdf:pdf},
isbn = {0916-8532},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Classposterior probability,Kernel logistic regression,Probabilistic classification,Squared-loss},
number = {10},
pages = {2690--2701},
title = {{Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting}},
volume = {E93-D},
year = {2010}
}
@article{Small2015,
author = {Small, Dylan S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Small - 2015 - Introduction to Observational Studies and the Reprint of Cochran ' s paper â Observational Studies â and Comments C.pdf:pdf},
journal = {Observational studies},
pages = {124--125},
title = {{Introduction to Observational Studies and the Reprint of Cochran ' s paper â Observational Studies â and Comments Causal Thinking in the Twilight Zone}},
volume = {1},
year = {2015}
}
@article{OpenScienceCollaboration2015a,
author = {Collaboration, Open Science},
doi = {10.1126/science.aac4716},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6251},
pages = {aac4716--aac4716},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{Baclawski2009,
author = {Baclawski, Kenneth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baclawski - 2009 - Journal of Statistical Software.pdf:pdf},
isbn = {9781420065213},
keywords = {goodness of fit,model selection,variable importance},
number = {April},
pages = {1--3},
title = {{Journal of Statistical Software}},
volume = {30},
year = {2009}
}
@article{Heitjan1994,
author = {Heitjan, Daniel F and Landis, J Richard},
doi = {10.2307/2290900},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Heitjan, Landis - 1994 - Assessing Secular Trends in Blood Pressure {\{}A{\}} Multiple-imputation Approach.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian bootstrap,hot deck,incomplete data,missing data,observational study,predictive-mean matching},
number = {August 2015},
pages = {750--759},
title = {{Assessing Secular Trends in Blood Pressure: {\{}A{\}} Multiple-imputation Approach}},
volume = {89},
year = {1994}
}
@article{Yang2015,
author = {Yang, Yun and Liu, Xingchen},
doi = {10.1016/j.patrec.2015.08.009},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Liu - 2015 - A robust semi-supervised learning approach via mixture of label information.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Clustering,semi-supervised learning},
publisher = {Elsevier Ltd.},
title = {{A robust semi-supervised learning approach via mixture of label information}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515002688},
year = {2015}
}
@article{Lucic2015,
abstract = {Coresets are efficient representations of datasets such that models trained on a coreset are provably competitive with models trained on the original dataset. As such, they have been successfully used to scale up clustering models such as K-Means and Gaussian mixture models to massive datasets. However, until now, the algorithms and corresponding theory were usually specific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for a large class of hard and soft clustering problems based on Bregman divergences. This class includes hard clustering with popular distortion measures such as the Squared Euclidean distance, the Mahalanobis distance, KL-divergence, Itakura-Saito distance and relative entropy. The corresponding soft clustering problems are directly related to popular mixture models due to a dual relationship between Bregman divergences and Exponential family distributions. Our results recover existing coreset constructions for K-Means and Gaussian mixture models and imply polynomial time approximations schemes for various hard clustering problems.},
archivePrefix = {arXiv},
arxivId = {1508.05243},
author = {Lucic, Mario and Bachem, Olivier and Krause, Andreas},
eprint = {1508.05243},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lucic, Bachem, Krause - 2015 - Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures.pdf:pdf},
title = {{Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures}},
url = {http://arxiv.org/abs/1508.05243},
year = {2015}
}
@article{Gelman2015,
abstract = {We argue that the words " objectivity " and " subjectivity " in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality, and correspon-dence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. The advantage of these reformulations is that the replacement terms do not oppose each other. Instead of debating over whether a given statistical method is subjec-tive or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledg-ment of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling, and socioeconomic stratification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.05453v1},
author = {Gelman, Andrew and Hennig, Christian},
eprint = {arXiv:1508.05453v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hennig - 2015 - Beyond subjective and objective in statistics.pdf:pdf},
keywords = {()},
number = {August},
title = {{Beyond subjective and objective in statistics}},
year = {2015}
}
@article{Akande2015,
abstract = {Multiple imputation is a common approach for dealing with missing values in statistical databases. The imputer fills in missing values with draws from predictive models estimated from the observed data, resulting in multiple, completed versions of the database. Researchers have developed a variety of default routines to implement multiple imputation; however, there has been limited research comparing the performance of these methods, particularly for categorical data. We use simulation studies to compare repeated sampling properties of three default multiple imputation methods for categorical data, including chained equations using generalized linear models, chained equations using classification and regression trees, and a fully Bayesian joint distribution based on Dirichlet Process mixture models. We base the simulations on categorical data from the American Community Survey. The results suggest that default chained equations approaches based on generalized linear models are dominated by the default regression tree and mixture model approaches. They also suggest competing advantages for the regression tree and mixture model approaches, making both reasonable default engines for multiple imputation of categorical data.},
archivePrefix = {arXiv},
arxivId = {1508.05918},
author = {Akande, Olanrewaju and Li, Fan and Reiter, Jerome},
eprint = {1508.05918},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Akande, Li, Reiter - 2015 - An Empirical Comparison of Multiple Imputation Methods for Categorical Data.pdf:pdf},
pages = {1--26},
title = {{An Empirical Comparison of Multiple Imputation Methods for Categorical Data}},
url = {http://arxiv.org/abs/1508.05918},
volume = {27708},
year = {2015}
}
@article{Hothorn2015,
abstract = {We propose and study properties of maximum likelihood estimators in the class of conditional transformation models. Based on a suitable explicit parameterisation of the unconditional or conditional transformation function, we establish a cascade of increasingly complex transformation models that can be estimated, compared and analysed in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable can be set-up and estimated in the same theoretical and computational framework simply by choosing an appropriate transformation function and parameterisation thereof. The ability to evaluate the distribution function directly allows us to estimate models based on the exact full likelihood, especially in the presence of random censoring or truncation. For discrete and continuous responses, we establish the asymptotic normality of the proposed estimators. A reference software implementation of maximum likelihood-based estimation for conditional transformation models allowing the same flexibility as the theory developed here was employed to illustrate the wide range of possible applications.},
archivePrefix = {arXiv},
arxivId = {1508.06749},
author = {Hothorn, Torsten and M{\"{o}}st, Lisa and B{\"{u}}hlmann, Peter},
eprint = {1508.06749},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hothorn, M{\"{o}}st, B{\"{u}}hlmann - 2015 - Most Likely Transformations.pdf:pdf},
keywords = {censoring,conditional distribution function,conditional quantile function,distribution regression,transformation model,truncation},
title = {{Most Likely Transformations}},
url = {http://arxiv.org/abs/1508.06749},
year = {2015}
}
@article{Yang2015a,
abstract = {Fractional imputation (FI) is a relatively new method of imputation for handling item nonresponse in survey sampling. In FI, several imputed values with their fractional weights are created for each missing item. Each fractional weight represents the conditional probability of the imputed value given the observed data, and the parameters in the conditional probabilities are often computed by an iterative method such as EM algorithm. The underlying model for FI can be fully parametric, semiparametric, or nonparametric, depending on plausibility of assumptions and the data structure. In this paper, we give an overview of FI, introduce key ideas and methods to readers who are new to the FI literature, and highlight some new development. We also provide guidance on practical implementation of FI and valid inferential tools after imputation. We demonstrate the empirical performance of FI with respect to multiple imputation using a pseudo finite population generated from a sample in Monthly Retail Trade Survey in US Census Bureau.},
archivePrefix = {arXiv},
arxivId = {1508.06945},
author = {Yang, Shu and Kim, Jae Kwang},
eprint = {1508.06945},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Kim - 2015 - Fractional Imputation in Survey Sampling A Comparative Review.pdf:pdf},
keywords = {and phrases,carlo em,item nonresponse,missing at random,monte,multiple imputation,synthetic imputation},
title = {{Fractional Imputation in Survey Sampling: A Comparative Review}},
url = {http://arxiv.org/abs/1508.06945},
volume = {02115},
year = {2015}
}
@article{Louppe2015a,
abstract = {Author name disambiguation in bibliographic databases is the problem of grouping together scientific publications written by the same person, accounting for potential homonyms and/or synonyms. Among solutions to this problem, digital libraries are increasingly offering tools for authors to manually curate their publications and claim those that are theirs. Indirectly, these tools allow for the inexpensive collection of large annotated training data, which can be further leveraged to build a complementary automated disambiguation system capable of inferring patterns for identifying publications written by the same person. Building on more than 1 million publicly released crowdsourced annotations, we propose an automated author disambiguation solution exploiting this data (i) to learn an accurate classifier for identifying coreferring authors and (ii) to guide the clustering of scientific publications by distinct authors in a semi-supervised way. To the best of our knowledge, our analysis is the first to be carried out on data of this size and coverage. With respect to the state of the art, we validate the general pipeline used in most existing solutions, and improve by: (i) proposing phonetic-based blocking strategies, thereby increasing recall; and (ii) adding strong ethnicity-sensitive features for learning a linkage function, thereby tailoring disambiguation to non-Western author names whenever necessary.},
archivePrefix = {arXiv},
arxivId = {1508.07744},
author = {Louppe, Gilles and Al-Natsheh, Hussein and Susik, Mateusz and Maguire, Eamonn},
eprint = {1508.07744},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Louppe et al. - 2015 - Ethnicity sensitive author disambiguation using semi-supervised learning.pdf:pdf},
pages = {1--14},
title = {{Ethnicity sensitive author disambiguation using semi-supervised learning}},
url = {http://arxiv.org/abs/1508.07744},
year = {2015}
}
@article{Shimodaira2015,
abstract = {We derive an information criterion for selecting a parametric model of complete-data distribution when only incomplete or partially observed data is available. Compared with AIC, the new criterion has an additional penalty term for missing data expressed by the Fisher information matrices of complete data and incomplete data. We prove that the new criterion is an asymptotically unbiased estimator of the expected Kullback-Leibler divergence between the true distribution and the estimated distribution for complete data, whereas AIC is that for the incomplete data. Information criteria PDIO (Shimodaira 1994) and AICcd (Cavanaugh and Shumway 1998) have been previously proposed for the same purpose. Recently, an error is found in the derivation of PDIO, and the new information criterion is obtained by correcting the error. The additional penalty for missing data turns out to be only the half of what is claimed in PDIO. Geometrical view of alternating minimizations of the EM algorithm plays an important role for the derivation.},
archivePrefix = {arXiv},
arxivId = {1509.02870},
author = {Shimodaira, Hidetoshi and Maeda, Haruyoshi},
eprint = {1509.02870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shimodaira, Maeda - 2015 - An information criterion for model selection with missing data via complete-data divergence.pdf:pdf},
keywords = {akaike information criterion,alternating projections,and phrases,data,divergence,em algorithm,fisher information matrix,incomplete data,kullback-leibler,manifold,misspecification,takeuchi information criterion},
title = {{An information criterion for model selection with missing data via complete-data divergence}},
url = {http://arxiv.org/abs/1509.02870},
year = {2015}
}
@article{Chen2015a,
abstract = {Consider the Gaussian sequence model {\$}y \backslashsim N(\backslashtheta{\^{}}*,\backslashsigma{\^{}}2 I{\_}n){\$}, where {\$}\backslashtheta{\^{}}*{\$} is unknown but known to belong to a closed convex polyhedral set {\$}\backslashmathcal{\{}C{\}} \backslashsubset \backslashmathbb{\{}R{\}}{\^{}}n{\$}. In this paper we provide a unified characterization of the degrees of freedom for estimators of {\$}\backslashtheta{\^{}}*{\$} obtained as the (linearly or quadratically perturbed) partial projection of {\$}y{\$} onto {\$}\backslashmathcal{\{}C{\}}{\$}. As special cases of our results, we derive explicit expressions for the degrees of freedom in many shape restricted regression problems, e.g., bounded isotonic regression, multivariate convex regression and penalized convex regression. Our general theory also yields, as special cases, known results on the degrees of freedom of many well-studied estimators in the statistics literature, such as ridge regression, Lasso and generalized Lasso. Our results can be readily used to choose the tuning parameter(s) involved in the estimation procedure by minimizing the Stein's unbiased risk estimate. We illustrate this through simulation studies for bounded isotonic regression and penalized convex regression.},
archivePrefix = {arXiv},
arxivId = {1509.01877},
author = {Chen, Xi and Lin, Qihang and Sen, Bodhisattva},
eprint = {1509.01877},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Lin, Sen - 2015 - On Degrees of Freedom of Projection Estimators with Applications to Multivariate Shape Restricted Regression.pdf:pdf},
keywords = {and phrases,bounded isotonic regression,convex polyhedral,convex regression,divergence of an estimator,generalized lasso,set},
number = {1},
pages = {1--45},
title = {{On Degrees of Freedom of Projection Estimators with Applications to Multivariate Shape Restricted Regression}},
url = {http://arxiv.org/abs/1509.01877},
year = {2015}
}
@article{Dwork2015,
author = {Dwork, C. and Feldman, V. and Hardt, M. and Pitassi, T. and Reingold, O. and Roth, a.},
doi = {10.1126/science.aaa9375},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dwork et al. - 2015 - The reusable holdout Preserving validity in adaptive data analysis.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6248},
pages = {636--638},
title = {{The reusable holdout: Preserving validity in adaptive data analysis}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa9375},
volume = {349},
year = {2015}
}
@article{Healy1956,
author = {Healy, Michael and Westmacott, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Healy, Westmacott - 1956 - Missing Values in Experiments Analysed on Automatic Computers.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {3},
pages = {203--206},
title = {{Missing Values in Experiments Analysed on Automatic Computers}},
volume = {5},
year = {1956}
}
@article{Little1992,
abstract = {The literature of regression analysis with missing values of the independent variables is reviewed. Six classes of procedures are distinguished: complete case analysis, available case methods, least squares on imputed data, maximum likelihood, Bayesian methods, and multiple imputation. Methods are compared and illustrated when missing data are confined to one independent variable, and extensions to more general patterns are indicated. Attention is paid to the performance of methods when the missing data are not missing completely at random. Least squares methods that fill in missing X's using only data on the X's are contrasted with likelihood-based methods that use data on the X's and Y. The latter approach is preferred and provides methods for elaboration of the basic normal linear regression model. It is suggested that more widely distributed software is needed that advances beyond complete-case analysis, available-case analysis, and naive imputation methods. Bayesian simulation methods and multiple imputation are reviewed; these provide fruitful avenues for future research.},
author = {Little, Roderick J. a.},
doi = {10.2307/2290664},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little - 1992 - Regression with missing x's A review.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian inference,imputation,incomplete data,multiple imputation},
number = {420},
pages = {1227--1237},
pmid = {318},
title = {{Regression with missing x's: A review}},
url = {http://www.jstor.org.libproxy1.nus.edu.sg/stable/2290664},
volume = {87},
year = {1992}
}
@book{Little2002,
author = {Little, Roderick J. A. and Rubin, Donald B.},
booktitle = {Wiley, New York.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Little, Rubin - 2002 - Statistical Analysis with Missing Data.pdf:pdf},
isbn = {3175723993},
title = {{Statistical Analysis with Missing Data}},
year = {2002}
}
@article{Li2015b,
abstract = {The scarcity of data annotated at the desired level of granularity is a recurring issue in many applications. Significant amounts of effort have been devoted to developing weakly supervised methods tailored to each individual setting, which are often carefully designed to take advantage of the particular properties of weak supervision regimes, form of available data and prior knowledge of the task at hand. Unfortunately, it is difficult to adapt these methods to new tasks and/or forms of data, which often require different weak supervision regimes or models. We present a general-purpose method that can solve any weakly supervised learning problem irrespective of the weak supervision regime or the model. The proposed method turns any off-the-shelf strongly supervised classifier into a weakly supervised classifier and allows the user to specify any arbitrary weakly supervision regime via a loss function. We apply the method to several different weak supervision regimes and demonstrate competitive results compared to methods specifically engineered for those settings.},
archivePrefix = {arXiv},
arxivId = {1509.06807},
author = {Li, Ke and Malik, Jitendra},
eprint = {1509.06807},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Malik - 2015 - Bandit Label Inference for Weakly Supervised Learning.pdf:pdf},
pages = {1--10},
title = {{Bandit Label Inference for Weakly Supervised Learning}},
url = {http://arxiv.org/abs/1509.06807},
year = {2015}
}
@article{Bouckaert2004,
abstract = {Empirical research in learning algorithms for classification tasks gen- erally requires the use of significance tests.The quality of a test is typically judged on Type I error (how often the test indicates a difference when it should not) and Type II error (how often it indicates no difference when it should). In this paper we argue that the replicability of a test is also of importance.We say that a test has low replicability if its outcome strongly depends on the particular random parti- tioning of the data that is used to perform it. We present empirical measures of replicability and use them to compare the performance of several popular tests in a realistic setting involving standard learning algorithms and benchmark datasets. Based on our results we give recommendations on which test to use.},
author = {Bouckaert, Remco R and Frank, Eibe},
doi = {10.1007/978-3-540-24775-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bouckaert, Frank - 2004 - Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms.pdf:pdf},
isbn = {3-540-22064-X},
issn = {0302-9743},
journal = {Advances in knowledge discovery and data mining},
pages = {3--12},
title = {{Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms}},
year = {2004}
}
@article{Skurichina1999,
author = {Skurichina, Marina and Duin, Robert P. W.},
doi = {10.1007/s100440050013},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 1999 - Regularisation of Linear Classifiers by Adding Redundant Features.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis {\&} Applications},
keywords = {critical sample size,generalisation error,noise injection,peaking behaviour,pseudo fisher linear discriminant,regularisation},
number = {1},
pages = {44--52},
title = {{Regularisation of Linear Classifiers by Adding Redundant Features}},
volume = {2},
year = {1999}
}
@article{Balcan2005,
abstract = {Semi Supervised Learning;},
author = {Balcan, M.F. and Blum, a. and Choi, P.P. and Lafferty, J. and Pantano, B. and Rwebangira, M.R. and Zhu, X.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2005 - Person identification in webcam images An application of semi-supervised learning.pdf:pdf},
journal = {ICML 2005 Workshop on Learning with Partially Classified Training Data},
pages = {6},
title = {{Person identification in webcam images: An application of semi-supervised learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.1706{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2005}
}
@article{Loog2016,
abstract = {Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets.},
archivePrefix = {arXiv},
arxivId = {1503.00269},
author = {Loog, Marco},
eprint = {1503.00269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2016 - Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {contrast,ear discriminant analysis,lin-,maximum likelihood,pessimism,semi-supervised learning},
number = {3},
pages = {462--475},
title = {{Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification}},
volume = {38},
year = {2016}
}
@article{Madeira2004a,
abstract = {A large number of clustering approaches have been proposed for the analysis of gene expression data obtained from microarray experiments. However, the results from the application of standard clustering methods to genes are limited. This limitation is imposed by the existence of a number of experimental conditions where the activity of genes is uncorrelated. A similar limitation exists when clustering of conditions is performed. For this reason, a number of algorithms that perform simultaneous clustering on the row and column dimensions of the data matrix has been proposed. The goal is to find submatrices, that is, subgroups of genes and subgroups of conditions, where the genes exhibit highly correlated activities for every condition. In this paper, we refer to this class of algorithms as biclustering. Biclustering is also referred in the literature as coclustering and direct clustering, among others names, and has also been used in fields such as information retrieval and data mining. In this comprehensive survey, we analyze a large number of existing approaches to biclustering, and classify them in accordance with the type of biclusters they can find, the patterns of biclusters that are discovered, the methods used to perform the search, the approaches used to evaluate the solution, and the target applications.},
author = {Madeira, Sara C and Oliveira, Arlindo L},
doi = {10.1109/TCBB.2004.2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Madeira, Oliveira - 2004 - Biclustering algorithms for biological data analysis a survey.pdf:pdf},
isbn = {1545-5963 (Print) 1545-5963 (Linking)},
issn = {1545-5963},
journal = {Transactions on computational biology and bioinformatics},
number = {1},
pages = {24--45},
pmid = {17048406},
title = {{Biclustering algorithms for biological data analysis: a survey.}},
volume = {1},
year = {2004}
}
@article{Zhu2014,
abstract = {In many situations we have some measurement of confidence on " positiveness " for a binary label. The " positiveness " is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called expectation loss SVM (e-SVM) that is devoted to the problems where only the " positiveness " instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further vali-date this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.},
author = {Zhu, Jun and Mao, Junhua and Yuille, Alan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Mao, Yuille - 2014 - Learning From Weakly Supervised Data by The Expectation Loss SVM ( e-SVM ) Algorithm.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Learning From Weakly Supervised Data by The Expectation Loss SVM ( e-SVM ) Algorithm}},
year = {2014}
}
@article{Donoho2015,
author = {Donoho, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Donoho - 2015 - 50 years of data collection UNHCR expce.pdf:pdf},
pages = {1--41},
title = {{50 years of data collection UNHCR expce}},
year = {2015}
}
@article{Ben-David2015a,
abstract = {It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such "clusterable" instances. This paper addresses the thesis that the computational hardness of clustering tasks goes away for inputs that one really cares about. In other words, that "Clustering is difficult only when it does not matter" (the $\backslash$emph{\{}CDNM thesis{\}} for short). I wish to present a a critical bird's eye overview of the results published on this issue so far and to call attention to the gap between available and desirable results on this issue. A longer, more detailed version of this note is available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal support to the the CDNM thesis and then examine existing results in view of these requirements and list some significant unsolved research challenges in that direction.},
archivePrefix = {arXiv},
arxivId = {1510.05336},
author = {Ben-David, Shai},
eprint = {1510.05336},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David - 2015 - Clustering is Easy When ....What.pdf:pdf},
pages = {2--7},
title = {{Clustering is Easy When ....What?}},
url = {http://arxiv.org/abs/1510.05336},
year = {2015}
}
@article{Liu,
author = {Liu, Anqi and Ziebart, Brian D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Ziebart - Unknown - Robust Classification Under Sample Selection Bias.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Ziebart - Unknown - Robust Classification Under Sample Selection Bias(2).pdf:pdf},
pages = {1--9},
title = {{Robust Classification Under Sample Selection Bias}}
}
@article{Pribram1978a,
author = {Afifi, A.A. and Elashoff, R.M.},
doi = {10.1017/S0140525X00060003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Afifi, Elashoff - 1967 - Missing Observations in Multivariate Statistics II Point Estimation in Simple Linear Regression.pdf:pdf},
isbn = {0300104251},
issn = {0140-525X},
journal = {Journal of the American Statistical Association},
number = {317},
pages = {10--29},
title = {{Missing Observations in Multivariate Statistics II: Point Estimation in Simple Linear Regression}},
volume = {62},
year = {1967}
}
@article{Chandrasekaran2013,
abstract = {Modern massive datasets create a fundamental problem at the intersection of the computational and statistical sciences: how to provide guarantees on the quality of statistical inference given bounds on computational resources, such as time or space. Our approach to this problem is to define a notion of "algorithmic weakening," in which a hierarchy of algorithms is ordered by both computational efficiency and statistical efficiency, allowing the growing strength of the data at scale to be traded off against the need for sophisticated processing. We illustrate this approach in the setting of denoising problems, using convex relaxation as the core inferential tool. Hierarchies of convex relaxations have been widely used in theoretical computer science to yield tractable approximation algorithms to many computationally intractable tasks. In the current paper, we show how to endow such hierarchies with a statistical characterization and thereby obtain concrete tradeoffs relating algorithmic runtime to amount of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.1073v2},
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
doi = {10.1073/pnas.1302293110},
eprint = {arXiv:1211.1073v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {convex geometry,convex relaxation,high-dimensional statistics,massive datasets},
month = {mar},
number = {13},
pages = {E1181--90},
pmid = {23479655},
title = {{Computational and statistical tradeoffs via convex relaxation.}},
volume = {110},
year = {2013}
}
@inproceedings{Krijthe2015,
address = {Saint {\'{E}}tienne. France},
author = {Krijthe, Jesse H. and Loog, Marco},
booktitle = {14th International Symposium on Advances in Intelligent Data Analysis XIV (Lecture Notes in Computer Science Volume 9385)},
editor = {Fromont, Elisa and Bie, Tijl De and van Leeuwen, Matthijs},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Krijthe, Loog - 2015 - Implicitly Constrained Semi-Supervised Least Squares Classification.pdf:pdf},
keywords = {constrained,least squares classification,semi-supervised learning},
pages = {158--169},
title = {{Implicitly Constrained Semi-Supervised Least Squares Classification}},
year = {2015}
}
@article{Reid2011,
abstract = {We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.},
archivePrefix = {arXiv},
arxivId = {0901.0356},
author = {Reid, Mark D. and Williamson, Robert C.},
eprint = {0901.0356},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {731--817},
title = {{Information, divergence and risk for binary experiments}},
url = {http://arxiv.org/abs/0901.0356 http://dl.acm.org/citation.cfm?id=2021029},
volume = {12},
year = {2011}
}
@article{VanErven2015,
abstract = {The pursuit of fast rates in online and statistical learning has led to the conception of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that, under surprisingly weak conditions, both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov-Mammen margin condition, which has played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov, and Vovk's notion of mixability. Our unifying conditions thus provide a significant step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
archivePrefix = {arXiv},
arxivId = {1507.02592},
author = {van Erven, Tim and Gr{\"{u}}nwald, Peter D. and Mehta, Nishant a. and Reid, Mark D. and Williamson, Robert C.},
eprint = {1507.02592},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Erven et al. - 2015 - Fast rates in statistical and online learning.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/van Erven et al. - 2015 - Fast rates in statistical and online learning(2).pdf:pdf},
pages = {1793--1861},
title = {{Fast rates in statistical and online learning}},
url = {http://arxiv.org/abs/1507.02592},
volume = {2014},
year = {2015}
}
@article{Efron,
abstract = {In the absence of relevant prior experience, popular Bayesian estimation techniques usually begin with some form of 'uninformative' prior distribution intended to have minimal infer-ential influence. The Bayes rule will still produce nice looking estimates and credible intervals, but these lack the logical force that is attached to experience-based priors and require further justification. The paper concerns the frequentist assessment of Bayes estimates. A simple for-mula is shown to give the frequentist standard deviation of a Bayesian point estimate. The same simulations as required for the point estimate also produce the standard deviation. Exponen-tial family models make the calculations particularly simple and bring in a connection to the parametric bootstrap.},
author = {Efron, Bradley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Efron - 2015 - Frequentist Accuracy of Bayesian Estimates.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Efron - 2015 - Frequentist Accuracy of Bayesian Estimates.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {Approximate bootstrap confidence intervals,General accuracy formula,Hierarchical and empirical Bayes,Markov chain Monte Carlo methods,Parametric bootstrap},
number = {3},
pages = {617--646},
title = {{Frequentist Accuracy of Bayesian Estimates}},
volume = {77},
year = {2015}
}
@article{Hong2015a,
author = {Hong, Yi and Zhu, Weiping},
doi = {10.1016/j.patrec.2015.06.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Zhu - 2015 - Spatial Co-Training for Semi-Supervised Image Classification(2).pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Co-training,Image classif,Image classification,Semi-supervised learning},
pages = {59--65},
publisher = {Elsevier Ltd.},
title = {{Spatial Co-Training for Semi-Supervised Image Classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515001816},
volume = {63},
year = {2015}
}
@article{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
eprint = {1506.03431},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:pdf},
pages = {1--22},
title = {{Automatic Variational Inference in Stan}},
url = {http://arxiv.org/abs/1506.03431},
year = {2015}
}
@article{Culp2008,
author = {Culp, Mark and Michailidis, George},
doi = {10.1198/106186008X344748},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting(2).pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
month = {sep},
number = {3},
pages = {545--571},
title = {{An iterative algorithm for extending learners to a semi-supervised setting}},
volume = {17},
year = {2008}
}
@article{Kawakita2014a,
abstract = {We are interested in developing a safe semi-supervised learning that works in any situation. Semi-supervised learning postulates that n(') unlabeled data are available in addition to n labeled data. However, almost all of the previous semi-supervised methods require additional assumptions (not only unlabeled data) to make improvements on supervised learning. If such assumptions are not met, then the methods possibly perform worse than supervised learning. Sokolovska, Capp{\'{e}}, and Yvon (2008) proposed a semi-supervised method based on a weighted likelihood approach. They proved that this method asymptotically never performs worse than supervised learning (i.e., it is safe) without any assumption. Their method is attractive because it is easy to implement and is potentially general. Moreover, it is deeply related to a certain statistical paradox. However, the method of Sokolovska et al. (2008) assumes a very limited situation, i.e., classification, discrete covariates, n(')ââ and a maximum likelihood estimator. In this paper, we extend their method by modifying the weight. We prove that our proposal is safe in a significantly wide range of situations as long as nâ¤n('). Further, we give a geometrical interpretation of the proof of safety through the relationship with the above-mentioned statistical paradox. Finally, we show that the above proposal is asymptotically safe even when n('){\textless}n by modifying the weight. Numerical experiments illustrate the performance of these methods.},
author = {Kawakita, Masanori and Takeuchi, Jun'ichi Jun'ichi},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood(2).pdf:pdf},
isbn = {8192802361},
issn = {1879-2782},
journal = {Neural Networks},
keywords = {semi-supervised learning},
month = {may},
pages = {146--64},
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24632000 http://linkinghub.elsevier.com/retrieve/pii/S0893608014000288},
volume = {53},
year = {2014}
}
@article{Leday2015,
abstract = {Reconstructing a gene network from high-throughput molecular data is often a challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighbourhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with non-sparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyper-parameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible.},
archivePrefix = {arXiv},
arxivId = {1510.03771},
author = {Leday, Gwena{\"{e}}l G. R. and de Gunst, Mathisca C. M. and Kpogbezan, Gino B. and {Van der Vaart}, Aad W. and {Van Wieringen}, Wessel N. and {Van de Wiel}, Mark A.},
eprint = {1510.03771},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leday et al. - 2015 - Gene network reconstruction using global-local shrinkage priors.pdf:pdf},
title = {{Gene network reconstruction using global-local shrinkage priors}},
url = {http://arxiv.org/abs/1510.03771},
year = {2015}
}
@article{Balsubramani2015,
abstract = {We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying "link functions" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning - all this structure follows from a minimax interpretation of loss minimization.},
archivePrefix = {arXiv},
arxivId = {1510.00452},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {1510.00452},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - 2015 - Minimax Binary Classifier Aggregation with General Losses.pdf:pdf},
pages = {1--13},
title = {{Minimax Binary Classifier Aggregation with General Losses}},
url = {http://arxiv.org/abs/1510.00452},
year = {2015}
}
@article{Maroufy2015,
abstract = {Despite the flexibility and popularity of mixture models, their associated parameter spaces are often difficult to represent due to fundamental identification problems. This paper looks at a novel way of representing such a space for general mixtures of exponential families, where the parameters are identifiable, interpretable, and, due to a tractable geometric structure, the space allows fast computational algorithms to be constructed.},
archivePrefix = {arXiv},
arxivId = {1510.04514},
author = {Maroufy, Vahed and Marriott, Paul},
eprint = {1510.04514},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maroufy, Marriott - 2015 - Mixture Models Building a Parameter Space.pdf:pdf},
keywords = {exponential family,finite mixture models,identifiability,local mixture models,model selection,number of components},
pages = {1--9},
title = {{Mixture Models: Building a Parameter Space}},
url = {http://arxiv.org/abs/1510.04514},
year = {2015}
}
@article{Johnson2015,
abstract = {We demonstrate the usefulness of submodularity in statistics. Greedy algorithms such as forward stepwise regression and the lasso perform well in situations that can be characterized by submodularity. In particular, submodularity of the coefficient of determination, R{\$}{\^{}}2{\$}, provides a natural way to analyze the effects of collinearity on model selection. In model selection, we encounter the search problem of identifying a subset of k covariates with predictive loss close to that of the best model of k covariates. Submodularity arises naturally in this setting due to its deep roots within combinatorial optimization. It provides structural results for discrete convexity as well as guarantees for the success of greedy algorithms. In statistics, submodularity isolates cases in which collinearity makes the choice of model features difficult from those in which this task is routine. Submodularity of R{\$}{\^{}}2{\$} is closely related to other statistical assumptions used in variable screening and proving the performance of the Lasso. This has important implications for the use of model selection procedures: the situations in which forward stepwise and Lasso are successful are closely related.},
archivePrefix = {arXiv},
arxivId = {1510.06301},
author = {Johnson, Kory D. and Stine, Robert A. and Foster, Dean P.},
eprint = {1510.06301},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson, Stine, Foster - 2015 - Submodularity in Statistics Comparing the Success of Model Selection Methods.pdf:pdf},
number = {1},
pages = {1--12},
title = {{Submodularity in Statistics: Comparing the Success of Model Selection Methods}},
url = {http://arxiv.org/abs/1510.06301},
year = {2015}
}
@article{Steinwart2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.8437v3},
author = {Steinwart, Ingo},
doi = {10.1214/15-AOS1331},
eprint = {arXiv:1409.8437v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steinwart - 2015 - Fully adaptive density-based clustering.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {5},
pages = {2132--2167},
title = {{Fully adaptive density-based clustering}},
url = {http://projecteuclid.org/euclid.aos/1442364148},
volume = {43},
year = {2015}
}
@article{Dasgupta2015,
abstract = {The development of algorithms for hierarchical clustering has been hampered by a shortage of precise objective functions. To help address this situation, we introduce a simple cost function on hierarchies over a set of points, given pairwise similarities between those points. We show that this criterion behaves sensibly in canonical instances and that it admits a top-down construction procedure with a provably good approximation ratio.},
archivePrefix = {arXiv},
arxivId = {1510.05043},
author = {Dasgupta, Sanjoy},
eprint = {1510.05043},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta - 2015 - A cost function for similarity-based hierarchical clustering.pdf:pdf},
number = {Section 4},
pages = {1--18},
title = {{A cost function for similarity-based hierarchical clustering}},
url = {http://arxiv.org/abs/1510.05043},
year = {2015}
}
@article{Nakamoto2008,
abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
archivePrefix = {arXiv},
arxivId = {43543534534v343453},
author = {Nakamoto, Satoshi},
doi = {10.1007/s10838-008-9062-0},
eprint = {43543534534v343453},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nakamoto - 2008 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:pdf},
isbn = {978-972-757-716-3},
issn = {09254560},
journal = {Consulted},
pages = {1--9},
pmid = {14533183},
title = {{Bitcoin: A Peer-to-Peer Electronic Cash System}},
url = {http://s.kwma.kr/pdf/Bitcoin/bitcoin.pdf},
year = {2008}
}
@article{Kenward1997,
abstract = {Restricted maximum likelihood (REML) is now well established as a method for estimating the parameters of the general Gaussian linear model with a structured covariance matrix, in particular for mixed linear models. Conventionally, estimates of precision and inference for fixed effects are based on their asymptotic distribution, which is known to be inadequate for some small-sample problems. In this paper, we present a scaled Wald statistic, together with an F approximation to its sampling distribution, that is shown to perform well in a range of small sample settings. The statistic uses an adjusted estimator of the covariance matrix that has reduced small sample bias. This approach has the advantage that it reproduces both the statistics and F distributions in those settings where the latter is exact, namely for Hotelling T2 type statistics and for analysis of variance F-ratios. The performance of the modified statistics is assessed through simulation studies of four different REML analyses and the methods are illustrated using three examples.},
author = {Kenward, M G and Roger, J H},
doi = {10.2307/2533558},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kenward, Roger - 1997 - Small sample inference for fixed effects from restricted maximum likelihood.pdf:pdf},
isbn = {0006-341X},
issn = {0006-341X},
journal = {Biometrics},
keywords = {alpha design,ante-dependence,crossover trial,likelihood,mixed models,residual maximum,small sample approximation},
number = {3},
pages = {983--997},
pmid = {9333350},
title = {{Small sample inference for fixed effects from restricted maximum likelihood.}},
volume = {53},
year = {1997}
}
@article{Franks2014,
abstract = {Although basketball is a dualistic sport, with all players competing on both offense and defense, almost all of the sport's conventional metrics are designed to summarize offensive play. As a result, player valuations are largely based on offensive performances and to a much lesser degree on defensive ones. Steals, blocks, and defensive rebounds provide only a limited summary of defensive effectiveness, yet they persist because they summarize salient events that are easy to observe. Due to the inefficacy of traditional defensive statistics, the state of the art in defensive analytics remains qualitative, based on expert intuition and analysis that can be prone to human biases and imprecision. Fortunately, emerging optical player tracking systems have the potential to enable a richer quantitative characterization of basketball performance, particularly defensive performance. Unfortunately, due to computational and methodological complexities, that potential remains unmet. This paper attempts to fill this void, combining spatial and spatio-temporal processes, matrix factorization techniques, and hierarchical regression models with player tracking data to advance the state of defensive analytics in the NBA. Our approach detects, characterizes, and quantifies multiple aspects of defensive play in basketball, supporting some common understandings of defensive effectiveness, challenging others, and opening up many new insights into the defensive elements of basketball.},
archivePrefix = {arXiv},
arxivId = {1405.0231},
author = {Franks, Alexander and Miller, Andrew and Bornn, Luke and Goldsberry, Kirk},
doi = {10.1214/14-AOAS799},
eprint = {1405.0231},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Franks et al. - 2014 - Characterizing the Spatial Structure of Defensive Skill in Professional Basketball.pdf:pdf},
issn = {19417330},
keywords = {Basketball, hidden Markov models, nonnegative matr,and phrases,basketball,bayesian hierarchical models,hidden markov models,nonnegative matrix fac-,torization},
number = {1},
pages = {0--31},
title = {{Characterizing the Spatial Structure of Defensive Skill in Professional Basketball}},
url = {http://arxiv.org/abs/1405.0231v1$\backslash$nhttp://arxiv.org/abs/1405.0231},
volume = {9},
year = {2014}
}
@article{Ryabko2015a,
archivePrefix = {arXiv},
arxivId = {1509.07776},
author = {Ryabko, Daniil and Ryabko, Boris},
eprint = {1509.07776},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ryabko, Ryabko - 2015 - Predicting the outcomes of every process for which an asymptotically accurate stationary predictor exists is imp.pdf:pdf},
isbn = {9781467377041},
pages = {1204--1206},
title = {{Predicting the outcomes of every process for which an asymptotically accurate stationary predictor exists is impossible}},
year = {2015}
}
@article{VanderMaaten2008,
author = {van der Maaten, L.J.P. and Hinton, G.E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Maaten, Hinton - 2008 - Visualizing High-Dimensional Data Using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
volume = {9},
year = {2008}
}
@article{Jung2008,
abstract = {In recent years, there has been a growing interest among researchers in the use of latent class and growth mixture modeling techniques for applications in the social and psychological sciences, in part due to advances in and availability of computer software designed for this purpose (e.g., Mplus and SAS Proc Traj). Latent growth modeling approaches, such as latent class growth analysis (LCGA) and growth mixture modeling (GMM), have been increasingly recognized for their usefulness for identifying homogeneous subpopulations within the larger heterogeneous population and for the identification of meaningful groups or classes of individuals. The purpose of this paper is to provide an overview of LCGA and GMM, compare the different techniques of latent growth modeling, discuss current debates and issues, and provide readers with a practical guide for conducting LCGA and GMM using the Mplus software.},
author = {Jung, Tony and Wickrama, K. A.},
doi = {10.1111/j.1751-9004.2007.00054.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jung, Wickrama - 2008 - An introduction to latent class growth analysis and growth mixture modeling.pdf:pdf},
isbn = {1751-9004},
issn = {1751-9004},
journal = {Social and Personality Psychology Compass},
number = {1},
pages = {302--317},
title = {{An introduction to latent class growth analysis and growth mixture modeling}},
url = {http://doi.wiley.com/10.1111/j.1751-9004.2007.00054.x$\backslash$nhttp://onlinelibrary.wiley.com/doi/10.1111/j.1751-9004.2007.00054.x/full},
volume = {2},
year = {2008}
}
@article{Nyamundanda2013,
abstract = {In a longitudinal metabolomics study, multiple metabolites are measured from several observations at many time points. Interest lies in reducing the dimensionality of such data and in highlighting influential metabolites which change over time. A dynamic probabilistic principal components analysis (DPPCA) model is proposed to achieve dimension reduction while appropriately modelling the correlation due to repeated measurements. This is achieved by assuming an autoregressive model for some of the model parameters. Linear mixed models are subsequently used to identify influential metabolites which change over time. The proposed model is used to analyse data from a longitudinal metabolomics animal study.},
archivePrefix = {arXiv},
arxivId = {1312.2393},
author = {Nyamundanda, Gift and Gormley, Isobel Claire and Brennan, Lorraine},
doi = {10.1111/rssc.12060},
eprint = {1312.2393},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nyamundanda, Gormley, Brennan - 2013 - A dynamic probabilistic principal components model for the analysis of longitudinal metabolomic d.pdf:pdf},
issn = {00359254},
journal = {arXiv preprint arXiv:1312.2393},
keywords = {auto-regressive model,linear mixed model,longitudinal metabolomic data,metabolomics,principal components analysis,probabilistic principal components analysis},
pages = {26},
title = {{A dynamic probabilistic principal components model for the analysis of longitudinal metabolomic data}},
url = {http://arxiv.org/abs/1312.2393},
year = {2013}
}
@article{Sion1958,
author = {Sion, Maurice},
doi = {1103040253},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sion - 1958 - On general minimax theorems.pdf:pdf},
issn = {0030-8730},
journal = {Pacific J. Math},
number = {1},
pages = {171--176},
title = {{On general minimax theorems}},
volume = {8},
year = {1958}
}
@article{Behl2014,
abstract = {The performance of binary classification tasks, such as action classification and object detection, is often measured in terms of the average precision (AP). Yet it is common practice in computer vision to employ the support vector machine (SVM) classifier, which optimizes a surrogate 0-1 loss. The popularity of SVM can be attributed to its empirical performance. Specifically, in fully supervised settings, SVM tends to provide similar accuracy to the AP-SVM classifier, which directly optimizes an AP-based loss. However, we hypothesize that in the significantly more challenging and practically useful setting of weakly supervised learning, it becomes crucial to optimize the right accuracy measure. In order to test this hypothesis, we propose a novel latent AP-SVM that minimizes a carefully designed upper bound on the AP-based loss function over weakly supervised samples. Using publicly available datasets, we demonstrate the advantage of our approach over standard loss-based binary classifiers on two challenging problems: action classification and character recognition.},
author = {Behl, Aseem and Jawahar, C.V. and Kumar, M. Pawan},
doi = {10.1109/CVPR.2014.133},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Behl, Jawahar, Kumar - 2014 - Optimizing Average Precision Using Weakly Supervised Data.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
number = {12},
pages = {1011--1018},
title = {{Optimizing Average Precision Using Weakly Supervised Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909529},
volume = {37},
year = {2014}
}
@article{Jayasumana2015,
abstract = {In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.},
archivePrefix = {arXiv},
arxivId = {1412.0265},
author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
doi = {10.1109/TPAMI.2015.2414422},
eprint = {1412.0265},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jayasumana et al. - 2015 - Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {10,1109,2015,2414422,a future issue of,accepted for publication in,analysis and machine intelligence,but has not been,citation information,content may change prior,doi,fully edited,ieee transactions on pattern,s article has been,this journal,to final publication,tpami},
number = {c},
pages = {1--1},
title = {{Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7063231},
volume = {8828},
year = {2015}
}
@article{Robinson1991a,
abstract = {In animal breeding, Best Linear Unbiased Predition, or BLUP, is a technique for estimating genetic merits. In general, it is a method of estimating random effects. It can be used to derive the Kalman filter, the method of Kriging used for ore reserve estimation ... Understanding of procedures for estimating random effects should help people to understand some complicated and controversial issues about fixed and random effects models and also help to bridge the apparent gulf between the Bayesian and Classical schools of thought.},
author = {Robinson, G. K.},
doi = {10.1214/ss/1177011926},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robinson - 1991 - That BLUP is a Good Thing The Estimation of Random Effects.pdf:pdf},
isbn = {08834237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,best linear unbiased prediction},
number = {1},
pages = {15--32},
title = {{That BLUP is a Good Thing: The Estimation of Random Effects}},
volume = {6},
year = {1991}
}
@article{Wilson2015a,
abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost {\$}O(n){\$} for {\$}n{\$} training points, and predictions cost {\$}O(1){\$} per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
archivePrefix = {arXiv},
arxivId = {1511.02222},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1511.02222},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson et al. - 2015 - Deep Kernel Learning.pdf:pdf},
number = {1998},
pages = {1--19},
title = {{Deep Kernel Learning}},
url = {http://arxiv.org/abs/1511.02222},
year = {2015}
}
@article{Abadi2015,
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
year = {2015}
}
@article{Maltoni2015,
abstract = {Recent works demonstrated the usefulness of temporal coherence to regularize supervised training or to learn invariant features with deep architectures. In particular, enforcing a smooth output change while presenting temporally-closed frames from video sequences, proved to be an effective strategy. In this paper we prove the efficacy of temporal coherence for semi-supervised incremental tuning. We show that a deep architecture, just mildly trained in a supervised manner, can progressively improve its classification accuracy, if exposed to video sequences of unlabeled data. The extent to which, in some cases, a semi-supervised tuning allows to improve classification accuracy (approaching the supervised one) is somewhat surprising. A number of control experiments pointed out the fundamental role of temporal coherence.},
archivePrefix = {arXiv},
arxivId = {1511.03163},
author = {Maltoni, Davide and Lomonaco, Vincenzo},
eprint = {1511.03163},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maltoni, Lomonaco - 2015 - Semi-supervised Tuning from Temporal Coherence.pdf:pdf},
number = {2005},
pages = {1--15},
title = {{Semi-supervised Tuning from Temporal Coherence}},
url = {http://arxiv.org/abs/1511.03163},
year = {2015}
}
@article{Matuschek2015,
abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. The advantages of LMMs over ANOVAs, however, come at a cost: Setting up an LMM is not as straightforward as running an ANOVA. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr et al., 2013), presumably to keep Type I error down to the nominal {\$}\backslashalpha{\$} in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, models with a random effect structure that is supported by the data have optimal Type I error and power properties.},
archivePrefix = {arXiv},
arxivId = {1511.01864},
author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
eprint = {1511.01864},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Matuschek et al. - 2015 - Balancing Type I Error and Power in Linear Mixed Models.pdf:pdf},
pages = {1--14},
title = {{Balancing Type I Error and Power in Linear Mixed Models}},
url = {http://arxiv.org/abs/1511.01864},
year = {2015}
}
@article{Waldram1971,
archivePrefix = {arXiv},
arxivId = {1511.01844},
author = {Waldram, J.M.},
doi = {10.1177/096032717100300408},
eprint = {1511.01844},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Waldram - 1971 - A note on the evaluation of modelling.pdf:pdf},
issn = {1477-1535},
journal = {Lighting Research and Technology},
number = {4},
pages = {284--285},
title = {{A note on the evaluation of modelling}},
url = {http://lrt.sagepub.com/cgi/doi/10.1177/096032717100300408},
volume = {3},
year = {1971}
}
@article{Chatterjee2015,
abstract = {The goal of importance sampling is to estimate the expected value of a given function with respect to a probability measure {\$}\backslashnu{\$} using a random sample of size {\$}n{\$} drawn from a different probability measure {\$}\backslashmu{\$}. If the two measures {\$}\backslashmu{\$} and {\$}\backslashnu{\$} are nearly singular with respect to each other, which is often the case in practice, the sample size required for accurate estimation is large. In this article it is shown that in a fairly general setting, a sample of size approximately {\$}\backslashexp(D(\backslashnu||\backslashmu)){\$} is necessary and sufficient for accurate estimation by importance sampling, where {\$}D(\backslashnu||\backslashmu){\$} is the Kullback--Leibler divergence of {\$}\backslashmu{\$} from {\$}\backslashnu{\$}. In particular, the required sample size exhibits a kind of cut-off in the logarithmic scale. The theory is applied to obtain a fairly general formula for the sample size required in importance sampling for exponential families (Gibbs measures). We also show that the standard variance-based diagnostic for convergence of importance sampling is fundamentally problematic. An alternative diagnostic that provably works in certain situations is suggested.},
archivePrefix = {arXiv},
arxivId = {1511.01437},
author = {Chatterjee, Sourav and Diaconis, Persi},
eprint = {1511.01437},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Diaconis - 2015 - The sample size required in importance sampling.pdf:pdf},
keywords = {and phrases,gibbs measure,importance sampling,monte carlo methods,phase transition},
pages = {1--31},
title = {{The sample size required in importance sampling}},
url = {http://arxiv.org/abs/1511.01437},
year = {2015}
}
@article{Fisher2015,
abstract = {Hierarchical Bayesian models can be especially useful in precision medicine settings, where clinicians are interested in estimating the patient-level latent variables associated with an individual's current health state and its trajectory. Such models are often fit using batch Markov Chain Monte Carlo (MCMC). However, the slow speed of batch MCMC computation makes it difficult to implement in clinical settings, where immediate latent variable estimates are often desired in response to new patient data. In this report, we discuss how importance sampling (IS) can instead be used to obtain fast, in-clinic estimates of patient-level latent variables. We apply IS to the hierarchical model proposed in Coley et al (2015) for predicting an individual's underlying prostate cancer state. We find that latent variable estimates via IS can typically be obtained in 1-10 seconds per person and have high agreement with estimates coming from longer-running batch MCMC methods. Alternative options for out-of-sample fitting and online updating are also discussed.},
archivePrefix = {arXiv},
arxivId = {1510.08802},
author = {Fisher, Aaron J and Coley, R Yates and Zeger, Scott L},
eprint = {1510.08802},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fisher, Coley, Zeger - 2015 - Fast Out-of-Sample Predictions for Bayesian Hierarchical Models of Latent Health States.pdf:pdf},
pages = {1--9},
title = {{Fast Out-of-Sample Predictions for Bayesian Hierarchical Models of Latent Health States}},
url = {http://arxiv.org/abs/1510.08802},
year = {2015}
}
@article{Wilson2015,
abstract = {Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive, for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.},
archivePrefix = {arXiv},
arxivId = {1510.07389},
author = {Wilson, Andrew Gordon and Dann, Christoph and Lucas, Christopher G. and Xing, Eric P.},
eprint = {1510.07389},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson et al. - 2015 - The Human Kernel.pdf:pdf},
pages = {11},
title = {{The Human Kernel}},
url = {http://arxiv.org/abs/1510.07389},
year = {2015}
}
@article{Afendras2015,
abstract = {An important question in constructing Cross Validation (CV) estimators of the generalization error is whether rules can be established that allow "optimal" selection of the size of the training set, for fixed sample size {\$}n{\$}. We define the {\{}$\backslash$it resampling effectiveness{\}} of random CV estimators of the generalization error as the ratio of the limiting value of the variance of the CV estimator over the estimated from the data variance. The variance and the covariance of different average test set errors are independent of their indices, thus, the resampling effectiveness depends on the correlation and the number of repetitions used in the random CV estimator. We discuss statistical rules to define optimality and obtain the "optimal" training sample size as the solution of an appropriately formulated optimization problem. We show that in a broad class of loss functions the optimal training size equals half of the total sample size, independently of the data distribution. We optimally select the number of folds in {\$}k{\$}-fold cross validation and offer a computational procedure for obtaining the optimal splitting in the case of classification (via logistic regression). We substantiate our claims both, theoretically and empirically.},
archivePrefix = {arXiv},
arxivId = {1511.02980},
author = {Afendras, Georgios and Markatou, Marianthi},
eprint = {1511.02980},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Afendras, Markatou - 2015 - Optimality of TrainingTest Size and Resampling Effectiveness of Cross-Validation Estimators of the Generaliz.pdf:pdf},
keywords = {alo,author,both authors acknowledge support,cross validation estimator,in,of biostatistics,optimality,package to the second,provided by the department,resampling e ff ectiveness,the form of start-up,training sample size,university at bu ff},
number = {716},
title = {{Optimality of Training/Test Size and Resampling Effectiveness of Cross-Validation Estimators of the Generalization Error}},
url = {http://arxiv.org/abs/1511.02980},
volume = {1},
year = {2015}
}
@article{Wang2015b,
abstract = {How should one statistically analyze privacy-enhanced data? In theory, one could process it exactly as if it were normal data since many differentially private algorithms asymptotically converge exponentially fast to their non-private counterparts and/or have error that asymptotically decreases as fast as sampling error. In practice, convergence often requires enormous amounts of data. Thus making differential privacy practical requires the development of techniques that specifically account for the noise that is added for the sake of providing privacy guarantees. Such techniques are especially needed for statistical hypothesis testing. Previous approaches either ignored the added noise (resulting in highly biased {\$}p{\$}-values), accounted for the noise but had high variance, or accounted for the noise while having small variance but were restricted to very specific types of data sets. In this paper, we propose statistical tests of independence that address all three problems simultaneously -- they add small amounts of noise, account for this noise to produce accurate {\$}p{\$}-values, and have no restrictions on the types of tables to which they are applicable. Along with these tests, we propose an alternative methodology for computing the asymptotic distributions of test statistics that results in better finite-sample approximations when using differential privacy to protect data.},
archivePrefix = {arXiv},
arxivId = {1511.03376},
author = {Wang, Yue and Lee, Jaewoo and Kifer, Daniel},
eprint = {1511.03376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Lee, Kifer - 2015 - Differentially Private Hypothesis Testing, Revisited.pdf:pdf},
title = {{Differentially Private Hypothesis Testing, Revisited}},
url = {http://arxiv.org/abs/1511.03376},
year = {2015}
}
@article{Lopez-Paz2015,
abstract = {We describe generalized distillation, a framework to learn from multiple representations in a semisupervised fashion. We show that distillation (Hinton et al., 2015) and privileged information (Vapnik {\&} Izmailov, 2015) are particular instances of generalized distillation, give insight about why and when generalized distillation works, and provide numerical simulations to assess its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1511.03643},
author = {Lopez-Paz, David and Bottou, L{\'{e}}on and Sch{\"{o}}lkopf, Bernhard and Vapnik, Vladimir},
eprint = {1511.03643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-Paz et al. - 2015 - Unifying distillation and privileged information.pdf:pdf},
number = {1},
pages = {1--9},
title = {{Unifying distillation and privileged information}},
url = {http://arxiv.org/abs/1511.03643},
year = {2015}
}
@article{Hernandez-Lobato2015,
abstract = {We present black-box alpha (BB-{\$}\backslashalpha{\$}), an approximate inference method based on the minimization of {\$}\backslashalpha{\$}-divergences between probability distributions. BB-{\$}\backslashalpha{\$} scales to large datasets since it can be implemented using stochastic gradient descent. BB-{\$}\backslashalpha{\$} can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By tuning the parameter {\$}\backslashalpha{\$}, we are able to interpolate between variational Bayes and an expectation propagation like algorithm. Experiments on probit and neural network regression problems illustrate the accuracy of the posterior approximations obtained with BB-{\$}\backslashalpha{\$}.},
archivePrefix = {arXiv},
arxivId = {1511.03243},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Li, Yingzhen and Hern{\'{a}}ndez-Lobato, Daniel and Bui, Thang and Turner, Richard E.},
eprint = {1511.03243},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Lobato et al. - 2015 - Black-box {\$}alpha{\$}-divergence Minimization.pdf:pdf},
pages = {1--5},
title = {{Black-box {\$}\backslashalpha{\$}-divergence Minimization}},
url = {http://arxiv.org/abs/1511.03243},
year = {2015}
}
@article{Krause2014,
abstract = {Submodularity1 is a property of set functions with deep theoretical consequences and farâ reaching applications. At first glance it appears very similar to concavity, in other ways it resembles convexity. It appears in a wide variety of applications: in Computer Science it ... $\backslash$n},
author = {Krause, Andreas and Golovin, Daniel},
doi = {10.1017/CBO9781139177801.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krause, Golovin - 2014 - Submodular function maximization.pdf:pdf},
isbn = {9781139177801},
issn = {{\textless}null{\textgreater}},
journal = {Tractability: Practical Approaches to Hard Problems},
pages = {71--104},
title = {{Submodular function maximization}},
volume = {3},
year = {2014}
}
@article{Kontschieder,
author = {Kontschieder, Peter and Fiterau, Madalina and Criminisi, Antonio and Bul, Samuel Rota and Kessler, Fondazione Bruno},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kontschieder et al. - Unknown - Deep neural decision forests.pdf:pdf},
title = {{Deep neural decision forests }}
}
@article{Leisch2008,
author = {Leisch, Friedrich},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leisch - 2008 - FlexMix Version 2 Finite Mixtures with Concomitant Variables and Varying and Constant Parameters.pdf:pdf},
keywords = {concomitant variables,finite mixture models,generalized linear models,r},
number = {4},
title = {{FlexMix Version 2 : Finite Mixtures with Concomitant Variables and Varying and Constant Parameters}},
volume = {28},
year = {2008}
}
@article{Springenberg2015,
abstract = {In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
archivePrefix = {arXiv},
arxivId = {1511.06390},
author = {Springenberg, Jost Tobias},
eprint = {1511.06390},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Springenberg - 2015 - Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.pdf:pdf},
number = {2009},
pages = {1--20},
title = {{Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06390},
year = {2015}
}
@article{Gardner2015,
abstract = {Machine learning is increasingly used in high impact applications such as prediction of hospital re-admission, cancer screening or bio-medical research applications. As predictions become increasingly accurate, practitioners may be interested in identifying actionable changes to inputs in order to alter their class membership. For example, a doctor might want to know what changes to a patient's status would predict him/her to not be re-admitted to the hospital soon. Szegedy et al. (2013b) demonstrated that identifying such changes can be very hard in image classification tasks. In fact, tiny, imperceptible changes can result in completely different predictions without any change to the true class label of the input. In this paper we ask the question if we can make small but meaningful changes in order to truly alter the class membership of images from a source class to a target class. To this end we propose deep manifold traversal, a method that learns the manifold of natural images and provides an effective mechanism to move images from one area (dominated by the source class) to another (dominated by the target class).The resulting algorithm is surprisingly effective and versatile. It allows unrestricted movements along the image manifold and only requires few images from source and target to identify meaningful changes. We demonstrate that the exact same procedure can be used to change an individual's appearance of age, facial expressions or even recolor black and white images.},
archivePrefix = {arXiv},
arxivId = {1511.06421},
author = {Gardner, Jacob R. and Kusner, Matt J. and Li, Yixuan and Upchurch, Paul and Weinberger, Kilian Q. and Hopcroft, John E.},
eprint = {1511.06421},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gardner et al. - 2015 - Deep Manifold Traversal Changing Labels with Convolutional Features.pdf:pdf},
pages = {1--11},
title = {{Deep Manifold Traversal: Changing Labels with Convolutional Features}},
url = {http://arxiv.org/abs/1511.06421},
year = {2015}
}
@article{Reid2015,
abstract = {Applied statistical problems often come with pre-specified groupings to predictors. It is natural to test for the presence of simultaneous group-wide signal for groups in isolation, or for multiple groups together. Classical tests for the presence of such signals rely either on tests for the omission of the entire block of variables (the classical F-test) or on the creation of an unsupervised prototype for the group (either a group centroid or first principal component) and subsequent t-tests on these prototypes. In this paper, we propose test statistics that aim for power improvements over these classical approaches. In particular, we first create group prototypes, with reference to the response, hopefully improving on the unsupervised prototypes, and then testing with likelihood ratio statistics incorporating only these prototypes. We propose a (potentially) novel model, called the "prototype model", which naturally models the two-step prototype-then-test procedure. Furthermore, we introduce an inferential schema detailing the unique considerations for different combinations of prototype formation and univariate/multivariate testing models. The prototype model also suggests new applications to estimation and prediction. Prototype formation often relies on variable selection, which invalidates classical Gaussian test theory. We use recent advances in selective inference to account for selection in the prototyping step and retain test validity. Simulation experiments suggest that our testing procedure enjoys more power than do classical approaches.},
archivePrefix = {arXiv},
arxivId = {1511.07839},
author = {Reid, Stephen and Taylor, Jonathan and Tibshirani, Robert},
eprint = {1511.07839},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Taylor, Tibshirani - 2015 - A general framework for estimation and inference from clusters of features.pdf:pdf},
number = {2010},
pages = {1--27},
title = {{A general framework for estimation and inference from clusters of features}},
url = {http://arxiv.org/abs/1511.07839},
year = {2015}
}
@article{Nishimura2015a,
abstract = {Hamiltonian Monte Carlo and related algorithms have become routinely used in Bayesian computation. The utility of such approaches is highlighted in the software package STAN, which provides a platform for automatic implementation of general Bayesian models. Hence, methods for improving the efficiency of general Hamiltonian Monte Carlo algorithms can have a substantial impact on practice. We propose such a method in this article by recycling the intermediate leap-frog steps used in approximating the Hamiltonian trajectories. Current algorithms use only the final step, and wastefully discard all the intermediate steps. We propose a simple and provably accurate approach for using these intermediate samples, boosting the effective sample size with little programming effort and essentially no extra computational cost. We show that our recycled Hamiltonian Monte Carlo algorithm can lead to substantial gains in computational efficiency in a variety of experiments.},
archivePrefix = {arXiv},
arxivId = {1511.06925},
author = {Nishimura, Akihiko and Dunson, David},
eprint = {1511.06925},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nishimura, Dunson - 2015 - Recycling intermediate steps to improve Hamiltonian Monte Carlo.pdf:pdf},
number = {1},
title = {{Recycling intermediate steps to improve Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1511.06925},
year = {2015}
}
@article{Tran2015,
abstract = {Representations offered by deep generative models are fundamentally tied to their inference method from data. Variational inference methods require a rich family of approximating distributions. We construct the variational Gaussian process (VGP), a Bayesian nonparametric model which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by autoencoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.},
archivePrefix = {arXiv},
arxivId = {1511.06499},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
eprint = {1511.06499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tran, Ranganath, Blei - 2015 - Variational Gaussian Process.pdf:pdf},
pages = {1--14},
title = {{Variational Gaussian Process}},
url = {http://arxiv.org/abs/1511.06499},
year = {2015}
}
@inproceedings{Hoi2006,
abstract = {The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be ineï¬cient since the classiï¬cation model has to be retrained for every labeled example. In this paper, we present a framework for âbatch mode active learningâ that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to eï¬ciently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an eï¬cient greedy algorithm that is based on the property of submodular functions. Our empirical studies with ï¬ve UCI datasets and one realworld medical image classiï¬cation show that the proposed batch mode active learning algorithm is more eï¬ective than the state-ofthe-art algorithms for active learning},
annote = {Badly written},
author = {Hoi, Steven C H and Jin, Rong and Zhu, Jianke and Lyu, Michael R},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
doi = {10.1145/1143844.1143897},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoi et al. - 2006 - Batch mode active learning and its application to medical image classification.pdf:pdf},
isbn = {1595933832},
pages = {417--424},
title = {{Batch mode active learning and its application to medical image classification}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143897},
year = {2006}
}
@incollection{Loog2016a,
author = {Loog, M and Krijthe, J H and Jensen, A C},
booktitle = {Handbook of Pattern Recognition and Computer Vision},
chapter = {1.3},
edition = {5},
editor = {Chen, C H},
publisher = {World Scientific},
title = {{On Measuring and Quantifying Performance: Error Rates, Surrogate Loss, and an Example in SSL}},
year = {2016}
}
@inproceedings{Narasimhan,
author = {Narasimhan, Mukund and Jojic, Nebojsa and Bilmes, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Narasimhan, Jojic, Bilmes - 2005 - Q-Clustering.pdf:pdf},
pages = {979--986},
title = {{Q-Clustering}},
year = {2005}
}
@article{Oneto2015a,
author = {Oneto, Luca and Ridella, Sandro and Anguita, Davide},
doi = {10.1007/s10994-015-5540-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Oneto, Ridella, Anguita - 2015 - Tikhonov, Ivanov and Morozov regularization for support vector machine learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Ivanov regularization,Morozov regularization,Structural risk minimization,Support vector machine,Tikhonov regularization,ivanov regularization,morozov regularization,structural risk minimization,support vector machine,tikhonov regularization},
publisher = {Springer US},
title = {{Tikhonov, Ivanov and Morozov regularization for support vector machine learning}},
url = {http://link.springer.com/10.1007/s10994-015-5540-x},
year = {2015}
}
@article{Stallings2015a,
abstract = {The standard approach to finding optimal experimental designs employs conventional measures of design efficacy, such as the [IMG]f1.gif" ALT="Formula" BORDER="0"{\textgreater}, [IMG]f2.gif" ALT="Formula" BORDER="0"{\textgreater}, and [IMG]f3.gif" ALT="Formula" BORDER="0"{\textgreater}-criterion, that assume equal interest in all estimable functions of model parameters. This paper develops a general theory for weighted optimality, allowing precise design selection according to expressed relative interest in different functions in the estimation space. The approach employs a very general class of matrix-specified weighting schemes that produce easily interpretable weighted optimality criteria. In particular, for any set of estimable functions, and any selected corresponding weights, analogs of standard optimality criteria are found that guide design selection according to the weighted variances of estimators of those particular functions. The results are applied to solve the [IMG]f1.gif" ALT="Formula" BORDER="0"{\textgreater}-optimal design problem for baseline factorial effects in unblocked experiments.},
author = {Stallings, J. W. and Morgan, J. P.},
doi = {10.1093/biomet/asv037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stallings, Morgan - 2015 - General weighted optimality of designed experiments.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {September},
pages = {925--935},
title = {{General weighted optimality of designed experiments}},
url = {http://biomet.oxfordjournals.org/content/early/2015/09/28/biomet.asv037.abstract?papetoc},
volume = {102},
year = {2015}
}
@article{An2016,
abstract = {In this paper, we present a simple and effective approach to the image parsing (or labeling image regions) problem. Inspired by sparse representation techniques for super-resolution, we convert the image parsing problem into a superpixel-wise sparse representation problem with coupled dictionaries related to features and likelihoods. This algorithm works by image-level classification with global image descriptors, followed by sparse representation based likelihood estimation with local features. Finally, Markov random field (MRF) optimization is applied to incorporate neighborhood context. Experimental results on the SIFTflow dataset support the use of our approach for solving the task of image parsing. The advantage of the proposed algorithm is that it can estimate likelihoods from a small set of bases (dictionary) whereas recent nonparametric scene parsing algorithms need features and labels of whole datasets to compute likelihoods. To our knowledge, this is the first approach that utilizes sparse representation to superpixel-based image parsing.},
author = {An, Taeg-Hyun and Hong, Ki-Sang},
doi = {10.1016/j.patrec.2015.11.009},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/An, Hong - 2016 - Label transfer via sparse representation.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Boosting,Scene parsing,Sparse representation},
pages = {1--7},
publisher = {Elsevier B.V.},
title = {{Label transfer via sparse representation}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865515003955},
volume = {70},
year = {2016}
}
@article{Mealli2015,
author = {Mealli, Fabrizia and Rubin, Donald B.},
doi = {10.1093/biomet/asv035},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mealli, Rubin - 2015 - Clarifying missing at random and related definitions, and implications when coupled with exchangeability Table 1.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {September},
pages = {asv035},
title = {{Clarifying missing at random and related definitions, and implications when coupled with exchangeability: Table 1.}},
url = {http://biomet.oxfordjournals.org/lookup/doi/10.1093/biomet/asv035},
year = {2015}
}
@article{Frongillo2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07212v1},
author = {Frongillo, Rafael and Kash, Ian A},
eprint = {arXiv:1506.07212v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Frongillo, Kash - 2015 - On Elicitation Complexity and Conditional Elicitation.pdf:pdf},
pages = {1--14},
title = {{On Elicitation Complexity and Conditional Elicitation}},
year = {2015}
}
@article{Swaminathan,
author = {Swaminathan, Adith and Joachims, Thorsten},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Swaminathan, Joachims - Unknown - The Self-Normalized Estimator for Counterfactual Learning.pdf:pdf},
pages = {1--9},
title = {{The Self-Normalized Estimator for Counterfactual Learning}}
}
@article{Cortes2014,
abstract = {We present a series of new theoretical, algorithmic, and empirical results for domain adaptation and sample bias correction in regression. We prove that the discrepancy is a distance for the squared loss when the hypothesis set is the reproducing kernel Hilbert space induced by a universal kernel such as the Gaussian kernel. We give new pointwise loss guarantees based on the discrepancy of the empirical source and target distributions for the general class of kernel-based regularization algorithms. These bounds have a simpler form than previous results and hold for a broader class of convex loss functions not necessarily differentiable, including Lq losses and the hinge loss. We also give finer bounds based on the discrepancy and a weighted feature discrepancy parameter. We extend the discrepancy minimization adaptation algorithm to the more significant case where kernels are used and show that the problem can be cast as an SDP similar to the one in the feature space. We also show that techniques from smooth optimization can be used to derive an efficient algorithm for solving such SDPs even for very high-dimensional feature spaces and large samples. We have implemented this algorithm and report the results of experiments both with artificial and real-world data sets demonstrating its benefits both for general scenario of adaptation and the more specific scenario of sample bias correction. Our results show that it can scale to large data sets of tens of thousands or more points and demonstrate its performance improvement benefits. ?? 2013 Elsevier B.V.},
author = {Cortes, Corinna and Mohri, Mehryar},
doi = {10.1016/j.tcs.2013.09.027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2014 - Domain adaptation and sample bias correction theory and algorithm for regression.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Domain adaptation,Learning theory,Machine learning,Optimization},
number = {June 2013},
pages = {103--126},
title = {{Domain adaptation and sample bias correction theory and algorithm for regression}},
url = {http://dx.doi.org/10.1016/j.tcs.2013.09.027},
volume = {519},
year = {2014}
}
@article{Bach2010,
abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its $\backslash$lova extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
archivePrefix = {arXiv},
arxivId = {1008.4220},
author = {Bach, Francis},
eprint = {1008.4220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bach - 2010 - Structured sparsity-inducing norms through submodular functions.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems NIPS'2010},
pages = {1--9},
title = {{Structured sparsity-inducing norms through submodular functions}},
url = {http://arxiv.org/abs/1008.4220},
year = {2010}
}
@article{Balsubramania,
author = {Balsubramani, Akshay},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani - Unknown - Scalable Semi-Supervised Aggregation of Classifiers.pdf:pdf},
pages = {1--9},
title = {{Scalable Semi-Supervised Aggregation of Classifiers}}
}
@article{Mattos2015,
abstract = {We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.},
archivePrefix = {arXiv},
arxivId = {1511.06644},
author = {Mattos, C{\'{e}}sar Lincoln C. and Dai, Zhenwen and Damianou, Andreas and Forth, Jeremy and Barreto, Guilherme A. and Lawrence, Neil D.},
eprint = {1511.06644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mattos et al. - 2015 - Recurrent Gaussian Processes.pdf:pdf},
number = {3},
pages = {1--11},
title = {{Recurrent Gaussian Processes}},
url = {http://arxiv.org/abs/1511.06644},
year = {2015}
}
@article{Resheff2015a,
abstract = {Often in real-world datasets, especially in high dimensional data, some feature values are missing. Since most data analysis and statistical methods do not handle gracefully missing values, the ?rst step in the analysis requires the imputation of missing values. Indeed, there has been a long standing interest in methods for the imputation of missing values as a pre-processing step. One recent and e?ective approach, the IRMI stepwise regression imputation method, uses a linear regression model for each real-valued feature on the basis of all other features in the dataset. However, the proposed iterative formulation lacks convergence guarantee. Here we propose a closely related method, stated as a single optimization problem and a block coordinate-descent solution which is guaranteed to converge to a local minimum. Experiments show results on both synthetic and benchmark datasets, which are comparable to the results of the IRMI method whenever it converges. However, while in the set of experiments described here IRMI often does not converge, the performance of our methods is shown to be markedly superior in comparison with other methods.},
archivePrefix = {arXiv},
arxivId = {1511.05309},
author = {Resheff, Yehezkel S. and Weinshall, Daphna},
eprint = {1511.05309},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Resheff, Weinshall - 2015 - Optimized Linear Imputation.pdf:pdf},
title = {{Optimized Linear Imputation}},
url = {http://arxiv.org/abs/1511.05309},
year = {2015}
}
@article{Bruce2013,
abstract = {The release of NBA player tracking data greatly enhances the granularity and di-mensionality of basketball statistics used to evaluate and compare player performance. However, the high dimensionality of this new data source can be troublesome as it demands more compu-tational resources and reduces the ability to easily analyze and interpret findings. To avoid such circumstances, we must find a way to reduce the dimensionality of the data set while retaining the ability to differentiate and compare player performance. In this paper Principal Component Analysis (PCA) is used to identify four principal com-ponents that account for over 70{\%} of the variation in player tracking data from the 2013-2014 regular season and intuitive interpretations of these new dimensions are developed by examining the statistics that influence them the most. In this new high variance, low dimensional space, you can easily compare player statistical profiles across any or all of the principal component dimensions to evaluate characteristics that make certain players similar or unique. We use the four principal component scores to construct a simple measure of the similarity between two player statistical profiles, the Statistical Diversity Index (SDI), and average this measure across all players on a team to produce a team level measure of the statistical diversity among its players, the team Statistical Diversity Index (tSDI). The tSDI is found to be positively correlated with winning percentage for the 2013-2014 regular season (p-value 0.01) indicating teams with higher statistical diversity among players generally hold higher winning percentages. We demonstrate potential applications using the prin-cipal component scores to better manage player personnel in terms of statistical diversity. The 2013-2014 Milwaukee Bucks and Philadelphia 76ers are evaluated to determine which dimen-sions they are lacking in statistical diversity and how their personnel changes impact statistical diversity along those dimensions.},
archivePrefix = {arXiv},
arxivId = {1511.04351},
author = {Bruce, Scott},
eprint = {1511.04351},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruce - 2013 - Evaluating the Importance of Statistical Diversity in the NBA Using Player Tracking Data.pdf:pdf},
keywords = {NBA player tracking data,Principal component analysis,dimension reduction,statistical diversity index,team statistical diversity index},
title = {{Evaluating the Importance of Statistical Diversity in the NBA Using Player Tracking Data}},
year = {2013}
}
@article{Hernandez-Gonzalez2015,
author = {Hern{\'{a}}ndez-Gonz{\'{a}}lez, Jer{\'{o}}nimo and naki Inza and Lozano, Jose A.},
doi = {10.1016/j.patrec.2015.10.008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Gonz{\'{a}}lez, Inza, Lozano - 2015 - Weak supervision and other non-standard classification problems a taxonomy.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {degrees of supervision,partially supervised classification,weakly supervised classification},
pages = {49--55},
title = {{Weak supervision and other non-standard classification problems: a taxonomy}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515003505},
volume = {69},
year = {2015}
}
@article{Leisch2004,
author = {Leisch, F},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leisch - 2004 - {\{}FlexMix{\}} A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}.pdf:pdf},
keywords = {finite mixture models,latent class regression,model based clustering,r},
number = {8},
pages = {1--18},
title = {{{\{}FlexMix{\}}: A General Framework for Finite Mixture Models and Latent Class Regression in {\{}R{\}}}},
volume = {11},
year = {2004}
}
@article{Seeger2009,
abstract = {Here, I review facts that are most probably known, namely that the information gain criterion used to drive experimental design in a linear-Gaussian model is submodular, so that a well-known approximation guarantee holds for the sequential greedy algorithm. The criterion is equal to a certain mutual information, which is not submodular in general. I point out the high potential relevance of obtaining approximation guarantees for nonlinear experimental design as well.},
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2009 - On the submodularity of linear experimental design.pdf:pdf},
journal = {Experimental Design},
pages = {1--3},
title = {{On the submodularity of linear experimental design}},
url = {http://infoscience.epfl.ch/record/175483},
year = {2009}
}
@article{Marcum2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02914v1},
author = {Marcum, Christopher Steven},
eprint = {arXiv:1512.02914v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marcum - 2015 - Yet Another Statistical Analysis of Bob Ross Paintings.pdf:pdf},
keywords = {art,bob ross,linear subspace,paintings},
pages = {1--16},
title = {{Yet Another Statistical Analysis of Bob Ross Paintings}},
year = {2015}
}
@article{Janzing2015,
abstract = {We postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law. We argue that this links thermodynamics and causal inference. On the one hand, it entails behaviour that is similar to the usual arrow of time. On the other hand, it motivates a statistical asymmetry between cause and effect that has recently postulated in the field of causal inference, namely, that the probability distribution P(cause) contains no information about the conditional distribution P(effect|cause) and vice versa, while P(effect) may contain information about P(cause|effect).},
archivePrefix = {arXiv},
arxivId = {1512.02057},
author = {Janzing, Dominik and Chaves, Rafael and Schoelkopf, Bernhard},
eprint = {1512.02057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing, Chaves, Schoelkopf - 2015 - Algorithmic independence of initial condition and dynamical law in thermodynamics and causal infere.pdf:pdf},
pages = {1--11},
title = {{Algorithmic independence of initial condition and dynamical law in thermodynamics and causal inference}},
url = {http://arxiv.org/abs/1512.02057},
year = {2015}
}
@article{Dette2015,
abstract = {In this paper we consider the problem of constructing {\$}T{\$}-optimal discriminating designs for Fourier regression models. We provide explicit solutions of the optimal design problem for discriminating between two Fourier regression models, which differ by at most three trigonometric functions. In general, the {\$}T{\$}-optimal discriminating design depends in a complicated way on the parameters of the larger model, and for special configurations of the parameters {\$}T{\$}-optimal discriminating designs can be found analytically. Moreover, we also study this dependence in the remaining cases by calculating the optimal designs numerically. In particular, it is demonstrated that {\$}D{\$}- and {\$}D{\_}s{\$}-optimal designs have rather low efficiencies with respect to the {\$}T{\$}-optimality criterion.},
archivePrefix = {arXiv},
arxivId = {1512.07441},
author = {Dette, Holger and Melas, Viatcheslav B. and Shpilev, Petr},
eprint = {1512.07441},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dette, Melas, Shpilev - 2015 - {\$}T{\$}-optimal discriminating designs for Fourier regression models.pdf:pdf},
keywords = {62k05,ams subject classification,and phrases,cheby-,linear optimality criteria,model discrimination,shev polynomial,t -optimal design,trigonometric models},
pages = {1--17},
title = {{{\$}T{\$}-optimal discriminating designs for Fourier regression models}},
url = {http://arxiv.org/abs/1512.07441},
year = {2015}
}
@article{Lin2011a,
abstract = {We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.},
author = {Lin, Hui and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Bilmes - 2011 - A Class of Submodular Functions for Document Summarization(2).pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {510--520},
title = {{A Class of Submodular Functions for Document Summarization}},
url = {http://ssli.ee.washington.edu/people/hlin/papers/lin-acl11-summ.pdf},
volume = {1},
year = {2011}
}
@article{Krahenbuhl2011,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Krahenbuhl, Philipp and Koltun, Vladlen and KrÂ¨ahenbÂ¨uhl, Philipp and Koltun, Vladlen and Krahenbuhl, Philipp},
eprint = {1210.5644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop//Krahenbuhl et al. - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf;:Users/jkrijthe/Documents/Mendeley Desktop//Krahenbuhl et al. - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24 (Proceedings of NIPS)},
keywords = {conditional random field,filtering,message passing,sampling,segmentation},
number = {4},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
year = {2011}
}
@article{Gelman2013b,
abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by an attitude among some theoreticians that Bayesian methods were absurdânotmerely misguided but obviously wrong in prin- ciple. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We an- alyze in this note the intellectual background behind various misconcep- tions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.},
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5366v5},
author = {Gelman, Andrew and Robert, Christian P.},
doi = {10.1080/00031305.2013.760987},
eprint = {arXiv:1006.5366v5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Robert - 2013 - âNot Only Defended But Also Appliedâ The Perceived Absurdity of Bayesian Inference(2).pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian,bogosity,doomsdsay argument,foundations,frequentist,laplace law of succession},
number = {1},
pages = {1--5},
title = {{âNot Only Defended But Also Appliedâ: The Perceived Absurdity of Bayesian Inference}},
url = {http://basepub.dauphine.fr/handle/123456789/11069$\backslash$nhttp://www.tandfonline.com/doi/abs/10.1080/00031305.2013.760987},
volume = {67},
year = {2013}
}
@article{Gelman2006,
abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant. It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant.},
author = {Gelman, Andrew and Stern, Hal},
doi = {10.1198/000313006X152649},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Stern - 2006 - The Difference Between âSignificantâ and âNot Significantâ is not Itself Statistically Significant.pdf:pdf},
isbn = {0602440371100},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {comparison,hypothesis testing,meta-analysis,pairwise,replication},
number = {4},
pages = {328--331},
title = {{The Difference Between âSignificantâ and âNot Significantâ is not Itself Statistically Significant}},
volume = {60},
year = {2006}
}
@article{Goel2003,
author = {Goel, Prem K. and Ginebra, Josep},
doi = {10.1046/j.1467-9884.2003.00376.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goel, Ginebra - 2003 - When is one experiment 'always better than' another.pdf:pdf},
issn = {00390526},
journal = {Journal of the Royal Statistical Society Series D: The Statistician},
keywords = {Comparison of experiments,Location experiments,Loewner ordering,Optimal design,Statistical information,Stochastic ordering,Sufficiency},
number = {4},
pages = {515--537},
title = {{When is one experiment 'always better than' another?}},
volume = {52},
year = {2003}
}
@book{Mohri2012,
abstract = {In distributed learning, the goal is to perform a learning task over data distributed across multiple nodes with minimal (expensive) communication. Prior work (Daume III et al., 2012) proposes a general model that bounds the communication required for learning classifiers while allowing for {\$}\backslasheps{\$} training error on linearly separable data adversarially distributed across nodes. In this work, we develop key improvements and extensions to this basic model. Our first result is a two-party multiplicative-weight-update based protocol that uses {\$}O(d{\^{}}2 \backslashlog{\{}1/\backslasheps{\}}){\$} words of communication to classify distributed data in arbitrary dimension {\$}d{\$}, {\$}\backslasheps{\$}-optimally. This readily extends to classification over {\$}k{\$} nodes with {\$}O(kd{\^{}}2 \backslashlog{\{}1/\backslasheps{\}}){\$} words of communication. Our proposed protocol is simple to implement and is considerably more efficient than baselines compared, as demonstrated by our empirical results. In addition, we illustrate general algorithm design paradigms for doing efficient learning over distributed data. We show how to solve fixed-dimensional and high dimensional linear programming efficiently in a distributed setting where constraints may be distributed across nodes. Since many learning problems can be viewed as convex optimization problems where constraints are generated by individual points, this models many typical distributed learning scenarios. Our techniques make use of a novel connection from multipass streaming, as well as adapting the multiplicative-weight-update framework more generally to a distributed setting. As a consequence, our methods extend to the wide range of problems solvable using these techniques.},
archivePrefix = {arXiv},
arxivId = {1204.3523},
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
eprint = {1204.3523},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mohri, Rostamizadeh, Talwalkar - 2012 - Foundations of Machine Learning.pdf:pdf},
isbn = {978-0-262-01825-8},
issn = {03029743},
number = {4},
pages = {1--8},
pmid = {18772260},
publisher = {The MIT Press},
title = {{Foundations of Machine Learning}},
volume = {17},
year = {2012}
}
@article{Zhang2016,
abstract = {Traditionally, the field of computational Bayesian statistics has been divided into two main subfields: variational methods and Markov chain Monte Carlo (MCMC). In recent years, however, several methods have been proposed based on combining variational Bayesian infer-ence and MCMC simulation in order to improve their overall accuracy and computational effi-ciency. This marriage of fast evaluation and flexible approximation provides a promising means of designing scalable Bayesian inference methods. In this paper, we explore the possibility of incorporating variational approximation into a state-of-the-art MCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradient computation in the simulation of Hamiltonian flow, which is the bottleneck for many applications of HMC in big data problems. To this end, we use a free-form approximation induced by a fast and flexible surrogate function based on single-hidden layer feedforward neural networks. The surrogate provides sufficiently accurate approximation while allowing for fast exploration of parameter space, resulting in an efficient approximate inference algorithm. We demonstrate the advantages of our method on both synthetic and real data problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.02219v1},
author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
eprint = {arXiv:1602.02219v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Shahbaba, Zhao - 2016 - Variational Hamiltonian Monte Carlo via Score Matching.pdf:pdf},
keywords = {Hamiltonian Monte Carlo,Score Matching,Variational Bayes},
number = {Icml},
title = {{Variational Hamiltonian Monte Carlo via Score Matching}},
year = {2016}
}
@article{Gelman2013e,
abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data. 1.},
author = {Gelman, Andrew and Loken, Eric},
doi = {10.1037/a0037714},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Loken - 2013 - The garden of forking paths Why multiple comparisons can be a problem, even when there is no âfishing exp.pdf:pdf},
issn = {1939-1455},
journal = {Downloaded January},
pages = {1--17},
pmid = {25180805},
title = {{The garden of forking paths: Why multiple comparisons can be a problem, even when there is no âfishing expeditionâ or âp-hackingâ and the research hypothesis}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/p{\_}hacking.pdf},
year = {2013}
}
@article{Ghosh2016a,
abstract = {In this paper, we provide some results that characterize the missing mechanism of a variable in terms of response and nonresponse odds for two and three dimensional incomplete tables. Log-linear parametrization and some properties of the missing data models for the above tables are discussed. All possible cases in which data on one, two or all variables may be missing are considered. For sensitivity analysis of the incomplete tables, we suggest some easily verifiable procedures to evaluate the missing at random (MAR) and not missing at random (NMAR) assumptions in the missing data models. These methods depend only on joint and marginal odds computed from fully and partially observed counts in the tables, respectively. They are based on ideas recently discussed by Kim et al. (2015) for two-way incomplete tables. Finally, some real-life datasets are analyzed to illustrate our results.},
archivePrefix = {arXiv},
arxivId = {1602.00954},
author = {Ghosh, S. and Vellaisamy, P.},
eprint = {1602.00954},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghosh, Vellaisamy - 2016 - Evaluation of missing data mechanisms in incomplete tables.pdf:pdf},
keywords = {and phrases,ghosh was supported by,govt,incomplete tables,mar models,missing data mechanism,nonresponse odds,of india,sponse,the research of s,ugc},
pages = {1--21},
title = {{Evaluation of missing data mechanisms in incomplete tables}},
url = {http://arxiv.org/abs/1602.00954},
year = {2016}
}
@article{Yoder2016,
abstract = {Traditionally, practitioners initialize the {\{}$\backslash$tt k-means{\}} algorithm with centers chosen uniformly at random. Randomized initialization with uneven weights ({\{}$\backslash$tt k-means++{\}}) has recently been used to improve the performance over this strategy in cost and run-time. We consider the k-means problem with semi-supervised information, where some of the data are pre-labeled, and we seek to label the rest according to the minimum cost solution. By extending the {\{}$\backslash$tt k-means++{\}} algorithm and analysis to account for the labels, we derive an improved theoretical bound on expected cost and observe improved performance in simulated and real data examples. This analysis provides theoretical justification for a roughly linear semi-supervised clustering algorithm.},
archivePrefix = {arXiv},
arxivId = {1602.00360},
author = {Yoder, Jordan and Priebe, Carey E.},
eprint = {1602.00360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yoder, Priebe - 2016 - Semi-supervised K-means.pdf:pdf},
keywords = {approximation,clustering,kmeans,partially labeled,semi-supervised},
number = {1},
pages = {1--16},
title = {{Semi-supervised K-means++}},
url = {http://arxiv.org/abs/1602.00360},
year = {2016}
}
@article{Powers2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.07994v1},
author = {Powers, Scott and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1214/15-AOAS866},
eprint = {arXiv:1601.07994v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Powers, Hastie, Tibshirani - 2015 - Customized training with an application to mass spectrometric imaging of cancer tissue.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {Transductive learning, local regression, classific},
number = {4},
pages = {1709--1725},
title = {{Customized training with an application to mass spectrometric imaging of cancer tissue}},
url = {http://projecteuclid.org/euclid.aoas/1453993091},
volume = {9},
year = {2015}
}
@article{Daniely2011,
abstract = {Multiclass learning is an area of growing practical relevance, for which the currently available theory is still far from providing satisfactory understanding. We study the learnability of multiclass prediction, and derive upper and lower bounds on the sample complexity of multiclass hypothesis classes in diï¬erent learning models: batch/online, realizable/unrealizable, full information/bandit feedback. Our analysis reveals a surprising$\backslash$r$\backslash$nphenomenon: In the multiclass setting, in sharp contrast to binary classiï¬cation, not all$\backslash$r$\backslash$nEmpirical Risk Minimization (ERM) algorithms are equally successful. We show that there$\backslash$r$\backslash$nexist hypotheses classes for which some ERM learners have lower sample complexity than$\backslash$r$\backslash$nothers. Furthermore, there are classes that are learnable by some ERM learners, while$\backslash$r$\backslash$nother ERM learner will fail to learn them. We propose a principle for designing good ERM$\backslash$r$\backslash$nlearners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes (that is, classes that are invariant under any$\backslash$r$\backslash$npermutation of label names). We demonstrate the relevance of the theory by analyzing$\backslash$r$\backslash$nthe sample complexity of two widely used hypothesis classes: generalized linear multiclass$\backslash$r$\backslash$nmodels and reduction trees. We also obtain some practically relevant conclusions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.2893v2},
author = {Daniely, Amit and Sabato, Sivan and Ben-David, Shai and Shalev-Shwartz, Shai},
eprint = {arXiv:1308.2893v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Daniely et al. - 2011 - Multiclass Learnability and the ERM principle.pdf:pdf},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1--26},
title = {{Multiclass Learnability and the ERM principle}},
url = {http://eprints.pascal-network.org/archive/00008913/},
volume = {16},
year = {2011}
}
@article{Bontempi2014,
abstract = {The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with {\$}n{\textgreater}2{\$} variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for {\$}n{\textgreater}2{\$} variate distributions.},
archivePrefix = {arXiv},
arxivId = {1412.6285},
author = {Bontempi, Gianluca and Flauder, Maxime},
eprint = {1412.6285},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bontempi, Flauder - 2014 - From dependency to causality a machine learning approach.pdf:pdf},
journal = {arXiv preprint arXiv:1412.6285},
keywords = {causal inference,information theory,machine learning},
pages = {1--24},
title = {{From dependency to causality: a machine learning approach}},
url = {http://arxiv.org/abs/1412.6285},
volume = {16},
year = {2014}
}
@article{Masnadi-shirazi2015,
author = {Masnadi-shirazi, Hamed},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Masnadi-shirazi - 2015 - A View of Margin Losses as Regularizers of Probability Estimates.pdf:pdf},
keywords = {boosting,classification,margin losses,probability elicitation,regularization},
pages = {2751--2795},
title = {{A View of Margin Losses as Regularizers of Probability Estimates}},
volume = {16},
year = {2015}
}
@article{Lopez-Paz2014,
abstract = {We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions regarding the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, and without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's "Fast Causation Coefficient Challenge" competition, with which we won the fastest code prize and ranked third in the overall leaderboard.},
archivePrefix = {arXiv},
arxivId = {1409.4366},
author = {Lopez-Paz, David and Muandet, Krikamol and Recht, Benjamin},
eprint = {1409.4366},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-Paz, Muandet, Recht - 2014 - The Randomized Causation Coefficient.pdf:pdf},
journal = {arXiv preprint arXiv:1409.4366},
keywords = {causality,cause-effect inference,kernel mean embeddings,random features},
pages = {1--4},
title = {{The Randomized Causation Coefficient}},
url = {http://arxiv.org/abs/1409.4366},
volume = {16},
year = {2014}
}
@article{Helmbold2014a,
abstract = {Dropout is a simple but effective technique for learning in neural networks and other settings. However, a sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager, et.al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show (a) when the dropout-regularized criterion has a unique minimizer, (b) when the dropout-regularization penalty goes to infinity with the weights, and when it remains bounded, and (c) that the dropout regularization penalty is not convex. This last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function.   In order to contrast dropout regularization with {\$}L{\_}2{\$} regularization, we formalize the notion of when different sources are more compatible with different regularizers. We then exhibit distributions that are provably more compatible with dropout regularization than {\$}L{\_}2{\$} regularization, and vice versa. These sources provide additional insight into how the inductive biases of dropout and {\$}L{\_}2{\$} regularization differ.},
archivePrefix = {arXiv},
arxivId = {1412.4736},
author = {Helmbold, David P. and Long, Philip M.},
eprint = {1412.4736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Helmbold, Long - 2014 - On the Inductive Bias of Dropout.pdf:pdf},
keywords = {dropout,inductive bias,learning theory},
pages = {1--41},
title = {{On the Inductive Bias of Dropout}},
url = {http://arxiv.org/abs/1412.4736},
volume = {16},
year = {2014}
}
@article{Cohen2016,
abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. By convolving over groups larger than the translation group, G-CNNs build representations that are equivariant to these groups, which makes it possible to greatly increase the degree of parameter sharing. We show how G-CNNs can be implemented with negligible computational overhead for discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. G-CNNs achieve state of the art results on rotated MNIST and significantly improve over a competitive baseline on augmented and non-augmented CIFAR-10.},
archivePrefix = {arXiv},
arxivId = {1602.07576},
author = {Cohen, Taco S. and Welling, Max},
eprint = {1602.07576},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cohen, Welling - 2016 - Group Equivariant Convolutional Networks.pdf:pdf},
title = {{Group Equivariant Convolutional Networks}},
url = {http://arxiv.org/abs/1602.07576},
year = {2016}
}
@article{Rodriguez-Girondo2016,
abstract = {Augmentation of previously established high-dimensional predictive models with new biomolecular markers is an important task in omic applications. We introduce a two-step procedure for the assessment of the augmented predictive value of omic predictors, based on sequential double cross-validation and regularized regression models. We propose several performance indices to summarize the two-stage prediction procedure and a permutation test to formally assess the augmented predictive value of a second omic set of predictors over a primary omic source. The performance of the test is investigated through simulations. We illustrate the new method through the systematic assessment and comparison of the performance of transcriptomics and metabolomics sources in the prediction of body mass index (BMI) using data from the Dietary, Lifestyle, and Genetic determinants of Obesity and Metabolic syndrome (DILGOM) study, a population-based cohort, from Finland.},
archivePrefix = {arXiv},
arxivId = {1601.08197},
author = {Rodr{\'{i}}guez-Girondo, Mar and Salo, Perttu and Burzykowski, Tomasz and Perola, Markus and Houwing-Duistermaat, Jeanine and Mertens, Bart},
eprint = {1601.08197},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rodr{\'{i}}guez-Girondo et al. - 2016 - Sequential double cross-validation for augmented prediction assessment in high-dimensional omic appli.pdf:pdf},
pages = {1--33},
title = {{Sequential double cross-validation for augmented prediction assessment in high-dimensional omic applications}},
url = {http://arxiv.org/abs/1601.08197},
year = {2016}
}
@article{Livingstone2016,
abstract = {We establish general conditions under under which Markov chains produced by the Hamiltonian Monte Carlo method will and will not be {\$}\backslashpi{\$}-irreducible and geometrically ergodic. We consider implementations with both fixed and dynamic integration times. In the fixed case we find that the conditions for geometric ergodicity are essentially a non-vanishing gradient of the log-density which asymptotically points towards the centre of the space and does not grow faster than linearly. In an idealised scenario in which the integration time is allowed to change in different regions of the space, we show that geometric ergodicity can be recovered for a much broader class of target distributions, leading to some guidelines for the choice of this free parameter in practice.},
archivePrefix = {arXiv},
arxivId = {1601.08057},
author = {Livingstone, Samuel and Betancourt, Michael and Byrne, Simon and Girolami, Mark},
eprint = {1601.08057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Livingstone et al. - 2016 - On the Geometric Ergodicity of Hamiltonian Monte Carlo.pdf:pdf},
keywords = {()},
pages = {1--33},
title = {{On the Geometric Ergodicity of Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1601.08057},
year = {2016}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We further propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). The usefulness of explanations is shown via novel experiments, both simulated and with human subjects. Our explanations empower users in various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1602.04938},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@article{Chwialkowski2016,
abstract = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
archivePrefix = {arXiv},
arxivId = {1602.02964},
author = {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
eprint = {1602.02964},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chwialkowski, Strathmann, Gretton - 2016 - A Kernel Test of Goodness of Fit.pdf:pdf},
title = {{A Kernel Test of Goodness of Fit}},
url = {http://arxiv.org/abs/1602.02964},
year = {2016}
}
@article{Lamb2016,
abstract = {We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders.},
archivePrefix = {arXiv},
arxivId = {1602.03220},
author = {Lamb, Alex and Dumoulin, Vincent and Courville, Aaron},
eprint = {1602.03220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lamb, Dumoulin, Courville - 2016 - Discriminative Regularization for Generative Models.pdf:pdf},
number = {Icml},
title = {{Discriminative Regularization for Generative Models}},
url = {http://arxiv.org/abs/1602.03220},
year = {2016}
}
@article{MaalÃ¸e2015a,
abstract = {Deep generative models based upon continuous variational distributions parameterized by deep networks give state-of-the-art performance. In this paper we propose a framework for extending the latent representation with extra auxiliary variables in order to make the variational distribution more expressive for semi-supervised learning. By utilizing the stochasticity of the auxiliary variable we demonstrate how to train discriminative classifiers resulting in state-of-the-art performance within semi-supervised learning exemplified by an 0.96{\%} error on MNIST using 100 labeled data points. Furthermore we observe empirically that using auxiliary variables increases convergence speed suggesting that less expressive variational distributions, not only lead to looser bounds but also slower model training.},
archivePrefix = {arXiv},
arxivId = {1602.05473},
author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
eprint = {1602.05473},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maal{\o}e et al. - 2015 - Improving Semi-Supervised Learning with Auxiliary Deep Generative Models.pdf:pdf},
journal = {NIPS workshop},
pages = {1--5},
title = {{Improving Semi-Supervised Learning with Auxiliary Deep Generative Models}},
year = {2015}
}
@article{Wang2016a,
abstract = {In adaptive data analysis, the user makes a sequence of queries on the data, where at each step the choice of query may depend on the results in previous steps. The releases are often randomized in order to reduce overfitting for such adaptively chosen queries. In this paper, we propose a minimax framework for adaptive data analysis. Assuming Gaussianity of queries, we establish the first sharp minimax lower bound on the squared error in the order of {\$}O(\backslashfrac{\{}\backslashsqrt{\{}k{\}}\backslashsigma{\^{}}2{\}}{\{}n{\}}){\$}, where {\$}k{\$} is the number of queries asked, and {\$}\backslashsigma{\^{}}2/n{\$} is the ordinary signal-to-noise ratio for a single query. Our lower bound is based on the construction of an approximately least favorable adversary who picks a sequence of queries that are most likely to be affected by overfitting. This approximately least favorable adversary uses only one level of adaptivity, suggesting that the minimax risk for 1-step adaptivity with k-1 initial releases and that for {\$}k{\$}-step adaptivity are on the same order. The key technical component of the lower bound proof is a reduction to finding the convoluting distribution that optimally obfuscates the sign of a Gaussian signal. Our lower bound construction also reveals a transparent and elementary proof of the matching upper bound as an alternative approach to Russo and Zou (2015), who used information-theoretic tools to provide the same upper bound. We believe that the proposed framework opens up opportunities to obtain theoretical insights for many other settings of adaptive data analysis, which would extend the idea to more practical realms.},
archivePrefix = {arXiv},
arxivId = {1602.04287},
author = {Wang, Yu-Xiang and Lei, Jing and Fienberg, Stephen E.},
eprint = {1602.04287},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Lei, Fienberg - 2016 - A Minimax Theory for Adaptive Data Analysis.pdf:pdf},
title = {{A Minimax Theory for Adaptive Data Analysis}},
url = {http://arxiv.org/abs/1602.04287},
year = {2016}
}
@article{Tolstikhin2016,
abstract = {In this paper, we study the minimax estimation of the Bochner integral {\$}{\$}$\backslash$mu{\_}k(P):=$\backslash$int{\_}{\{}$\backslash$mathcal{\{}X{\}}{\}} k($\backslash$cdot,x)$\backslash$,dP(x),{\$}{\$} also called as the $\backslash$emph{\{}kernel mean embedding{\}}, based on random samples drawn i.i.d.{\~{}}from {\$}P{\$}, where {\$}k:\backslashmathcal{\{}X{\}}\backslashtimes\backslashmathcal{\{}X{\}}\backslashrightarrow\backslashmathbb{\{}R{\}}{\$} is a positive definite kernel. Various estimators (including the empirical estimator), {\$}\backslashhat{\{}\backslashtheta{\}}{\_}n{\$} of {\$}\backslashmu{\_}k(P){\$} are studied in the literature wherein all of them satisfy {\$}\backslashbigl\backslash| \backslashhat{\{}\backslashtheta{\}}{\_}n-\backslashmu{\_}k(P)\backslashbigr\backslash|{\_}{\{}\backslashmathcal{\{}H{\}}{\_}k{\}}=O{\_}P(n{\^{}}{\{}-1/2{\}}){\$} with {\$}\backslashmathcal{\{}H{\}}{\_}k{\$} being the reproducing kernel Hilbert space induced by {\$}k{\$}. The main contribution of the paper is in showing that the above mentioned rate of {\$}n{\^{}}{\{}-1/2{\}}{\$} is minimax in {\$}\backslash|\backslashcdot\backslash|{\_}{\{}\backslashmathcal{\{}H{\}}{\_}k{\}}{\$} and {\$}\backslash|\backslashcdot\backslash|{\_}{\{}L{\^{}}2(\backslashmathbb{\{}R{\}}{\^{}}d){\}}{\$}-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with {\$}k{\$} being a continuous translation-invariant kernel on {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$}. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of {\$}P{\$} (if it exists). This result has practical consequences in statistical applications as the mean embedding has been widely employed in non-parametric hypothesis testing, density estimation, causal inference and feature selection, through its relation to energy distance (and distance covariance).},
archivePrefix = {arXiv},
arxivId = {1602.04361},
author = {Tolstikhin, Ilya and Sriperumbudur, Bharath and Muandet, Krikamol},
eprint = {1602.04361},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Sriperumbudur, Muandet - 2016 - Minimax Estimation of Kernel Mean Embeddings.pdf:pdf},
keywords = {and phrases,bochner integral,empirical estimator,imax lower bounds,kernel mean embeddings,min-,nonparametric function estimation,reproducing kernel hilbert space},
number = {1},
pages = {1--44},
title = {{Minimax Estimation of Kernel Mean Embeddings}},
url = {http://arxiv.org/abs/1602.04361},
year = {2016}
}
@article{Bui2016,
abstract = {Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.},
archivePrefix = {arXiv},
arxivId = {1602.04133},
author = {Bui, Thang D. and Hern{\'{a}}ndez-Lobato, Daniel and Li, Yingzhen and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Turner, Richard E.},
eprint = {1602.04133},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bui et al. - 2016 - Deep Gaussian Processes for Regression using Approximate Expectation Propagation.pdf:pdf},
title = {{Deep Gaussian Processes for Regression using Approximate Expectation Propagation}},
url = {http://arxiv.org/abs/1602.04133},
year = {2016}
}
@article{Barber,
archivePrefix = {arXiv},
arxivId = {1602.03574},
author = {Barber, Rina Foygel and Cand{\`{e}}s, Emmanuel J},
eprint = {1602.03574},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber, Cand{\`{e}}s - Unknown - A knockoff filter for high-dimensional selective inference.pdf:pdf},
keywords = {direc-,errors of type s,false discovery rate,inference in high-dimensional regression,knockoffs,lasso,martingales,models,tional,variable selection},
number = {1},
pages = {1--32},
title = {{A knockoff filter for high-dimensional selective inference}},
volume = {16}
}
@article{Tolstikhin2016a,
abstract = {Transductive learning considers a training set of {\$}m{\$} labeled samples and a test set of {\$}u{\$} unlabeled samples, with the goal of best labeling that particular test set. Conversely, inductive learning considers a training set of {\$}m{\$} labeled samples drawn iid from {\$}P(X,Y){\$}, with the goal of best labeling any future samples drawn iid from {\$}P(X){\$}. This comparison suggests that transduction is a much easier type of inference than induction, but is this really the case? This paper provides a negative answer to this question, by proving the first known minimax lower bounds for transductive, realizable, binary classification. Our lower bounds show that {\$}m{\$} should be at least {\$}\backslashOmega(d/\backslashepsilon + \backslashlog(1/\backslashdelta)/\backslashepsilon){\$} when {\$}\backslashepsilon{\$}-learning a concept class {\$}\backslashmathcal{\{}H{\}}{\$} of finite VC-dimension {\$}d{\textless}\backslashinfty{\$} with confidence {\$}1-\backslashdelta{\$}, for all {\$}m \backslashleq u{\$}. This result draws three important conclusions. First, general transduction is as hard as general induction, since both problems have {\$}\backslashOmega(d/m){\$} minimax values. Second, the use of unlabeled data does not help general transduction, since supervised learning algorithms such as ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring the unlabeled test set. Third, our transductive lower bounds imply lower bounds for semi-supervised learning, which add to the important discussion about the role of unlabeled data in machine learning.},
archivePrefix = {arXiv},
arxivId = {1602.03027},
author = {Tolstikhin, Ilya and Lopez-Paz, David},
eprint = {1602.03027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Lopez-Paz - 2016 - Minimax Lower Bounds for Realizable Transductive Classification.pdf:pdf},
keywords = {binary classification,minimax lower bounds,realizable learning,transductive learning},
title = {{Minimax Lower Bounds for Realizable Transductive Classification}},
url = {http://arxiv.org/abs/1602.03027},
year = {2016}
}
@article{Shcherbatyi2016,
abstract = {Regularized empirical risk minimization with constrained labels (in contrast to fixed labels) is a remarkably general abstraction of learning. For common loss and regularization functions, this optimization problem assumes the form of a mixed integer program (MIP) whose objective function is non-convex. In this form, the problem is resistant to standard optimization techniques. We construct MIPs with the same solutions whose objective functions are convex. Specifically, we characterize the tightest convex extension of the objective function, given by the Legendre-Fenchel biconjugate. Computing values of this tightest convex extension is NP-hard. However, by applying our characterization to every function in an additive decomposition of the objective function, we obtain a class of looser convex extensions that can be computed efficiently. For some decompositions, common loss and regularization functions, we derive a closed form.},
archivePrefix = {arXiv},
arxivId = {1602.06746},
author = {Shcherbatyi, Iaroslav and Andres, Bjoern},
eprint = {1602.06746},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shcherbatyi, Andres - 2016 - Convexification of Learning from Constraints.pdf:pdf},
number = {1},
pages = {1--13},
title = {{Convexification of Learning from Constraints}},
url = {http://arxiv.org/abs/1602.06746},
year = {2016}
}
@article{W??rtz2014,
abstract = {BACKGROUND: Increased adiposity is linked with higher risk for cardiometabolic diseases. We aimed to determine to what extent elevated body mass index (BMI) within the normal weight range has causal effects on the detailed systemic metabolite profile in early adulthood.$\backslash$n$\backslash$nMETHODS AND FINDINGS: We used Mendelian randomization to estimate causal effects of BMI on 82 metabolic measures in 12,664 adolescents and young adults from four population-based cohorts in Finland (mean age 26 y, range 16-39 y; 51{\%} women; mean Â± standard deviation BMI 24 Â± 4 kg/m(2)). Circulating metabolites were quantified by high-throughput nuclear magnetic resonance metabolomics and biochemical assays. In cross-sectional analyses, elevated BMI was adversely associated with cardiometabolic risk markers throughout the systemic metabolite profile, including lipoprotein subclasses, fatty acid composition, amino acids, inflammatory markers, and various hormones (p{\textless}0.0005 for 68 measures). Metabolite associations with BMI were generally stronger for men than for women (median 136{\%}, interquartile range 125{\%}-183{\%}). A gene score for predisposition to elevated BMI, composed of 32 established genetic correlates, was used as the instrument to assess causality. Causal effects of elevated BMI closely matched observational estimates (correspondence 87{\%} Â± 3{\%}; R(2)= 0.89), suggesting causative influences of adiposity on the levels of numerous metabolites (p{\textless}0.0005 for 24 measures), including lipoprotein lipid subclasses and particle size, branched-chain and aromatic amino acids, and inflammation-related glycoprotein acetyls. Causal analyses of certain metabolites and potential sex differences warrant stronger statistical power. Metabolite changes associated with change in BMI during 6 y of follow-up were examined for 1,488 individuals. Change in BMI was accompanied by widespread metabolite changes, which had an association pattern similar to that of the cross-sectional observations, yet with greater metabolic effects (correspondence 160{\%} Â± 2{\%}; R(2) = 0.92).$\backslash$n$\backslash$nCONCLUSIONS: Mendelian randomization indicates causal adverse effects of increased adiposity with multiple cardiometabolic risk markers across the metabolite profile in adolescents and young adults within the non-obese weight range. Consistent with the causal influences of adiposity, weight changes were paralleled by extensive metabolic changes, suggesting a broadly modifiable systemic metabolite profile in early adulthood. Please see later in the article for the Editors' Summary.},
author = {W??rtz, Peter and Wang, Qin and Kangas, Antti J. and Richmond, Rebecca C. and Skarp, Joni and Tiainen, Mika and Tynkkynen, Tuulia and Soininen, Pasi and Havulinna, Aki S. and Kaakinen, Marika and Viikari, Jorma S. and Savolainen, Markku J. and K??h??nen, Mika and Lehtim??ki, Terho and M??nnist??, Satu and Blankenberg, Stefan and Zeller, Tanja and Laitinen, Jaana and Pouta, Anneli and M??ntyselk??, Pekka and Vanhala, Mauno and Elliott, Paul and Pietil??inen, Kirsi H. and Ripatti, Samuli and Salomaa, Veikko and Raitakari, Olli T. and J??rvelin, Marjo Riitta and Smith, George Davey and Ala-Korpela, Mika},
doi = {10.1371/journal.pmed.1001765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wrtz et al. - 2014 - Metabolic Signatures of Adiposity in Young Adults Mendelian Randomization Analysis and Effects of Weight Change.pdf:pdf},
isbn = {2201127891},
issn = {15491676},
journal = {PLoS Medicine},
number = {12},
pmid = {25490400},
title = {{Metabolic Signatures of Adiposity in Young Adults: Mendelian Randomization Analysis and Effects of Weight Change}},
volume = {11},
year = {2014}
}
@article{Xiang2016,
abstract = {In this article, we propose a class of semiparametric mixture regression models with single-index. We argue that many recently proposed semiparametric/nonparametric mixture regression models can be considered special cases of the proposed model. However, unlike existing semiparametric mixture regression models, the new pro- posed model can easily incorporate multivariate predictors into the nonparametric components. Back?tting estimates and the corresponding algorithms have been proposed for to achieve the optimal convergence rate for both the parameters and the nonparametric functions. We show that nonparametric functions can be esti- mated with the same asymptotic accuracy as if the parameters were known and the index parameters can be estimated with the traditional parametric root n convergence rate. Simulation studies and an application of NBA data have been conducted to demonstrate the ?nite sample performance of the proposed models.},
archivePrefix = {arXiv},
arxivId = {1602.06610},
author = {Xiang, Sijia and Yao, Weixin},
eprint = {1602.06610},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiang, Yao - 2016 - Mixture of Regression Models with Single-Index.pdf:pdf},
keywords = {em algorithm,kernel regression,mixture regression model,single-index},
pages = {1--29},
title = {{Mixture of Regression Models with Single-Index}},
url = {http://arxiv.org/abs/1602.06610},
year = {2016}
}
@article{Young2016,
abstract = {The NIH Library of Integrated Network-based Cellular Signatures (LINCS) contains gene expression data from over a million experiments, using Luminex Bead technology. Only 500 colors are used to measure the expression levels of the 1,000 landmark genes measured, and the data for the resulting pairs of genes are deconvolved. The raw data are sometimes inadequate for reliable deconvolution leading to artifacts in the final processed data. These include the expression levels of paired genes being flipped or given the same value, and clusters of values that are not at the true expression level. We propose a new method called model-based clustering with data correction (MCDC) that is able to identify and correct these three kinds of artifacts simultaneously. We show that MCDC improves the resulting gene expression data in terms of agreement with external baselines, as well as improving results from subsequent analysis.},
archivePrefix = {arXiv},
arxivId = {1602.06316},
author = {Young, William Chad and Yeung, Ka Yee and Raftery, Adrian E.},
eprint = {1602.06316},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Young, Yeung, Raftery - 2016 - Model-based clustering with data correction for removing artifacts in gene expression data.pdf:pdf},
keywords = {a phd student,associate professor,box 354322,campus box 358426 1900,commerce street tacoma,department of statistics,edu,email,gene regulatory network,institute of,ka yee yeung is,lincs,mcdc,model-based clustering,seattle,tacoma,technology,university of washington,university of washington -,usa,uw,wa,wa 98195-4322,william chad young is,wmchad},
pages = {1--28},
title = {{Model-based clustering with data correction for removing artifacts in gene expression data}},
url = {http://arxiv.org/abs/1602.06316},
year = {2016}
}
@article{Mnih2016,
abstract = {Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.},
archivePrefix = {arXiv},
arxivId = {1602.06725},
author = {Mnih, Andriy and Rezende, Danilo J.},
eprint = {1602.06725},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mnih, Rezende - 2016 - Variational inference for Monte Carlo objectives.pdf:pdf},
title = {{Variational inference for Monte Carlo objectives}},
url = {http://arxiv.org/abs/1602.06725},
year = {2016}
}
@article{Frostig2016,
abstract = {We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a "smooth projection" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.},
archivePrefix = {arXiv},
arxivId = {1602.06872},
author = {Frostig, Roy and Musco, Cameron and Musco, Christopher and Sidford, Aaron},
eprint = {1602.06872},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Frostig et al. - 2016 - Principal Component Projection Without Principal Component Analysis.pdf:pdf},
pages = {1--18},
title = {{Principal Component Projection Without Principal Component Analysis}},
url = {http://arxiv.org/abs/1602.06872},
year = {2016}
}
@article{Bareinboim2014,
abstract = {Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.},
author = {Bareinboim, Elias and Tian, Jin and Pearl, Judea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bareinboim, Tian, Pearl - 2014 - Recovering from Selection Bias in Causal and Statistical Inference.pdf:pdf},
journal = {Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI 2014)},
keywords = {causal inference,causality,sampling bias,selection bias,statistical inference},
number = {Pearl},
pages = {2410--2416},
title = {{Recovering from Selection Bias in Causal and Statistical Inference}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8628/8707 http://ftp.cs.ucla.edu/pub/stat{\_}ser/r425.pdf},
year = {2014}
}
@article{Jain2016,
abstract = {The problem of developing binary classifiers from positive and unlabeled data is often encountered in machine learning. A common requirement in this setting is to approximate posterior probabilities of positive and negative classes for a previously unseen data point. This problem can be decomposed into two steps: (i) the development of accurate predictors that discriminate between positive and unlabeled data, and (ii) the accurate estimation of the prior probabilities of positive and negative examples. In this work we primarily focus on the latter subproblem. We study nonparametric class prior estimation and formulate this problem as an estimation of mixing proportions in two-component mixture models, given a sample from one of the components and another sample from the mixture itself. We show that estimation of mixing proportions is generally ill-defined and propose a canonical form to obtain identifiability while maintaining the flexibility to model any distribution. We use insights from this theory to elucidate the optimization surface of the class priors and propose an algorithm for estimating them. To address the problems of high-dimensional density estimation, we provide practical transformations to low-dimensional spaces that preserve class priors. Finally, we demonstrate the efficacy of our method on univariate and multivariate data.},
archivePrefix = {arXiv},
arxivId = {1601.01944},
author = {Jain, Shantanu and White, Martha and Trosset, Michael W. and Radivojac, Predrag},
eprint = {1601.01944},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain et al. - 2016 - Nonparametric semi-supervised learning of class proportions.pdf:pdf},
title = {{Nonparametric semi-supervised learning of class proportions}},
url = {http://arxiv.org/abs/1601.01944},
year = {2016}
}
@article{Yang2015b,
abstract = {The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates, guaranteeing geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions.},
archivePrefix = {arXiv},
arxivId = {1512.08269},
author = {Yang, Fanny and Balakrishnan, Sivaraman and Wainwright, Martin J},
eprint = {1512.08269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang, Balakrishnan, Wainwright - 2015 - Statistical and Computational Guarantees for the Baum-Welch Algorithm.pdf:pdf},
title = {{Statistical and Computational Guarantees for the Baum-Welch Algorithm}},
url = {http://arxiv.org/abs/1512.08269},
year = {2015}
}
@article{Ambikasaran2016,
abstract = {A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the {\$}n{\$} -dimensional setting, however, it requires the inversion of an {\$}n times n{\$} covariance matrix, {\$}C{\$} , as well as the evaluation of its determinant, {\$}det (C){\$} . In many cases, such as regression using Gaussian processes, the covariance matrix is of the form {\$}C = sigma {\^{}}2 I + K{\$} , where {\$}K{\$} is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix {\$}C{\$} is typically dense, causing standard direct methods for inversion and determinant evaluation to require {\$}mathcal {\{}O{\}}(n{\^{}}3){\$} work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix {\$}C{\$} can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an {\$}mathcal {\{}O{\}} (n,log{\^{}}2, n){\$} algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant {\$}det (C){\$}, permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining {\$}K{\$} . Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels.},
archivePrefix = {arXiv},
arxivId = {1403.6015},
author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
doi = {10.1109/TPAMI.2015.2448083},
eprint = {1403.6015},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ambikasaran et al. - 2016 - Fast Direct Methods for Gaussian Processes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Acceleration,Approximation methods,Bayesian analysis,Covariance matrices,Gaussian process,Gaussian processes,Kernel,Matrix decomposition,Symmetric matrices,covariance function,covariance matrix,determinant,direct solver,evidence,fast multipole method,hierarchical off-diagonal low-rank,likelihood},
number = {2},
pages = {252--265},
title = {{Fast Direct Methods for Gaussian Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7130620},
volume = {38},
year = {2016}
}
@article{Berthold2008,
archivePrefix = {arXiv},
arxivId = {1601.02213},
author = {Berthold, Michael R and H{\"{o}}ppner, Frank},
eprint = {1601.02213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berthold, H{\"{o}}ppner - 2008 - On Clustering Time Series Using Euclidean Distance and Pearson Correlation.pdf:pdf},
title = {{On Clustering Time Series Using Euclidean Distance and Pearson Correlation}},
year = {2008}
}
@article{Hanneke2014,
abstract = {This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the mini-max label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically significantly smaller than the best previously-published upper bounds in the active learning literature. In high-noise regimes, it turns out that all active learn-ing problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the la-bel complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.0996v1},
author = {Hanneke, Steve and Yang, Liu},
eprint = {arXiv:1410.0996v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanneke, Yang - 2014 - Minimax Analysis of Active Learning.pdf:pdf},
keywords = {()},
pages = {3487--3602},
title = {{Minimax Analysis of Active Learning}},
volume = {16},
year = {2014}
}
@article{Calandriello2016,
abstract = {While the harmonic function solution performs well in many semi-supervised learning (SSL) tasks, it is known to scale poorly with the number of samples. Recent successful and scalable methods, such as the eigenfunction method focus on efficiently approximating the whole spectrum of the graph Laplacian constructed from the data. This is in contrast to various subsampling and quantization methods proposed in the past, which may fail in preserving the graph spectra. However, the impact of the approximation of the spectrum on the final generalization error is either unknown, or requires strong assumptions on the data. In this paper, we introduce Sparse-HFS, an efficient edge-sparsification algorithm for SSL. By constructing an edge-sparse and spectrally similar graph, we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of Sparse-HFS. As a result, we obtain a theoretically-grounded approximation scheme for graph-based SSL that also empirically matches the performance of known large-scale methods.},
archivePrefix = {arXiv},
arxivId = {1601.05675},
author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal and Koutis, Ioannis},
eprint = {1601.05675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Calandriello et al. - 2016 - Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning.pdf:pdf},
title = {{Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning}},
url = {http://arxiv.org/abs/1601.05675},
year = {2016}
}
@article{2016b,
author = {æ¾å, å¡ä¸},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/æ¾å - 2016 - èª­æ¸ä¼è³æ Semi-supervised learning with Ladder Networks.pdf:pdf},
pages = {1--9},
title = {{èª­æ¸ä¼è³æ Semi-supervised learning with Ladder Networks}},
year = {2016}
}
@article{Zhou2003,
abstract = {We consider the general problem of hlearning from labelled and unlabelled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labelled and unlabelled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrated effective use of unlabelled data.},
author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Sch, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou et al. - 2003 - Learning with Local and Global Consistency.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {595--602},
title = {{Learning with Local and Global Consistency}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}AA41.pdf},
volume = {1},
year = {2003}
}
@article{Yates1933,
author = {Yates, Frank},
journal = {Empire Journal of Experimental Agriculture},
number = {2},
pages = {129--142},
title = {{The analysis of replicated experiments when the field results are incomplete}},
volume = {1},
year = {1933}
}
@article{Krijthe2016,
author = {Krijthe, Jesse H and Loog, Marco},
journal = {arXiv preprint arXiv:1602.07865},
title = {{Projected Estimators for Robust Semi-supervised Classification}},
year = {2016}
}
@article{Wilkinson1958,
author = {Wilkinson, G. N.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilkinson - 1958 - Estimation of Missing Values for the Analysis of Incomplete Data.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
number = {2},
pages = {257--286},
title = {{Estimation of Missing Values for the Analysis of Incomplete Data}},
volume = {14},
year = {1958}
}
